<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>12-13-2010</start_date>
		<end_date>12-17-2010</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[]]></city>
		<state></state>
		<country></country>
	</conference_loc>
	<conference_url>http://computer.org/proceedings/icdm/2010/4256</conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES11036</series_id>
		<series_title><![CDATA[ICDM]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>1933307</proc_id>
	<acronym>ICDM '10</acronym>
	<proc_desc>Proceedings of the 2010 IEEE International Conference on Data Mining</proc_desc>
	<conference_number></conference_number>
	<proc_class></proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn13>978-0-7695-4256-0</isbn13>
	<issn>1550-4786</issn>
	<eissn></eissn>
	<copyright_year>2010</copyright_year>
	<publication_date>12-13-2010</publication_date>
	<pages></pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<publisher>
		<publisher_id>PUB769</publisher_id>
		<publisher_code>IEEEW</publisher_code>
		<publisher_name>IEEE Computer Society</publisher_name>
		<publisher_address>1730 Massachusetts Ave., NW             Washington, DC</publisher_address>
		<publisher_city></publisher_city>
		<publisher_state></publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>20036-1992</publisher_zip_code>
		<publisher_contact></publisher_contact>
		<publisher_phone></publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url></publisher_url>
	</publisher>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
</proceeding_rec>
<content>
	<article_rec>
		<article_id>1934533</article_id>
		<sort_key>10</sort_key>
		<display_label>Page</display_label>
		<pages>C4,C1</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Cover Art]]></title>
		<page_from>C1</page_from>
		<doi_number>10.1109/ICDM.2010.173</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934533</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934534</article_id>
		<sort_key>20</sort_key>
		<display_label>Page</display_label>
		<pages>i</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Title Page i]]></title>
		<page_from>i</page_from>
		<doi_number>10.1109/ICDM.2010.1</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934534</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934535</article_id>
		<sort_key>30</sort_key>
		<display_label>Page</display_label>
		<pages>iii</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Title Page iii]]></title>
		<page_from>iii</page_from>
		<doi_number>10.1109/ICDM.2010.2</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934535</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934536</article_id>
		<sort_key>40</sort_key>
		<display_label>Page</display_label>
		<pages>iv</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Copyright Page]]></title>
		<page_from>iv</page_from>
		<doi_number>10.1109/ICDM.2010.3</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934536</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934537</article_id>
		<sort_key>60</sort_key>
		<display_label>Pages</display_label>
		<pages>xv-xvi</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Welcome Message from the Conference Chairs]]></title>
		<page_from>xv</page_from>
		<page_to>xvi</page_to>
		<doi_number>10.1109/ICDM.2010.4</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934537</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934538</article_id>
		<sort_key>70</sort_key>
		<display_label>Pages</display_label>
		<pages>xvii-xviii</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Message from the  Program Committee Co-Chairs]]></title>
		<page_from>xvii</page_from>
		<page_to>xviii</page_to>
		<doi_number>10.1109/ICDM.2010.5</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934538</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934539</article_id>
		<sort_key>100</sort_key>
		<display_label>Page</display_label>
		<pages>5</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Mining Billion-node Graphs]]></title>
		<subtitle><![CDATA[Patterns, Generators and Tools]]></subtitle>
		<page_from>5</page_from>
		<doi_number>10.1109/ICDM.2010.170</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934539</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2529759</person_id>
				<author_profile_id><![CDATA[81479659310]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Faloutsos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934540</article_id>
		<sort_key>110</sort_key>
		<display_label>Page</display_label>
		<pages>6</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Assessing the Significance of Groups in High-Dimensional Data]]></title>
		<page_from>6</page_from>
		<doi_number>10.1109/ICDM.2010.171</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934540</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531952</person_id>
				<author_profile_id><![CDATA[81540928556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Geoff]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McLachlan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934541</article_id>
		<sort_key>120</sort_key>
		<display_label>Page</display_label>
		<pages>7</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[10 Years of Data Mining Research]]></title>
		<subtitle><![CDATA[Retrospect and Prospect]]></subtitle>
		<page_from>7</page_from>
		<doi_number>10.1109/ICDM.2010.172</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934541</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532510</person_id>
				<author_profile_id><![CDATA[81479659137]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xindong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934542</article_id>
		<sort_key>130</sort_key>
		<display_label>Pages</display_label>
		<pages>8-17</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Detecting Novel Discrepancies in Communication Networks]]></title>
		<page_from>8</page_from>
		<page_to>17</page_to>
		<doi_number>10.1109/ICDM.2010.145</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934542</url>
		<abstract>
			<par><![CDATA[We address the problem of detecting characteristic patterns in communication networks. We introduce a scalable approach based on set-system discrepancy. By implicitly labeling each network edge with the sequence of times in which its two endpoints communicate, we view an entire communication network as a set-system. This view allows us to use combinatorial discrepancy as a mechanism to "observe" system behavior at different time scales. We illustrate our approach, called Discrepancy-based Novelty Detector (DND), on networks obtained from emails, blue tooth connections, IP traffic, and tweets. DND has almost linear runtime complexity and linear storage complexity in the number of communications. Examples of novel discrepancies that it detects are (i) asynchronous communications and (ii) disagreements in the firing rates of nodes and edges relative to the communication network as a whole.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Novelty detection, communication networks, set-system discrepancy]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533559</person_id>
				<author_profile_id><![CDATA[81479647717]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Abello]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533560</person_id>
				<author_profile_id><![CDATA[81100597717]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eliassi-Rad]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533561</person_id>
				<author_profile_id><![CDATA[81479658877]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nishchal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Devanur]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934543</article_id>
		<sort_key>140</sort_key>
		<display_label>Pages</display_label>
		<pages>18-27</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Multi-agent Random Walks for Local Clustering on Graphs]]></title>
		<page_from>18</page_from>
		<page_to>27</page_to>
		<doi_number>10.1109/ICDM.2010.87</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934543</url>
		<abstract>
			<par><![CDATA[We consider the problem of local graph clustering where the aim is to discover the local cluster corresponding to a point of interest. The most popular algorithms to solve this problem start a random walk at the point of interest and let it run until some stopping criterion is met. The vertices visited are then considered the local cluster. We suggest a more powerful alternative, the multi-agent random walk. It consists of several ``agents'' connected by a fixed rope of length l. All agents move independently like a standard random walk on the graph, but they are constrained to have distance at most l from each other. The main insight is that for several agents it is harder to simultaneously travel over the bottleneck of a graph than for just one agent. Hence, the multi-agent random walk has less tendency to mistakenly merge two different clusters than the original random walk. In our paper we analyze the multi-agent random walk theoretically and compare it experimentally to the major local graph clustering algorithms from the literature. We find that our multi-agent random walk consistently outperforms these algorithms.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Local Clustering, Graph Clustering, Random Walk, Mixing Time]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532511</person_id>
				<author_profile_id><![CDATA[81479640793]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Morteza]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Alamgir]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532512</person_id>
				<author_profile_id><![CDATA[81100383399]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ulrike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[von Luxburg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934544</article_id>
		<sort_key>150</sort_key>
		<display_label>Pages</display_label>
		<pages>28-37</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Spatiotemporal Event Detection in Mobility Network]]></title>
		<page_from>28</page_from>
		<page_to>37</page_to>
		<doi_number>10.1109/ICDM.2010.29</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934544</url>
		<abstract>
			<par><![CDATA[Learning and identifying events in network traffic is crucial for service providers to improve their mobility network performance. In fact, large special events attract cell phone users to relative small areas, which causes sudden surge in network traffic. To handle such increased load, it is necessary to measure the increased network traffic and quantify the impact of the events, so that relevant resources can be optimized to enhance the network capability. However, this problem is challenging due to several issues: (1) Multiple periodic temporal traffic patterns (i.e., nonhomogeneous process) even for normal traffic, (2) Irregularly distributed spatial neighbor information, (3) Different temporal patterns driven by different events even for spatial neighborhoods, (4) Large scale data set. This paper proposes a systematic event detection method that deals with the above problems. With the additivity property of Poisson process, we propose an algorithm to integrate spatial information by aggregating the behavior of temporal data under various areas. Markov Modulated Nonhomogeneous Poisson Process (MMNHPP) is employed to estimate the probability with which event happens, when and where the events take place, and assess the spatial and temporal impacts of the events. Localized events are then ranked globally for prioritizing more significant events. Synthetic data are generated to illustrate our procedure and validate the performance. An industrial example from a telecommunication company is also presented to show the effectiveness of the proposed method.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Spatiotemporal, Event Detection, Markov Modulated Nonhomogeneous Poisson Process, Network Traffic]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533562</person_id>
				<author_profile_id><![CDATA[81479643692]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Au]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533563</person_id>
				<author_profile_id><![CDATA[81330490213]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Rong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Duan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533564</person_id>
				<author_profile_id><![CDATA[81479653617]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Heeyoung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533565</person_id>
				<author_profile_id><![CDATA[81479643259]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Guang-Qin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934551</article_id>
		<sort_key>160</sort_key>
		<display_label>Pages</display_label>
		<pages>38-47</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[An Unsupervised Approach to Modeling Personalized Contexts of Mobile Users]]></title>
		<page_from>38</page_from>
		<page_to>47</page_to>
		<doi_number>10.1109/ICDM.2010.16</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934551</url>
		<abstract>
			<par><![CDATA[Mobile context modeling is a process of recognizing and reasoning about contexts and situations in a mobile environment, which is critical for the success of context-aware mobile services. While there are prior work on mobile context modeling, the use of unsupervised learning techniques for mobile context modeling is still under-explored. Indeed, unsupervised techniques have the ability to learn personalized contexts which are difficult to be predefined. To that end, in this paper, we propose an unsupervised approach to modeling personalized contexts of mobile users. Along this line, we first segment the raw context data sequences of mobile users into context sessions where a context session contains a group of adjacent context records which are mutually similar and usually reflect the similar contexts. Then, we exploit topic models to learn personalized contexts in the form of probabilistic distributions of raw context data from the context sessions. Finally, experimental results on real-world data show that the proposed approach is efficient and effective for mining personalized contexts of mobile users.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[mobile context modeling, unsupervised approach]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533024</person_id>
				<author_profile_id><![CDATA[81472651290]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tengfei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533025</person_id>
				<author_profile_id><![CDATA[81479658478]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Happia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533026</person_id>
				<author_profile_id><![CDATA[81323488612]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Enhong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533027</person_id>
				<author_profile_id><![CDATA[81440597994]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jilei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tian]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533028</person_id>
				<author_profile_id><![CDATA[81451596433]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Hui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xiong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934552</article_id>
		<sort_key>170</sort_key>
		<display_label>Pages</display_label>
		<pages>48-57</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Fast and Flexible Multivariate Time Series Subsequence Search]]></title>
		<page_from>48</page_from>
		<page_to>57</page_to>
		<doi_number>10.1109/ICDM.2010.36</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934552</url>
		<abstract>
			<par><![CDATA[Multivariate Time-Series (MTS) are ubiquitous, and are generated in areas as disparate as sensor recordings in aerospace systems, music and video streams, medical monitoring, and financial systems. Domain experts are often interested in searching for interesting multivariate patterns from these MTS databases which can contain up to several gigabytes of data. Surprisingly, research on MTS search is very limited. Most existing work only supports queries with the same length of data, or queries on a fixed set of variables. In this paper, we propose an efficient and flexible subsequence search framework for massive MTS databases, that, for the first time, enables querying on any subset of variables with arbitrary time delays between them. We propose two provably correct algorithms to solve this problem #x2014; (1) an R*-tree Based Search (RBS) which uses Minimum Bounding Rectangles (MBR) to organize the subsequences, and (2) a List Based Search (LBS) algorithm which uses sorted lists for indexing. We demonstrate the performance of these algorithms using two large MTS databases from the aviation domain, each containing several millions of observations. Both these tests show that our algorithms have very high prune rates (>95%) thus needing actual disk access for only less than 5% of the observations. To the best of our knowledge, this is the first flexible MTS search algorithm capable of subsequence search on any subset of variables. Moreover, MTS subsequence search has never been attempted on datasets of the size we have used in this paper.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[similarity search, multivariate analysis]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2529814</person_id>
				<author_profile_id><![CDATA[81317491726]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kanishka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bhaduri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529815</person_id>
				<author_profile_id><![CDATA[81479653154]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Qiang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529816</person_id>
				<author_profile_id><![CDATA[81100418152]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nikunj]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Oza]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529817</person_id>
				<author_profile_id><![CDATA[81436597130]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ashok]]></first_name>
				<middle_name><![CDATA[N.]]></middle_name>
				<last_name><![CDATA[Srivastava]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934553</article_id>
		<sort_key>180</sort_key>
		<display_label>Pages</display_label>
		<pages>58-67</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[iSAX 2.0]]></title>
		<subtitle><![CDATA[Indexing and Mining One Billion Time Series]]></subtitle>
		<page_from>58</page_from>
		<page_to>67</page_to>
		<doi_number>10.1109/ICDM.2010.124</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934553</url>
		<abstract>
			<par><![CDATA[There is an increasingly pressing need, by several applications in diverse domains, for developing techniques able to index and mine very large collections of time series. Examples of such applications come from astronomy, biology, the web, and other domains. It is not unusual for these applications to involve numbers of time series in the order of hundreds of millions to billions. However, all relevant techniques that have been proposed in the literature so far have not considered any data collections much larger than one-million time series. In this paper, we describe iSAX 2.0, a data structure designed for indexing and mining truly massive collections of time series. We show that the main bottleneck in mining such massive datasets is the time taken to build the index, and we thus introduce a novel bulk loading mechanism, the first of this kind specifically tailored to a time series index. We show how our method allows mining on datasets that would otherwise be completely untenable, including the first published experiments to index one billion time series, and experiments in mining massive data from domains as diverse as entomology, DNA and web-scale image collections.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[time series, data mining, representations, indexing]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533603</person_id>
				<author_profile_id><![CDATA[81479663222]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alessandro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Camerra]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533604</person_id>
				<author_profile_id><![CDATA[81331501619]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Themis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Palpanas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533605</person_id>
				<author_profile_id><![CDATA[81367595197]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shieh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533606</person_id>
				<author_profile_id><![CDATA[81100209161]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Eamonn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Keogh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934554</article_id>
		<sort_key>190</sort_key>
		<display_label>Pages</display_label>
		<pages>68-77</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[Abstraction Augmented Markov Models]]></title>
		<page_from>68</page_from>
		<page_to>77</page_to>
		<doi_number>10.1109/ICDM.2010.158</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934554</url>
		<abstract>
			<par><![CDATA[High accuracy sequence classification often requires the use of higher order Markov models (MMs). However, the number of MM parameters increases exponentially with the range of direct dependencies between sequence elements, thereby increasing the risk of over fitting when the data set is limited in size. We present abstraction augmented Markov models (AAMMs) that effectively reduce the number of numeric parameters of kth order MMs by successively grouping strings of length k (i.e., k-grams) into abstraction hierarchies. We evaluate AAMMs on three protein sub cellular localization prediction tasks. The results of our experiments show that abstraction makes it possible to construct predictive models that use significantly smaller number of features (by one to three orders of magnitude) as compared to MMs. AAMMs are competitive with and, in some cases, significantly outperform MMs. Moreover, the results show that AAMMs often perform significantly better than variable order Markov models, such as decomposed context tree weighting, prediction by partial match, and probabilistic suffix trees.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Markov models, abstraction, sequence classification]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533607</person_id>
				<author_profile_id><![CDATA[81392612505]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cornelia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Caragea]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533608</person_id>
				<author_profile_id><![CDATA[81100327674]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Adrian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Silvescu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533609</person_id>
				<author_profile_id><![CDATA[81100256663]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Doina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Caragea]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533610</person_id>
				<author_profile_id><![CDATA[81100409481]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Vasant]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Honavar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934560</article_id>
		<sort_key>200</sort_key>
		<display_label>Pages</display_label>
		<pages>78-87</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[A Graph-Based Approach for Multi-folder Email Classification]]></title>
		<page_from>78</page_from>
		<page_to>87</page_to>
		<doi_number>10.1109/ICDM.2010.55</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934560</url>
		<abstract>
			<par><![CDATA[This paper presents a novel framework for multi-folder email classification using graph mining as the underlying technique. Although several techniques exist (e.g., SVM, TF-IDF, n-gram) for addressing this problem in a delimited context, they heavily rely on extracting high-frequency keywords, thus ignoring the inherent structural aspects of an email (or document in general) which can play a critical role in classification. Some of the models (e.g., n-gram) consider only the words without taking into consideration where in the structure these words appear together. This paper presents a supervised learning model that leverages graph mining techniques for multi-folder email classification. A ranking formula is presented for ordering the representative - common and recurring - substructures generated from pre-classified emails. These ranked representative substructures are then used for categorizing incoming emails. This approach is based on a global ranking model that incorporates several relevant parameters for email classification and overcomes numerous problems faced by extant approaches used for multi-folder classification. A number of parameters which influence the generation of representative substructures are analyzed, reexamined, and adapted to multiple folders. The effect of graph representations has been analyzed. The effectiveness of the proposed approach has been validated experimentally.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2529862</person_id>
				<author_profile_id><![CDATA[81100651311]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sharma]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chakravarthy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529863</person_id>
				<author_profile_id><![CDATA[81479659418]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Aravind]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Venkatachalam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529864</person_id>
				<author_profile_id><![CDATA[81435607455]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Aditya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Telang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934561</article_id>
		<sort_key>210</sort_key>
		<display_label>Pages</display_label>
		<pages>88-97</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[Scalable Influence Maximization in Social Networks under the Linear Threshold Model]]></title>
		<page_from>88</page_from>
		<page_to>97</page_to>
		<doi_number>10.1109/ICDM.2010.118</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934561</url>
		<abstract>
			<par><![CDATA[Influence maximization is the problem of finding a small set of most influential nodes in a social network so that their aggregated influence in the network is maximized. In this paper, we study influence maximization in the linear threshold model, one of the important models formalizing the behavior of influence propagation in social networks. We first show that computing exact influence in general networks in the linear threshold model is #P-hard, which closes an open problem left in the seminal work on influence maximization by Kempe, Kleinberg, and Tardos, 2003. As a contrast, we show that computing influence in directed a cyclic graphs (DAGs) can be done in time linear to the size of the graphs. Based on the fast computation in DAGs, we propose the first scalable influence maximization algorithm tailored for the linear threshold model. We conduct extensive simulations to show that our algorithm is scalable to networks with millions of nodes and edges, is orders of magnitude faster than the greedy approximation algorithm proposed by Kempe et al. and its optimized versions, and performs consistently among the best algorithms while other heuristic algorithms not design specifically for the linear threshold model have unstable performances on different real-world networks.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[influence maximization, social networks, linear threshold model]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530424</person_id>
				<author_profile_id><![CDATA[81444596101]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530425</person_id>
				<author_profile_id><![CDATA[81479642553]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yifei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yuan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530426</person_id>
				<author_profile_id><![CDATA[81479641888]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Li]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934562</article_id>
		<sort_key>220</sort_key>
		<display_label>Pages</display_label>
		<pages>98-107</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[CLUSMASTER]]></title>
		<subtitle><![CDATA[A Clustering Approach for Sampling Data Streams in Sensor Networks]]></subtitle>
		<page_from>98</page_from>
		<page_to>107</page_to>
		<doi_number>10.1109/ICDM.2010.32</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934562</url>
		<abstract>
			<par><![CDATA[The growing usage of embedded devices and sensors in our daily lives has been profoundly reshaping the way we interact with our environment and our peers. As more and more sensors will pervade our future cities, increasingly efficient infrastructures to collect, process, and store massive amounts of data streams from a wide variety of sources will be required. Despite the different application-specific features and hardware platforms, sensor network applications share a common goal: periodically sample and store data collected from different sensors in a common persistent memory. In this article we present a clustering approach for rapidly and efficiently computing the best sampling rate which minimizes the SSE (Sum of Square Errors) for each particular sensor in a network. In order to evaluate the efficiency of the proposed approach, we carried out experiments on real electric power consumption data streams produced by a 1-thousand sensor network provided by the French energy group &#8211; EDF (Electricite de France).]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[clustering, data streams, sampling, sensor network]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530427</person_id>
				<author_profile_id><![CDATA[81317492993]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alzennyr]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Da Silva]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530428</person_id>
				<author_profile_id><![CDATA[81384602315]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Raja]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chiky]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530429</person_id>
				<author_profile_id><![CDATA[81100617740]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Georges]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hebrail]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934563</article_id>
		<sort_key>230</sort_key>
		<display_label>Pages</display_label>
		<pages>108-117</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[Bayesian Maximum Margin Clustering]]></title>
		<page_from>108</page_from>
		<page_to>117</page_to>
		<doi_number>10.1109/ICDM.2010.117</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934563</url>
		<abstract>
			<par><![CDATA[Most well-known discriminative clustering models, such as spectral clustering (SC) and maximum margin clustering (MMC), are non-Bayesian. Moreover, they merely considered to embed domain-dependent prior knowledge into data-specific kernels, while other forms of prior knowledge were seldom considered in these models. In this paper, we propose a Bayesian maximum margin clustering model (BMMC) based on the low-density separation assumption, which unifies the merits of both Bayesian and discriminative approaches. In addition to stating prior distribution on functions explicitly as traditional Gaussian processes, special prior knowledge can be embedded into BMMC implicitly via the Universum set easily. Furthermore, it is much easier to solve a BMMC than an MMC since the integer variables in the optimization are eliminated. Experimental results show that the BMMC achieves comparable or even better performance than state-of-the-art clustering methods and solving BMMC is more efficiently.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Clustering, Bayesian, Maximum Margin Principle, Universum]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533057</person_id>
				<author_profile_id><![CDATA[81479657098]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533058</person_id>
				<author_profile_id><![CDATA[81479650500]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Baogang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533059</person_id>
				<author_profile_id><![CDATA[81479643121]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Niu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934570</article_id>
		<sort_key>240</sort_key>
		<display_label>Pages</display_label>
		<pages>118-127</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[Viral Marketing for Multiple Products]]></title>
		<page_from>118</page_from>
		<page_to>127</page_to>
		<doi_number>10.1109/ICDM.2010.52</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934570</url>
		<abstract>
			<par><![CDATA[Viral Marketing, the idea of exploiting social interactions of users to propagate awareness for products, has gained considerable focus in recent years. One of the key issues in this area is to select the best seeds that maximize the influence propagated in the social network. In this paper, we define the seed selection problem (called t-Influence Maximization, or t-IM) for multiple products. Specifically, given the social network and t products along with their seed requirements, we want to select seeds for each product that maximize the overall influence. As the seeds are typically sent promotional messages, to avoid spamming users, we put a hard constraint on the number of products for which any single user can be selected as a seed. In this paper, we design two efficient techniques for the t-IM problem, called Greedy and FairGreedy. The Greedy algorithm uses simple greedy hill climbing, but still results in a 1/3-approximation to the optimum. Our second technique, FairGreedy, allocates seeds with not only high overall influence (close to Greedy in practice), but also ensures fairness across the influence of different products. We also design efficient heuristics for estimating the influence of the selected seeds, that are crucial for running the seed selection on large social network graphs. Finally, using extensive simulations on real-life social graphs, we show the effectiveness and scalability of our techniques compared to existing and naive strategies.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[viral marketing, influence propagation, social networks]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532599</person_id>
				<author_profile_id><![CDATA[81479644342]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Samik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Datta]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532600</person_id>
				<author_profile_id><![CDATA[81351598081]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Anirban]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Majumder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532601</person_id>
				<author_profile_id><![CDATA[81342511241]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nisheeth]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shrivastava]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934571</article_id>
		<sort_key>250</sort_key>
		<display_label>Pages</display_label>
		<pages>128-137</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[Finding Local Anomalies in Very High Dimensional Space]]></title>
		<page_from>128</page_from>
		<page_to>137</page_to>
		<doi_number>10.1109/ICDM.2010.151</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934571</url>
		<abstract>
			<par><![CDATA[Time, cost and energy efficiency are critical factors for many data analysis techniques when the size and dimensionality of data is very large. We investigate the use of Local Outlier Factor (LOF) for data of this type, providing a motivating example from real world data. We propose Projection-Indexed Nearest-Neighbours (PINN), a novel technique that exploits extended nearest neighbour sets in the a reduced dimensional space to create an accurate approximation for k-nearest-neighbour distances, which is used as the core density measurement within LOF. The reduced dimensionality allows for efficient sub-quadratic indexing in the number of items in the data set, where previously only quadratic performance was possible. A detailed theoretical analysis of Random Projection(RP) and PINN shows that we are able to preserve the density of the intrinsic manifold of the data set after projection. Experimental results show that PINN outperforms the standard projection methods RP and PCA when measuring LOF for many high-dimensional real-world data sets of up to 300000 elements and 102600 dimensions.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[anomaly detection, dimensionality reduction]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530983</person_id>
				<author_profile_id><![CDATA[81447593988]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Timothy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[de Vries]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530984</person_id>
				<author_profile_id><![CDATA[81100002847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sanjay]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chawla]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530985</person_id>
				<author_profile_id><![CDATA[81100299673]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Houle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934572</article_id>
		<sort_key>260</sort_key>
		<display_label>Pages</display_label>
		<pages>138-147</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>23</seq_no>
		<title><![CDATA[PGLCM]]></title>
		<subtitle><![CDATA[Efficient Parallel Mining of Closed Frequent Gradual Itemsets]]></subtitle>
		<page_from>138</page_from>
		<page_to>147</page_to>
		<doi_number>10.1109/ICDM.2010.101</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934572</url>
		<abstract>
			<par><![CDATA[Numerical data (e.g., DNA micro-array data, sensor data) pose a challenging problem to existing frequent pattern mining methods which hardly handle them. In this framework, gradual patterns have been recently proposed to extract covariations of attributes, such as: "When X increases, Y decreases". There exist some algorithms for mining frequent gradual patterns, but they cannot scale to real-world databases. We present in this paper GLCM, the first algorithm for mining closed frequent gradual patterns, which proposes strong complexity guarantees: the mining time is linear with the number of closed frequent gradual item sets. Our experimental study shows that GLCM is two orders of magnitude faster than the state of the art, with a constant low memory usage. We also present PGLCM, a parallelization of GLCM capable of exploiting multicore processors, with good scale-up properties on complex datasets. These algorithms are the first algorithms capable of mining large real world datasets to discover gradual patterns.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Data mining, frequent pattern mining, gradual itemsets, parallelism]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531508</person_id>
				<author_profile_id><![CDATA[81479643042]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Trong]]></first_name>
				<middle_name><![CDATA[Dinh Thac]]></middle_name>
				<last_name><![CDATA[Do]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531509</person_id>
				<author_profile_id><![CDATA[81332511032]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Anne]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Laurent]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531510</person_id>
				<author_profile_id><![CDATA[81100245571]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Alexandre]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Termier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934573</article_id>
		<sort_key>270</sort_key>
		<display_label>Pages</display_label>
		<pages>148-157</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>24</seq_no>
		<title><![CDATA[Sequential Latent Dirichlet Allocation]]></title>
		<subtitle><![CDATA[Discover Underlying Topic Structures within a Document]]></subtitle>
		<page_from>148</page_from>
		<page_to>157</page_to>
		<doi_number>10.1109/ICDM.2010.51</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934573</url>
		<abstract>
			<par><![CDATA[Understanding how topics within a document evolve over its structure is an interesting and important problem. In this paper, we address this problem by presenting a novel variant of Latent Dirichlet Allocation (LDA): Sequential LDA (SeqLDA). This variant directly considers the underlying sequential structure, {\it i.e.}, a document consists of multiple segments ({\it e.g.}, chapters, paragraphs), each of which is correlated to its previous and subsequent segments. In our model, a document and its segments are modelled as random mixtures of the same set of latent topics, each of which is a distribution over words, and the topic distribution of each segment depends on that of its previous segment, the one for first segment will depend on the document topic distribution. The progressive dependency is captured by using the nested two-parameter Poisson Dirichlet process (PDP). We develop an efficient collapsed Gibbs sampling algorithm to sample from the posterior of the PDP. Our experimental results on patent documents show that by taking into account the sequential structure within a document, our SeqLDA model has a higher fidelity over LDA in terms of perplexity (a standard measure of dictionary-based compressibility). The SeqLDA model also yields a nicer sequential topic structure than LDA, as we show in experiments on books such as Melville's "The Whale''.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Latent Dirichlet Allocation, Poisson-Dirichlet process, collapsed Gibbs sampler, document structure]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2529918</person_id>
				<author_profile_id><![CDATA[81546874056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Du]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529919</person_id>
				<author_profile_id><![CDATA[81100245904]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wray]]></first_name>
				<middle_name><![CDATA[Lindsay]]></middle_name>
				<last_name><![CDATA[Buntine]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529920</person_id>
				<author_profile_id><![CDATA[81479652934]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Huidong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934579</article_id>
		<sort_key>280</sort_key>
		<display_label>Pages</display_label>
		<pages>158-167</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>25</seq_no>
		<title><![CDATA[Subgroup Discovery Meets Bayesian Networks -- An Exceptional Model Mining Approach]]></title>
		<page_from>158</page_from>
		<page_to>167</page_to>
		<doi_number>10.1109/ICDM.2010.53</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934579</url>
		<abstract>
			<par><![CDATA[Whenever a dataset has multiple discrete target variables, we want our algorithms to consider not only the variables themselves, but also the interdependencies between them. We propose to use these interdependencies to quantify the quality of subgroups, by integrating Bayesian networks with the Exceptional Model Mining framework. Within this framework, candidate subgroups are generated. For each candidate, we fit a Bayesian network on the target variables. Then we compare the network&#8217;s structure to the structure of the Bayesian network fitted on the whole dataset. To perform this comparison, we define an edit distance-based distance metric that is appropriate for Bayesian networks. We show interesting subgroups that we experimentally found with our method on datasets from music theory, semantic scene classification, biology and zoogeography.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Exceptional Model Mining, Subgroup Discovery, Bayesian networks]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531018</person_id>
				<author_profile_id><![CDATA[81384596325]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wouter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Duivesteijn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531019</person_id>
				<author_profile_id><![CDATA[81100563482]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Arno]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Knobbe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531020</person_id>
				<author_profile_id><![CDATA[81100162409]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ad]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Feelders]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531021</person_id>
				<author_profile_id><![CDATA[81335498822]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Matthijs]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[van Leeuwen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934545</article_id>
		<sort_key>290</sort_key>
		<display_label>Pages</display_label>
		<pages>168-175</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>26</seq_no>
		<title><![CDATA[Feature Selection for Unsupervised Learning Using Random Cluster Ensembles]]></title>
		<page_from>168</page_from>
		<page_to>175</page_to>
		<doi_number>10.1109/ICDM.2010.137</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934545</url>
		<abstract>
			<par><![CDATA[In this paper, we propose another extension of the Random Forests paradigm to unlabeled data, leading to localized unsupervised feature selection (FS). We show that the way internal estimates are used to measure variable importance in Random Forests are also applicable to FS in unsupervised learning. We first illustrate the clustering performance of the proposed method on various data sets based on widely used external criteria of clustering quality. We then assess the accuracy and the scalability of the FS procedure on UCI and real labeled data sets and compare its effectiveness against other FS methods.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Unsupervised learning, feature selection, Random Forest]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531407</person_id>
				<author_profile_id><![CDATA[81387607440]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Haytham]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Elghazel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531408</person_id>
				<author_profile_id><![CDATA[81100175657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alex]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aussem]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934580</article_id>
		<sort_key>300</sort_key>
		<display_label>Pages</display_label>
		<pages>176-185</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>27</seq_no>
		<title><![CDATA[Learning Attribute-to-Feature Mappings for Cold-Start Recommendations]]></title>
		<page_from>176</page_from>
		<page_to>185</page_to>
		<doi_number>10.1109/ICDM.2010.129</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934580</url>
		<abstract>
			<par><![CDATA[Cold-start scenarios in recommender systems are situations in which no prior events, like ratings or clicks, are known for certain users or items. To compute predictions in such cases, additional information about users (user attributes, e.g. gender, age, geographical location, occupation) and items (item attributes, e.g. genres, product categories, keywords) must be used. We describe a method that maps such entity (e.g. user or item) attributes to the latent features of a matrix (or higher-dimensional) factorization model. With such mappings, the factors of a MF model trained by standard techniques can be applied to the new-user and the new-item problem, while retaining its advantages, in particular speed and predictive accuracy. We use the mapping concept to construct an attribute-aware matrix factorization model for item recommendation from implicit, positive-only feedback. Experiments on the new-item problem show that this approach provides good predictive accuracy, while the prediction time only grows by a constant factor.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[collaborative filtering, cold-start, matrix factorization, factorization models, long tail, recommender systems]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2529949</person_id>
				<author_profile_id><![CDATA[81458655614]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Zeno]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gantner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529950</person_id>
				<author_profile_id><![CDATA[81385602017]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Lucas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Drumond]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529951</person_id>
				<author_profile_id><![CDATA[81460654833]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Christoph]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Freudenthaler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529952</person_id>
				<author_profile_id><![CDATA[81321497327]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Steffen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rendle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529953</person_id>
				<author_profile_id><![CDATA[81332525921]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Lars]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schmidt-Thieme]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934581</article_id>
		<sort_key>310</sort_key>
		<display_label>Pages</display_label>
		<pages>186-195</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>28</seq_no>
		<title><![CDATA[An Extensive Empirical Study on Semi-supervised Learning]]></title>
		<page_from>186</page_from>
		<page_to>195</page_to>
		<doi_number>10.1109/ICDM.2010.66</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934581</url>
		<abstract>
			<par><![CDATA[Semi-supervised classification methods utilize unlabeled data to help learn better classifiers, when only a small amount of labeled data is available. Many semi-supervised learning methods have been proposed in the past decade. However, some questions have not been well answered, e.g., whether semi-supervised learning methods outperform base classifiers learned only from the labeled data, when different base classifiers are used, whether selecting unlabeled data with efforts is superior to random selection, and how the quality of the learned classifier changes at each iteration of learning process. This paper conducts an extensive empirical study on the performance of several commonly used semi-supervised learning methods when different Bayesian classifiers (NB, NBTree, TAN, HGC, HNB, and DNB) are used as the base classifier, respectively. Results on Transductive SVM and a graph-based semi-supervised learning method LLGC are also studied for comparison. The experimental results on 26 UCI datasets and 6 widely used benchmark datasets show that these semi-supervised learning methods generally do not obtain better performance than classifiers learned only from the labeled data. Moreover, for standard self-training and co-training, when selecting the most confident unlabeled instances during learning process, the performance is not necessarily better than that of random selection of unlabeled instances. We especially discovered interesting outcomes when drawing learning curves for using NB in self-training on some UCI datasets. The accuracy of the learned classifier on the testing set may fluctuate or decrease as more unlabeled instances are used. Also on the mushroom dataset, even when all the selected unlabeled instances are correctly labeled in each iteration, the accuracy on the testing set still goes down.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Semi-supervised learning, Bayesian classifiers]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531531</person_id>
				<author_profile_id><![CDATA[81488655892]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuanyuan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531532</person_id>
				<author_profile_id><![CDATA[81479645818]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Xiaoda]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Niu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531533</person_id>
				<author_profile_id><![CDATA[81100120645]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Harry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934582</article_id>
		<sort_key>320</sort_key>
		<display_label>Pages</display_label>
		<pages>196-205</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>29</seq_no>
		<title><![CDATA[Efficient Discovery of the Top-K Optimal Dependency Rules with Fisher's Exact Test of Significance]]></title>
		<page_from>196</page_from>
		<page_to>205</page_to>
		<doi_number>10.1109/ICDM.2010.143</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934582</url>
		<abstract>
			<par><![CDATA[Statistical dependency analysis is the basis of all empirical science. A commonly occurring problem is to find the most significant dependency rules, which describe either positive or negative dependencies between categorical attributes. For example, in medical science one is interested in genetic factors, which can either predispose or prevent diseases. The requirement of statistical significance is essential, because the discoveries should hold also in the future data. Typically, the significance is estimated either by Fisher's exact test or the $\chi^2$-measure. The problem is computationally very difficult, because the number of all possible dependency rules increases exponentially with the number of attributes. As a solution, different kinds of restrictions and heuristics have been applied, but a general, scalable search method has been missing. In this paper, we introduce an efficient algorithm for searching for the top-K globally optimal dependency rules using Fisher's exact test as a measure function. The rules can express either positive or negative dependencies between a set of positive attributes and a single consequent attribute. The algorithm is based on an application of the branch-and-bound search strategy, supplemented by several pruning properties. Especially, we prove a new lower-bound for the Fisher's p, and introduce a new effective pruning principle. The general search algorithm is applicable to other goodness measures, like the $\chi^2$-measure, as well. According to our experiments on classical benchmark data, the algorithm is well scalable and can efficiently handle even dense and high dimensional data sets. In addition, the quality of rules is significantly better than with the $\chi^2$-measure using the same search algorithm.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[dependency rule, negative rule, statistical significance, Fisher's exact test, rule discovery]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533682</person_id>
				<author_profile_id><![CDATA[81464646148]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wilhelmiina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hamalainen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934590</article_id>
		<sort_key>330</sort_key>
		<display_label>Pages</display_label>
		<pages>206-215</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>30</seq_no>
		<title><![CDATA[A Variance Reduction Framework for Stable Feature Selection]]></title>
		<page_from>206</page_from>
		<page_to>215</page_to>
		<doi_number>10.1109/ICDM.2010.144</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934590</url>
		<abstract>
			<par><![CDATA[Besides high accuracy, stability of feature selection has recently attracted strong interest in knowledge discovery from high-dimensional data. In this study, we present a theoretical framework about the relationship between the stability and accuracy of feature selection based on a formal bias-variance decomposition of feature selection error. The framework also suggests a variance reduction approach for improving the stability of feature selection algorithms. Furthermore, we propose an empirical variance reduction framework, margin based instance weighting, which weights training instances according to their influence to the estimation of feature relevance. We also develop an efficient algorithm under this framework. Experiments based on synthetic data and real-world micro array data verify both the theoretical framework and the effectiveness of the proposed algorithm on variance reduction. The proposed algorithm is also shown to be effective at improving subset stability, while maintaining comparable classification accuracy based on selected features.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[feature selection, stability, bias-variance decomposition, variance reduction, high-dimensional data]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532123</person_id>
				<author_profile_id><![CDATA[81474672826]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yue]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Han]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532124</person_id>
				<author_profile_id><![CDATA[81452597347]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Lei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934583</article_id>
		<sort_key>340</sort_key>
		<display_label>Pages</display_label>
		<pages>216-225</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>31</seq_no>
		<title><![CDATA[Exponential Family Tensor Factorization for Missing-Values Prediction and Anomaly Detection]]></title>
		<page_from>216</page_from>
		<page_to>225</page_to>
		<doi_number>10.1109/ICDM.2010.39</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934583</url>
		<abstract>
			<par><![CDATA[In this paper, we study probabilistic modeling of heterogeneously attributed multi-dimensional arrays. The model can manage the heterogeneity by employing an individual exponential-family distribution for each attribute of the tensor array. These entries are connected by latent variables and are shared information across the different attributes. Because a Bayesian inference for our model is intractable, we cast the EM algorithm approximated by using the Lap lace method and Gaussian process. This approximation enables us to derive a predictive distribution for missing values in a consistent manner. Simulation experiments show that our method outperforms other methods such as PARAFAC and Tucker decomposition in missing-values prediction for cross-national statistics and is also applicable to discover anomalies in heterogeneous office-logging data.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[tensor factorization, Bayesian probabilistic model, Gaussian process, data fusion]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2529954</person_id>
				<author_profile_id><![CDATA[81418593749]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kohei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hayashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529955</person_id>
				<author_profile_id><![CDATA[81309492625]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takenouchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529956</person_id>
				<author_profile_id><![CDATA[81100165699]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tomohiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shibata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529957</person_id>
				<author_profile_id><![CDATA[81539793756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kamiya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529958</person_id>
				<author_profile_id><![CDATA[81100330650]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Daishi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529959</person_id>
				<author_profile_id><![CDATA[81100301137]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Kazuo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kunieda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529960</person_id>
				<author_profile_id><![CDATA[81100354539]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Keiji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529961</person_id>
				<author_profile_id><![CDATA[81100469577]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Kazushi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ikeda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934591</article_id>
		<sort_key>350</sort_key>
		<display_label>Pages</display_label>
		<pages>226-235</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>32</seq_no>
		<title><![CDATA[Rare Category Characterization]]></title>
		<page_from>226</page_from>
		<page_to>235</page_to>
		<doi_number>10.1109/ICDM.2010.154</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934591</url>
		<abstract>
			<par><![CDATA[Rare categories abound and their characterization has heretofore received little attention. Fraudulent banking transactions, network intrusions, and rare diseases are examples of rare classes whose detection and characterization are of high value. However, accurate characterization is challenging due to high-skewness and non-separability from majority classes, e.g., fraudulent transactions masquerade as legitimate ones. This paper proposes the RACH algorithm by exploring the compactness property of the rare categories. It is based on an optimization framework which encloses the rare examples by a minimum-radius hyper ball. The framework is then converted into a convex optimization problem, which is in turn effectively solved in its dual form by the projected sub gradient method. RACH can be naturally kernelized. Experimental results validate the effectiveness of RACH.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[rare category, minority class, characterization, compactness, optimization, hyperball, subgradient]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532125</person_id>
				<author_profile_id><![CDATA[81540269856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jingrui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[He]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532126</person_id>
				<author_profile_id><![CDATA[81337494052]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hanghang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532127</person_id>
				<author_profile_id><![CDATA[81452611253]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jaime]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Carbonell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934592</article_id>
		<sort_key>360</sort_key>
		<display_label>Pages</display_label>
		<pages>236-245</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>33</seq_no>
		<title><![CDATA[Algorithm for Discovering Low-Variance 3-Clusters from Real-Valued Datasets]]></title>
		<page_from>236</page_from>
		<page_to>245</page_to>
		<doi_number>10.1109/ICDM.2010.77</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934592</url>
		<abstract>
			<par><![CDATA[The concept of Triclusters has been investigated recently in the context of two relational datasets that share labels along one of the dimensions. By simultaneously processing two datasets to unveil triclusters, new useful knowledge and insights can be obtained. However, some recently reported methods are either closely linked to specific problems or constrain datasets to have some specific distributions. Algorithms for generating triclusters whose cell-values demonstrate simple well known statistical properties, such as upper bounds on standard deviations, are needed for many applications. In this paper we present a 3-Clustering algorithm that searches for meaningful combinations of biclusters in two related datasets. The algorithm can handle situations involving: (i) datasets in which a few data objects may be present in only one dataset and not in both datasets, (ii) the two datasets may have different numbers of objects and/or attributes, and (iii) the cell-value distributions in two datasets may be different. In our formulation the cell-values of each selected tricluster, formed by two independent biclusters, are such that the standard deviations in each bicluster obeys an upper bound and the sets of objects in the two biclusters overlap to the maximum possible extent. We present validation of our algorithm by presenting the properties of the 3-Clusters discovered from a synthetic dataset and from a real world cross-species genomic dataset. The results of our algorithm unveil interesting insights for the cross-species genomic domain.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Triclusters, Co-clustering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532128</person_id>
				<author_profile_id><![CDATA[81479641937]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Zhen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532129</person_id>
				<author_profile_id><![CDATA[81100498916]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Raj]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bhatnagar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934593</article_id>
		<sort_key>370</sort_key>
		<display_label>Pages</display_label>
		<pages>246-255</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>34</seq_no>
		<title><![CDATA[Improved Consistent Sampling, Weighted Minhash and L1 Sketching]]></title>
		<page_from>246</page_from>
		<page_to>255</page_to>
		<doi_number>10.1109/ICDM.2010.80</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934593</url>
		<abstract>
			<par><![CDATA[We propose a new Consistent Weighted Sampling method, where the probability of drawing identical samples for a pair of inputs is equal to their Jaccard similarity. Our method takes deterministic constant time per non-zero weight, improving on the best previous approach which takes expected constant time. The samples can be used as Weighted Minhash for efficient retrieval and compression (sketching) under Jaccard or L1 metric. A method is presented for using simple data statistics to reduce the running time of hash computation by two orders of magnitude. We compare our method with the random projection method and show that it has better characteristics for retrieval under L1. We present a novel method of mapping hashes to short bit-strings, apply it to Weighted Minhash, and achieve more accurate distance estimates from sketches than existing methods, as long as the inputs are sufficiently distinct. We show how to choose the optimal number of bits per hash for sketching, and demonstrate experimental results which agree with the theoretical analysis.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Sampling, Hashing, Sketching, Retrieval, Compression, Minhash]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532658</person_id>
				<author_profile_id><![CDATA[81493645717]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sergey]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ioffe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934594</article_id>
		<sort_key>380</sort_key>
		<display_label>Pages</display_label>
		<pages>256-265</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>35</seq_no>
		<title><![CDATA[An Approach Based on Tree Kernels for Opinion Mining of Online Product Reviews]]></title>
		<page_from>256</page_from>
		<page_to>265</page_to>
		<doi_number>10.1109/ICDM.2010.104</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934594</url>
		<abstract>
			<par><![CDATA[Opinion mining is a challenging task to identify the opinions or sentiments underlying user generated contents, such as online product reviews, blogs, discussion forums, etc. Previous studies that adopt machine learning algorithms mainly focus on designing effective features for this complex task. This paper presents our approach based on tree kernels for opinion mining of online product reviews. Tree kernels alleviate the complexity of feature selection and generate effective features to satisfy the special requirements in opinion mining. In this paper, we define several tree kernels for sentiment expression extraction and sentiment classification, which are subtasks of opinion mining. Our proposed tree kernels encode not only syntactic structure information, but also sentiment related information, such as sentiment boundary and sentiment polarity, which are important features to opinion mining. Experimental results on a benchmark data set indicate that tree kernels can significantly improve the performance of both sentiment expression extraction and sentiment classification. Besides, a linear combination of our proposed tree kernels and traditional feature vector kernel achieves the best performances using the benchmark data set.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[opinion mining, sentiment analysis, tree kernels, text mining]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531575</person_id>
				<author_profile_id><![CDATA[81314482711]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Peng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jiang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531576</person_id>
				<author_profile_id><![CDATA[81453613200]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chunxia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531577</person_id>
				<author_profile_id><![CDATA[81479656513]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hongping]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531578</person_id>
				<author_profile_id><![CDATA[81322502637]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Zhendong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Niu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531579</person_id>
				<author_profile_id><![CDATA[81479650187]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Qing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934601</article_id>
		<sort_key>390</sort_key>
		<display_label>Pages</display_label>
		<pages>266-273</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>36</seq_no>
		<title><![CDATA[A Pairwise-Systematic Microaggregation for Statistical Disclosure Control]]></title>
		<page_from>266</page_from>
		<page_to>273</page_to>
		<doi_number>10.1109/ICDM.2010.111</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934601</url>
		<abstract>
			<par><![CDATA[Microdata protection in statistical databases has recently become a major societal concern and has been intensively studied in recent years. Statistical Disclosure Control (SDC) is often applied to statistical databases before they are released for public use. Micro aggregation for SDC is a family of methods to protect micro data from individual identification. SDC seeks to protect micro data in such a way that can be published and mined without providing any private information that can be linked to specific individuals. Micro aggregation works by partitioning the micro data into groups of at least k records and then replacing the records in each group with the centroid of the group. An optimal micro aggregation method must minimize the information loss resulting from this replacement process. The challenge is how to minimize the information loss during the micro aggregation process. This paper presents a pair wise systematic (P-S) micro aggregation method to minimize the information loss. The proposed technique simultaneously forms two distant groups at a time with the corresponding similar records together in a systematic way and then anonymized with the centroid of each group individually. The structure of P-S problem is defined and investigated and an algorithm of the proposed problem is developed. The performance of the P-S algorithm is compared against the most recent micro aggregation methods. Experimental results show that P-S algorithm incurs less than half information loss than the latest micro aggregation methods for all of the test situations.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Privacy, Microaggregation, Microdata protection, ?-anonymity, Disclosure control]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531108</person_id>
				<author_profile_id><![CDATA[81470651464]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Md.]]></first_name>
				<middle_name><![CDATA[Enamul]]></middle_name>
				<last_name><![CDATA[Kabir]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531109</person_id>
				<author_profile_id><![CDATA[81474666726]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hua]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531110</person_id>
				<author_profile_id><![CDATA[81487649465]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yanchun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934602</article_id>
		<sort_key>400</sort_key>
		<display_label>Pages</display_label>
		<pages>274-283</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>37</seq_no>
		<title><![CDATA[Multi-label Feature Selection for Graph Classification]]></title>
		<page_from>274</page_from>
		<page_to>283</page_to>
		<doi_number>10.1109/ICDM.2010.58</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934602</url>
		<abstract>
			<par><![CDATA[Nowadays, the classification of graph data has become an important and active research topic in the last decade, which has a wide variety of real world applications, e.g. drug activity predictions and kinase inhibitor discovery. Current research on graph classification focuses on single-label settings. However, in many applications, each graph data can be assigned with a set of multiple labels simultaneously. Extracting good features using multiple labels of the graphs becomes an important step before graph classification. In this paper, we study the problem of multi-label feature selection for graph classification and propose a novel solution, called gMLC, to efficiently search for optimal sub graph features for graph objects with multiple labels. Different from existing feature selection methods in vector spaces which assume the feature set is given, we perform multi-label feature selection for graph data in a progressive way together with the sub graph feature mining process. We derive an evaluation criterion, named gHSIC, to estimate the dependence between sub graph features and multiple labels of graphs. Then a branch-and-bound algorithm is proposed to efficiently search for optimal sub graph features by judiciously pruning the sub graph search space using multiple labels. Empirical studies on real-world tasks demonstrate that our feature selection approach can effectively boost multi-label graph classification performances and is more efficient by pruning the sub graph search space using multiple labels.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[feature selection, graph classification, multi-label learning]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531601</person_id>
				<author_profile_id><![CDATA[81466643630]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xiangnan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531602</person_id>
				<author_profile_id><![CDATA[81350576309]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Philip]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934603</article_id>
		<sort_key>410</sort_key>
		<display_label>Pages</display_label>
		<pages>284-293</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>38</seq_no>
		<title><![CDATA[A Binary Decision Diagram-Based One-Class Classifier]]></title>
		<page_from>284</page_from>
		<page_to>293</page_to>
		<doi_number>10.1109/ICDM.2010.84</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934603</url>
		<abstract>
			<par><![CDATA[We propose a novel approach for one-class classification problems where a logical formula is used to estimate the region that covers all examples. A formula is viewed as a model that represents a region and is approximated with respect to its hierarchical local densities. The approximation is done quite efficiently via direct manipulations of a binary decision diagram that is a compressed representation of a Boolean formula. The proposed method has only one parameter to be tuned, and the parameter can be selected properly with the help of the minimum description length principle, which requires no labeled training data. In other words, a one-class classifier is generated from an unlabeled training data thoroughly and automatically. Experimental results show that the proposed method works quite well with synthetic data and some realistic data.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[one-class classification, binary decision diagram, minimum description length principle]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533203</person_id>
				<author_profile_id><![CDATA[81479662263]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takuro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kutsuna]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934604</article_id>
		<sort_key>420</sort_key>
		<display_label>Pages</display_label>
		<pages>294-303</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>39</seq_no>
		<title><![CDATA[Detecting Blackhole and Volcano Patterns in Directed Networks]]></title>
		<page_from>294</page_from>
		<page_to>303</page_to>
		<doi_number>10.1109/ICDM.2010.37</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934604</url>
		<abstract>
			<par><![CDATA[In this paper, we formulate a novel problem for finding black hole and volcano patterns in a large directed graph. Specifically, a black hole pattern is a group which is made of a set of nodes in a way such that there are only in links to this group from the rest nodes in the graph. In contrast, a volcano pattern is a group which only has out links to the rest nodes in the graph. Both patterns can be observed in real world. For instance, in a trading network, a black hole pattern may represent a group of traders who are manipulating the market. In the paper, we first prove that the black hole mining problem is a dual problem of finding volcanoes. Therefore, we focus on finding the black hole patterns. Along this line, we design two pruning schemes to guide the black hole finding process. In the first pruning scheme, we strategically prune the search space based on a set of pattern-size-independent pruning rules and develop an iBlack hole algorithm. The second pruning scheme follows a divide-and-conquer strategy to further exploit the pruning results from the first pruning scheme. Indeed, a target directed graphs can be divided into several disconnected sub graphs by the first pruning scheme, and thus the black hole finding can be conducted in each disconnected sub graph rather than in a large graph. Based on these two pruning schemes, we also develop an iBlackhole-DC algorithm. Finally, experimental results on real-world data show that the iBlackhole-DC algorithm can be several orders of magnitude faster than the iBlackhole algorithm, which has a huge computational advantage over a brute-force method.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[blackhole pattern, volcano pattern, fraud detection, graph mining, network model]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531603</person_id>
				<author_profile_id><![CDATA[81479642616]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Zhongmou]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531604</person_id>
				<author_profile_id><![CDATA[81451596433]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xiong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531605</person_id>
				<author_profile_id><![CDATA[81479649388]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yanchi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531606</person_id>
				<author_profile_id><![CDATA[81479652077]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Aoying]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934611</article_id>
		<sort_key>430</sort_key>
		<display_label>Pages</display_label>
		<pages>304-313</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>40</seq_no>
		<title><![CDATA[Exploiting Local Data Uncertainty to Boost Global Outlier Detection]]></title>
		<page_from>304</page_from>
		<page_to>313</page_to>
		<doi_number>10.1109/ICDM.2010.10</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934611</url>
		<abstract>
			<par><![CDATA[This paper presents a novel hybrid approach to outlier detection by incorporating local data uncertainty into the construction of a global classifier. To deal with local data uncertainty, we introduce a confidence value to each data example in the training data, which measures the strength of the corresponding class label. Our proposed method works in two steps. Firstly, we generate a pseudo training dataset by computing a confidence value of each input example on its class label. We present two different mechanisms: kernel k-means clustering algorithm and kernel LOF-based algorithm, to compute the confidence values based on the local data behavior. Secondly, we construct a global classifier for outlier detection by generalizing the SVDD-based learning framework to incorporate both positive and negative examples as well as their associated confidence values. By integrating local and global outlier detection, our proposed method explicitly handles the uncertainty of the input data and enhances the ability of SVDD in reducing the sensitivity to noise. Extensive experiments on real life datasets demonstrate that our proposed method can achieve a better tradeoff between detection rate and false alarm rate as compared to four state-of-the-art outlier detection algorithms.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Outlier detection, Data uncertainty, SVDD]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532193</person_id>
				<author_profile_id><![CDATA[81328489008]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532194</person_id>
				<author_profile_id><![CDATA[81479658992]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532195</person_id>
				<author_profile_id><![CDATA[81472652199]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yanshan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xiao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532196</person_id>
				<author_profile_id><![CDATA[81451595792]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Longbing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532197</person_id>
				<author_profile_id><![CDATA[81350576309]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Philip]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934612</article_id>
		<sort_key>440</sort_key>
		<display_label>Pages</display_label>
		<pages>314-323</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>41</seq_no>
		<title><![CDATA[Training Conditional Random Fields Using Transfer Learning for Gesture Recognition]]></title>
		<page_from>314</page_from>
		<page_to>323</page_to>
		<doi_number>10.1109/ICDM.2010.31</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934612</url>
		<abstract>
			<par><![CDATA[Recently, combining Conditional Random Fields (CRF) with Neural Network has shown the success of learning high-level features in sequence labeling tasks. However, such models are difficult to train because of the increase of the parameters to tune which needs enormous of labeled data to avoid over fitting. In this paper, we propose a transfer learning framework for the sequence labeling task of gesture recognition. Taking advantage of the frame correlation, we design an unsupervised sequence model as a pseudo auxiliary task to capture the underlying information from both the labeled and unlabeled data. The knowledge learnt by the auxiliary task can be transferred to the main task of CRF with a deep architecture by sharing the hidden layers, which is very helpful for learning meaningful representation and reducing the need of labeled data. We evaluate our model under 3 gesture recognition datasets. The experimental results of both supervised learning and semi-supervised learning show that the proposed model improves the performance of the CRF with Neural Network and other baseline models.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Conditional Random Fields, Transfer Learning, Semi-supervised Learning, Gesture Recognition]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530581</person_id>
				<author_profile_id><![CDATA[81488650664]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530582</person_id>
				<author_profile_id><![CDATA[81479645280]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530583</person_id>
				<author_profile_id><![CDATA[81407593276]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530584</person_id>
				<author_profile_id><![CDATA[81423595577]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yalou]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934613</article_id>
		<sort_key>450</sort_key>
		<display_label>Pages</display_label>
		<pages>324-333</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>42</seq_no>
		<title><![CDATA[Stratified Sampling for Data Mining on the Deep Web]]></title>
		<page_from>324</page_from>
		<page_to>333</page_to>
		<doi_number>10.1109/ICDM.2010.17</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934613</url>
		<abstract>
			<par><![CDATA[In recent years, one mode of data dissemination has become extremely popular, which is the deep web. Like any other data source, data mining on the deep web can produce important insights or summary of results. However, data mining on the deep web is challenging because the databases cannot be accessed directly, and therefore, data mining must be performed based on sampling of the datasets. The samples, in turn, can only be obtained by querying the deep web databases with specific inputs. In this paper, we target two related data mining problems, which are association mining and differential rule mining. We develop stratified sampling methods to perform these mining tasks on a deep web source. Our contributions include a novel greedy stratification approach, which processes the query space of a deep web data source recursively, and considers both the estimation error and the sampling costs. We have also developed an optimized sample allocation method that integrates estimation error and sampling costs. Our experiment results show that our algorithms effectively and consistently reduce sampling costs, compared with a stratified sampling method that only considers estimation error. In addition, compared with simple random sampling, our algorithm has higher sampling accuracy and lower sampling costs.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Deep Web, Data Mining, Stratified Sampling]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532713</person_id>
				<author_profile_id><![CDATA[81442605355]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tantan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532714</person_id>
				<author_profile_id><![CDATA[81384591112]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Fan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532715</person_id>
				<author_profile_id><![CDATA[81100288827]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gagan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Agrawal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934614</article_id>
		<sort_key>460</sort_key>
		<display_label>Pages</display_label>
		<pages>334-343</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>43</seq_no>
		<title><![CDATA[Learning Markov Network Structure with Decision Trees]]></title>
		<page_from>334</page_from>
		<page_to>343</page_to>
		<doi_number>10.1109/ICDM.2010.128</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934614</url>
		<abstract>
			<par><![CDATA[Traditional Markov network structure learning algorithms perform a search for globally useful features. However, these algorithms are often slow and prone to finding local optima due to the large space of possible structures. Ravikumar et al. recently proposed the alternative idea of applying L1 logistic regression to learn a set of pair wise features for each variable, which are then combined into a global model. This paper presents the DTSL algorithm, which uses probabilistic decision trees as the local model. Our approach has two significant advantages: it is more efficient, and it is able to discover features that capture more complex interactions among the variables. Our approach can also be seen as a method for converting a dependency network into a consistent probabilistic model. In an extensive empirical evaluation on 13 datasets, our algorithm obtains comparable accuracy to three standard structure learning algorithms while running 1-4 orders of magnitude faster.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Markov networks, structure learning, decision trees, probabilistic methods]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533782</person_id>
				<author_profile_id><![CDATA[81381602582]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lowd]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533783</person_id>
				<author_profile_id><![CDATA[81100603602]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jesse]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Davis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934621</article_id>
		<sort_key>470</sort_key>
		<display_label>Pages</display_label>
		<pages>344-353</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>44</seq_no>
		<title><![CDATA[Towards Structural Sparsity]]></title>
		<subtitle><![CDATA[An Explicit l2/l0 Approach]]></subtitle>
		<page_from>344</page_from>
		<page_to>353</page_to>
		<doi_number>10.1109/ICDM.2010.155</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934621</url>
		<abstract>
			<par><![CDATA[In many cases of machine learning or data mining applications, we are not only aimed to establish accurate {\em black box} predictors, we are also interested in discovering predictive patterns in data which enhance our interpretation and understanding of underlying physical, biological and other natural processes. Sparse representation is one of the focuses in this direction. More recently, structural sparsity has attracted increasing attentions. The structural sparsity is often achieved by imposing l2/l1 norms. In this paper, we present the explicit l2/l0 norm to directly achieve structural sparsity. To tackle the problem of intractable l2/l0 optimization, we develop a general Lipschitz auxiliary function which leads to simple iterative algorithms. In each iteration, optimal solution is achieved for the induced sub-problem and a guarantee of convergence is provided. Further more, the local convergent rate is also theoretically bounded. We test our optimization techniques in the multi-task feature learning problem. Experimental results suggest that our approaches outperform other approaches in both synthetic and real world data sets.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Structural sparsity, ?2/?0-norm, Non-smooth optimization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531663</person_id>
				<author_profile_id><![CDATA[81363601495]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dijun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Luo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531664</person_id>
				<author_profile_id><![CDATA[81100136610]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ding]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531665</person_id>
				<author_profile_id><![CDATA[81336489675]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Heng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934605</article_id>
		<sort_key>480</sort_key>
		<display_label>Pages</display_label>
		<pages>354-363</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>45</seq_no>
		<title><![CDATA[Multi-document Summarization Using Minimum Distortion]]></title>
		<page_from>354</page_from>
		<page_to>363</page_to>
		<doi_number>10.1109/ICDM.2010.106</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934605</url>
		<abstract>
			<par><![CDATA[Document summarization plays an important role in the area of natural language processing and text mining. This paper proposes several novel information-theoretic models for multi-document summarization. They consider document summarization as a transmission system and assume that the best summary should have the minimum distortion. By defining a proper distortion measure and a new representation method, the combination of the last two models (the linear representation model and the facility location model) gains good experimental results on the DUC2002 and DUC2004 datasets. Moreover, we also indicate that the model has high interpretability and extensibility.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[multi-document summarization, information-theoretic summarization, minimum distortion, J-S Divergence, linear representation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532678</person_id>
				<author_profile_id><![CDATA[81479642886]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tengfei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532679</person_id>
				<author_profile_id><![CDATA[81339534515]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Xiaojun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934622</article_id>
		<sort_key>490</sort_key>
		<display_label>Pages</display_label>
		<pages>364-373</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>46</seq_no>
		<title><![CDATA[A Log-Linear Model with Latent Features for Dyadic Prediction]]></title>
		<page_from>364</page_from>
		<page_to>373</page_to>
		<doi_number>10.1109/ICDM.2010.148</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934622</url>
		<abstract>
			<par><![CDATA[In dyadic prediction, labels must be predicted for pairs (dyads) whose members possess unique identifiers and, sometimes, additional features called side-information. Special cases of this problem include collaborative filtering and link prediction. We present a new {log-linear} model for dyadic prediction that is the first to satisfy several important desiderata: (i) labels may be ordinal or nominal, (ii) side-information can be easily exploited if present, (iii) with or without side-information, latent features are inferred for dyad members, (iv) the model is resistant to sample-selection bias, (v) it can learn well-calibrated probabilities, and (vi) it can scale to large datasets. To our knowledge, no existing method satisfies all the above criteria. In particular, many methods assume that the labels are binary or numerical, and cannot use side-information. Experimental results show that the new method is competitive with previous specialized methods for collaborative filtering and link prediction. Other experimental results demonstrate that the new method succeeds for dyadic prediction tasks where previous methods cannot be used. In particular, the new method predicts nominal labels accurately, and by using side-information it solves the cold-start problem in collaborative filtering.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Dyadic prediction, collaborative filtering, link prediction, log-linear model]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532220</person_id>
				<author_profile_id><![CDATA[81467651321]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Aditya]]></first_name>
				<middle_name><![CDATA[Krishna]]></middle_name>
				<last_name><![CDATA[Menon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532221</person_id>
				<author_profile_id><![CDATA[81339498029]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Charles]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Elkan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934623</article_id>
		<sort_key>500</sort_key>
		<display_label>Pages</display_label>
		<pages>374-383</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>47</seq_no>
		<title><![CDATA[Edge Weight Regularization over Multiple Graphs for Similarity Learning]]></title>
		<page_from>374</page_from>
		<page_to>383</page_to>
		<doi_number>10.1109/ICDM.2010.156</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934623</url>
		<abstract>
			<par><![CDATA[The growth of the web has directly influenced the increase in the availability of relational data. One of the key problems in mining such data is computing the similarity between objects with heterogeneous feature types. For example, publications have many heterogeneous features like text, citations, authorship information, venue information, etc. In most approaches, similarity is estimated using each feature type in isolation and then combined in a linear fashion. However, this approach does not take advantage of the dependencies between the different feature spaces. In this paper, we propose a novel approach to combine the different sources of similarity using a regularization framework over edges in multiple graphs. We show that the objective function induced by the framework is convex. We also propose an efficient algorithm using coordinate descent [1] to solve the optimization problem. We extrinsically evaluate the performance of the proposed unified similarity measure on two different tasks, clustering and classification. The proposed similarity measure outperforms three baselines and a state-of-the-art classification algorithm on a variety of standard, large data sets.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Machine Learning, Similarity Learning, Heterogeneous Features, Classification, Clustering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530105</person_id>
				<author_profile_id><![CDATA[81442596665]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pradeep]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Muthukrishnan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530106</person_id>
				<author_profile_id><![CDATA[81100376078]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dragomir]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Radev]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530107</person_id>
				<author_profile_id><![CDATA[81100214997]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Qiaozhu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mei]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934624</article_id>
		<sort_key>510</sort_key>
		<display_label>Pages</display_label>
		<pages>384-392</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>48</seq_no>
		<title><![CDATA[A New SVM Approach to Multi-instance Multi-label Learning]]></title>
		<page_from>384</page_from>
		<page_to>392</page_to>
		<doi_number>10.1109/ICDM.2010.109</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934624</url>
		<abstract>
			<par><![CDATA[In this paper, we address the problem of multi-instance multi-label learning (MIML) where each example is associated with not only multiple instances but also multiple class labels. In our novel approach, given an MIML example, each instance in the example is only associated with a single label and the label set of the example is the aggregation of all instance labels. Many real-world tasks such as scene classification, text categorization and gene sequence encoding can be properly formalized under our proposed approach. We formulate our MIML problem as a combination of two optimizations: (1) a quadratic programming (QP) that minimizes the empirical risk with L2-norm regularization, and (2) an integer programing (IP) assigning each instance to a single label. We also present an efficient method combining the stochastic gradient decent and alternating optimization approaches to solve our QP and IP optimizations. In our experiments with both an artificially generated data set and real-world applications, i.e. scene classification and text categorization, our proposed method achieves superior performance over existing state-of-the-art MIML methods such as MIMLBOOST, MIMLSVM, M$^3$MIML and MIMLRBF.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Multi-Instance Multi-Label, Classification, SVM]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530616</person_id>
				<author_profile_id><![CDATA[81479652515]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nguyen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934625</article_id>
		<sort_key>520</sort_key>
		<display_label>Pages</display_label>
		<pages>393-402</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>49</seq_no>
		<title><![CDATA[Bayesian Aggregation of Binary Classifiers]]></title>
		<page_from>393</page_from>
		<page_to>402</page_to>
		<doi_number>10.1109/ICDM.2010.81</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934625</url>
		<abstract>
			<par><![CDATA[Multiclass classification problems are often decomposed into multiple binary problems that are solved by individual binary classifiers whose results are integrated into a final answer. Various methods have been developed to aggregate binary classifiers, including voting heuristics, loss-based decoding, and probabilistic decoding methods, but a little work on the optimal aggregation has been done. In this paper we present a Bayesian method for optimally aggregating binary classifiers where class membership probabilities are determined by predictive probabilities. We model the class membership probability as a softmax function whose input argument is a linear combination of discrepancies between code words and probability estimates obtained by the binary classifiers. We consider a lower bound on the softmax function, which is represented as a product of logistic sigmoids, and we formulate the problem of learning aggregation weights as a variational logistic regression. Predictive probabilities computed by variational logistic regression yield the class membership probabilities. We stress two notable advantages over existing methods in the viewpoint of complexity and over fitting. Numerical experiments on several datasets confirm its useful behavior.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Classifier aggregation, multiclass classification, variational logistic regression]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532737</person_id>
				<author_profile_id><![CDATA[81479654715]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sunho]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Park]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532738</person_id>
				<author_profile_id><![CDATA[81479660246]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Seungjin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Choi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934632</article_id>
		<sort_key>530</sort_key>
		<display_label>Pages</display_label>
		<pages>403-410</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>50</seq_no>
		<title><![CDATA[Permutations as Angular Data]]></title>
		<subtitle><![CDATA[Efficient Inference in Factorial Spaces]]></subtitle>
		<page_from>403</page_from>
		<page_to>410</page_to>
		<doi_number>10.1109/ICDM.2010.122</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934632</url>
		<abstract>
			<par><![CDATA[Distributions over permutations arise in applications ranging from multi-object tracking to ranking of instances. The difficulty of dealing with these distributions is caused by the size of their domain, which is factorial in the number of considered entities ($n!$). It makes the direct definition of a multinomial distribution over permutation space impractical for all but a very small $n$. In this work we propose an embedding of all $n!$ permutations for a given $n$ in a surface of a hyper sphere defined in $\mathbbm{R}^{(n-1)}$. As a result of the embedding, we acquire ability to define continuous distributions over a hyper sphere with all the benefits of directional statistics. We provide polynomial time projections between the continuous hyper sphere representation and the $n!$-element permutation space. The framework provides a way to use continuous directional probability densities and the methods developed thereof for establishing densities over permutations. As a demonstration of the benefits of the framework we derive an inference procedure for a state-space model over permutations. We demonstrate the approach with simulations on a large number of objects hardly manageable by the state of the art inference methods, and an application to a real flight traffic control dataset.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532762</person_id>
				<author_profile_id><![CDATA[81435608518]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sergey]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Plis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532763</person_id>
				<author_profile_id><![CDATA[81100001124]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Terran]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lane]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532764</person_id>
				<author_profile_id><![CDATA[81316487807]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Vince]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Calhoun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934633</article_id>
		<sort_key>540</sort_key>
		<display_label>Pages</display_label>
		<pages>411-420</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>51</seq_no>
		<title><![CDATA[Separation of Interleaved Web Sessions with Heuristic Search]]></title>
		<page_from>411</page_from>
		<page_to>420</page_to>
		<doi_number>10.1109/ICDM.2010.43</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934633</url>
		<abstract>
			<par><![CDATA[We describe a heuristic search-based method for interleaved HTTP (Web) session reconstruction building upon first order Markov models. An interleaved session is generated by a user who is concurrently browsing the same web site in two or more web sessions (browser tabs or windows). In order to assure data quality for subsequent phases in analyzing user's browsing behavior, such sessions need to be separated in advance. We propose a separating process based on best-first search and trained first order Markov chains. We develop a testing method based on various measures of reconstructed sessions similarity to original ones. We evaluate the developed method on two real world click stream data sources: a web shop and a university student records information system. Preliminary results show that the proposed method performs well.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[HTTP session, clickstream, data quality, Markov model, user behavior, sessionization, interleaved session, session separation process]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532241</person_id>
				<author_profile_id><![CDATA[81479664177]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pozenel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532242</person_id>
				<author_profile_id><![CDATA[81100539926]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Viljan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mahnic]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532243</person_id>
				<author_profile_id><![CDATA[81100022199]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Matjaz]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kukar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934634</article_id>
		<sort_key>550</sort_key>
		<display_label>Pages</display_label>
		<pages>421-430</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>52</seq_no>
		<title><![CDATA[Consequences of Variability in Classifier Performance Estimates]]></title>
		<page_from>421</page_from>
		<page_to>430</page_to>
		<doi_number>10.1109/ICDM.2010.110</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934634</url>
		<abstract>
			<par><![CDATA[The prevailing approach to evaluating classifiers in the machine learning community involves comparing the performance of several algorithms over a series of usually unrelated data sets. However, beyond this there are many dimensions along which methodologies vary wildly. We show that, depending on the stability and similarity of the algorithms being compared, these sometimes-arbitrary methodological choices can have a significant impact on the conclusions of any study, including the results of statistical tests. In particular, we show that performance metrics and data sets used, the type of cross-validation employed, and the number of iterations of cross-validation run have a significant, and often predictable, effect. Based on these results, we offer a series of recommendations for achieving consistent, reproducible results in classifier performance comparisons.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[evaluation, reproducibility, classification]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530635</person_id>
				<author_profile_id><![CDATA[81442609629]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Troy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raeder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530636</person_id>
				<author_profile_id><![CDATA[81472653700]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[T.]]></first_name>
				<middle_name><![CDATA[Ryan]]></middle_name>
				<last_name><![CDATA[Hoens]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530637</person_id>
				<author_profile_id><![CDATA[81100002770]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nitesh]]></first_name>
				<middle_name><![CDATA[V.]]></middle_name>
				<last_name><![CDATA[Chawla]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934635</article_id>
		<sort_key>560</sort_key>
		<display_label>Pages</display_label>
		<pages>431-440</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>53</seq_no>
		<title><![CDATA[Mining Sensor Streams for Discovering Human Activity Patterns over Time]]></title>
		<page_from>431</page_from>
		<page_to>440</page_to>
		<doi_number>10.1109/ICDM.2010.40</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934635</url>
		<abstract>
			<par><![CDATA[In recent years, new emerging application domains have introduced new constraints and methods in data mining field. One of such application domains is activity discovery from sensor data. Activity discovery and recognition plays an important role in a wide range of applications from assisted living to security and surveillance. Most of the current approaches for activity discovery assume a static model of the activities and ignore the problem of mining and discovering activities from a data stream over time. Inspired by the unique requirements of activity discovery application domain, in this paper we propose a new stream mining method for finding sequential patterns over time from streaming non-transaction data using multiple time granularities. Our algorithm is able to find sequential patterns, even if the patterns exhibit discontinuities (interruptions) or variations in the sequence order. Our algorithm also addresses the problem of dealing with rare events across space and over time. We validate the results of our algorithms using data collected from two different smart apartments.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Activity Data Mining, Smart Environments, Sensor Data, Stream Sequence Mining]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532765</person_id>
				<author_profile_id><![CDATA[81363606574]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Parisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rashidi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532766</person_id>
				<author_profile_id><![CDATA[81100111814]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Diane]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Cook]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934636</article_id>
		<sort_key>570</sort_key>
		<display_label>Pages</display_label>
		<pages>441-450</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>54</seq_no>
		<title><![CDATA[Decision Trees for Uplift Modeling]]></title>
		<page_from>441</page_from>
		<page_to>450</page_to>
		<doi_number>10.1109/ICDM.2010.62</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934636</url>
		<abstract>
			<par><![CDATA[Most classification approaches aim at achieving high prediction accuracy on a given dataset. However, in most practical cases, some action, such as mailing an offer or treating a patient, is to be taken on the classified objects and we should model not the class probabilities themselves, but instead, the change in class probabilities caused by the action. The action should then be performed on those objects for which it will be most profitable. This problem is known as uplift modeling, differential response analysis or true lift modeling, but has received very little attention in Machine Learning literature. In the paper we present a tree based classifier tailored specifically to this task. To this end, we design new splitting criteria and pruning methods. The experiments confirm the usefulness of the proposed approach and show significant improvement over previous uplift modeling techniques.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[uplift modeling, decision trees, information theory]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532244</person_id>
				<author_profile_id><![CDATA[81479662303]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Piotr]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rzepakowski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532245</person_id>
				<author_profile_id><![CDATA[83458701857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Szymon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jaroszewicz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934643</article_id>
		<sort_key>580</sort_key>
		<display_label>Pages</display_label>
		<pages>451-460</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>55</seq_no>
		<title><![CDATA[Co-clustering of Lagged Data]]></title>
		<page_from>451</page_from>
		<page_to>460</page_to>
		<doi_number>10.1109/ICDM.2010.44</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934643</url>
		<abstract>
			<par><![CDATA[The paper focuses on mining clusters that are characterized by a lagged relationship between the data objects. We call such clusters lagged co-clusters. A lagged co-cluster of a matrix is a sub matrix determined by a subset of rows and their corresponding lag over a subset of columns. Extracting such subsets (not necessarily successive) may reveal an underlying governing regulatory mechanism. Such a regulatory mechanism is quite common in real life settings. It appears in a variety of fields: meteorology, seismic activity, stock market behavior, neuronal brain activity, river flow and navigation, are but a limited list of examples. Mining such lagged co-clusters not only helps in understanding the relationship between objects in the domain, but assists in forecasting their future behavior. For most interesting variants of this problem, finding an optimal lagged co-cluster is an NP-complete problem. We present a polynomial-time Monte-Carlo algorithm for finding a set of lagged co-clusters whose error does not exceed a pre-specified value, which handles noise, anti-correlations, missing values, and overlapping patterns. Moreover, we prove that the list includes, with fixed probability, a lagged co-cluster which is optimal in its dimensions. The algorithm was extensively evaluated using various environments. First, artificial data, enabling the evaluation of specific, isolated properties of the algorithm. Secondly, real-world data, using river flow and topographic data, enabling the evaluation of the algorithm to efficiently mine relevant and coherent lagged co-clusters in environments that are temporal, i.e., time reading data, and non-temporal, respectively.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[clustering, co-clustering, lagged clustering, timelagged, data mining]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530166</person_id>
				<author_profile_id><![CDATA[81479646409]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eran]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shaham]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530167</person_id>
				<author_profile_id><![CDATA[81365596751]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sarne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530168</person_id>
				<author_profile_id><![CDATA[81329487687]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Boaz]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ben-Moshe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934644</article_id>
		<sort_key>590</sort_key>
		<display_label>Pages</display_label>
		<pages>461-470</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>56</seq_no>
		<title><![CDATA[Polishing the Right Apple]]></title>
		<subtitle><![CDATA[Anytime Classification Also Benefits Data Streams with Constant Arrival Times]]></subtitle>
		<page_from>461</page_from>
		<page_to>470</page_to>
		<doi_number>10.1109/ICDM.2010.120</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934644</url>
		<abstract>
			<par><![CDATA[Classification of items taken from data streams requires algorithms that operate in time sensitive and computationally constrained environments. Often, the available time for classification is not known a priori and may change as a consequence of external circumstances. Many traditional algorithms are unable to provide satisfactory performance while supporting the highly variable response times that exemplify such applications. In such contexts, anytime algorithms, which are amenable to trading time for accuracy, have been found to be exceptionally useful and constitute an area of increasing research activity. Previous techniques for improving anytime classification have generally been concerned with optimizing the probability of correctly classifying individual objects. However, as we shall see, serially optimizing the probability of correctly classifying individual objects K times, generally gives inferior results to batch optimizing the probability of correctly classifying K objects. In this work, we show that this simple observation can be exploited to improve overall classification performance by using an anytime framework to allocate resources among a set of objects buffered from a fast arriving stream. Our ideas are independent of object arrival behavior, and, perhaps unintuitively, even in data streams with constant arrival rates our technique exhibits a marked improvement in performance. The utility of our approach is demonstrated with extensive experimental evaluations conducted on a wide range of diverse datasets.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[anytime algorithms, classification, nearest neighbor, streaming data]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530169</person_id>
				<author_profile_id><![CDATA[81367595197]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shieh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530170</person_id>
				<author_profile_id><![CDATA[81100209161]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eamonn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Keogh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934645</article_id>
		<sort_key>600</sort_key>
		<display_label>Pages</display_label>
		<pages>471-480</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>57</seq_no>
		<title><![CDATA[Discovering Correlated Subspace Clusters in 3D Continuous-Valued Data]]></title>
		<page_from>471</page_from>
		<page_to>480</page_to>
		<doi_number>10.1109/ICDM.2010.19</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934645</url>
		<abstract>
			<par><![CDATA[Subspace clusters represent useful information in high-dimensional data. However, mining significant subspace clusters in continuous-valued 3D data such as stock-financial ratio-year data, or gene-sample-time data, is difficult. Firstly, typical metrics either find subspaces with very few objects, or they find too many insignificant subspaces &#8211; those which exist by chance. Besides, typical 3D subspace clustering approaches abound with parameters, which are usually set under biased assumptions, making the mining process a &#8216;guessing game&#8217;. We address these concerns by proposing an information theoretic measure, which allows us to identify 3D subspace clusters that stand out from the data. We also develop a highly effective, efficient and parameter-robust algorithm, which is a hybrid of information theoretical and statistical techniques, to mine these clusters. From extensive experimentations, we show that our approach can discover significant 3D subspace clusters embedded in 110 synthetic datasets of varying conditions. We also perform a case study on real-world stock datasets, which shows that our clusters can generate higher profits compared to those mined by other approaches.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[3D subspace clustering, financial data mining, information theory]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532306</person_id>
				<author_profile_id><![CDATA[81321498030]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kelvin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532307</person_id>
				<author_profile_id><![CDATA[81100540584]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Zeyar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aung]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532308</person_id>
				<author_profile_id><![CDATA[81319492485]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Vivekanand]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gopalkrishnan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934646</article_id>
		<sort_key>610</sort_key>
		<display_label>Pages</display_label>
		<pages>481-490</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>58</seq_no>
		<title><![CDATA[gSkeletonClu]]></title>
		<subtitle><![CDATA[Density-Based Network Clustering via Structure-Connected Tree Division or Agglomeration]]></subtitle>
		<page_from>481</page_from>
		<page_to>490</page_to>
		<doi_number>10.1109/ICDM.2010.69</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934646</url>
		<abstract>
			<par><![CDATA[Community detection is an important task for mining the structure and function of complex networks. Many pervious approaches are difficult to detect communities with arbitrary size and shape, and are unable to identify hubs and outliers. A recently proposed network clustering algorithm, SCAN, is effective and can overcome this difficulty. However, it depends on a sensitive parameter: minimum similarity threshold $\varepsilon$, but provides no automated way to find it. In this paper, we propose a novel density-based network clustering algorithm, called gSkeletonClu (graph-skeleton based clustering). By projecting a network to its Core-Connected Maximal Spanning Tree (CCMST), the network clustering problem is converted to finding core-connected components in the CCMST. We discover that all possible values of the parameter $\varepsilon$ lie in the edge weights of the corresponding CCMST. By means of tree divisive or agglomerative clustering, our algorithm can find the optimal parameter $\varepsilon$ and detect communities, hubs and outliers in large-scale undirected networks automatically without any user interaction. Extensive experiments on both real-world and synthetic networks demonstrate the superior performance of gSkeletonClu over the baseline methods.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Density-based Network Clustering, Community Discovery, Hubs and Outliers, Parameter Selection]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531724</person_id>
				<author_profile_id><![CDATA[81444599956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Heli]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531725</person_id>
				<author_profile_id><![CDATA[81444600951]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jianbin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531726</person_id>
				<author_profile_id><![CDATA[81479644694]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jiawei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Han]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531727</person_id>
				<author_profile_id><![CDATA[81413600167]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hongbo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Deng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531728</person_id>
				<author_profile_id><![CDATA[81331508306]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Peixiang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531729</person_id>
				<author_profile_id><![CDATA[81541141556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Boqin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Feng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934637</article_id>
		<sort_key>620</sort_key>
		<display_label>Pages</display_label>
		<pages>491-500</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>59</seq_no>
		<title><![CDATA[LogTree]]></title>
		<subtitle><![CDATA[A Framework for Generating System Events from Raw Textual Logs]]></subtitle>
		<page_from>491</page_from>
		<page_to>500</page_to>
		<doi_number>10.1109/ICDM.2010.76</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934637</url>
		<abstract>
			<par><![CDATA[Modern computing systems are instrumented to generate huge amounts of system logs and these data can be utilized for understanding and complex system behaviors. One main fundamental challenge in automated log analysis is the generation of system events from raw textual logs. Recent works apply clustering techniques to translate the raw log messages into system events using only the word/term information. In this paper, we first illustrate the drawbacks of existing techniques for event generation from system logs. We then propose Log Tree, a novel and algorithm-independent framework for events generation from raw system log messages. Log Tree utilizes the format and structural information of the raw logs in the clustering process to generate system events with better accuracy. In addition, an indexing data structure, Message Segment Table, is proposed in Log Tree to significantly improve the efficiency of events creation. Extensive experiments on real system logs demonstrate the effectiveness and efficiency of Log Tree.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[log analysis, event creation, message clustering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533337</person_id>
				<author_profile_id><![CDATA[81466642027]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Liang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533338</person_id>
				<author_profile_id><![CDATA[81100475528]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934653</article_id>
		<sort_key>630</sort_key>
		<display_label>Pages</display_label>
		<pages>501-510</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>60</seq_no>
		<title><![CDATA[Mining Closed Strict Episodes]]></title>
		<page_from>501</page_from>
		<page_to>510</page_to>
		<doi_number>10.1109/ICDM.2010.89</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934653</url>
		<abstract>
			<par><![CDATA[Discovering patterns in a sequence is an important aspect of data mining. One popular choice of such patterns are episodes, patterns in sequential data describing events that often occur in the vicinity of each other. Episodes also enforce in which order events are allowed to occur. In this work we introduce a technique for discovering closed episodes. Adopting existing approaches for discovering traditional patterns, such as closed item sets, to episodes is not straightforward. First of all, we cannot define a unique closure based on frequency because an episode may have several closed super episodes. Moreover, to define a closedness concept for episodes we need a subset relationship between episodes, which is not trivial to define. We approach these problems by introducing strict episodes. We argue that this class is general enough, and at the same time we are able to define a natural subset relationship within it and use it efficiently. In order to mine closed episodes we define an auxiliary closure operator. We show that this closure satisfies the needed Galois connection so that we can use the existing framework for mining closed patterns. Discovering the true closed episodes can be done as a post-processing step. We combine these observations into an efficient mining algorithm and demonstrate empirically its performance in practice.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Frequent Episode Mining, Closed Episodes, Level-wise Algorithm]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533417</person_id>
				<author_profile_id><![CDATA[81363600602]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nikolaj]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tatti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533418</person_id>
				<author_profile_id><![CDATA[81479660792]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Boris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cule]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934654</article_id>
		<sort_key>640</sort_key>
		<display_label>Pages</display_label>
		<pages>511-520</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>61</seq_no>
		<title><![CDATA[Multi-dimensional Mass Estimation and Mass-based Clustering]]></title>
		<page_from>511</page_from>
		<page_to>520</page_to>
		<doi_number>10.1109/ICDM.2010.49</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934654</url>
		<abstract>
			<par><![CDATA[Mass estimation, an alternative to density estimation, has been shown recently to be an effective base modelling mechanism for three data mining tasks of regression, information retrieval and anomaly detection. This paper advances this work in two directions. First, we generalise the previously proposed one-dimensional mass estimation to multi-dimensional mass estimation, and significantly reduce the time complexity to $O(\psi h)$ from $O({\psi}^{h})&#x2014;making it feasible for a full range of generic problems. Second, we introduce the first clustering method based on mass#x2014;it is unique because it does not employ any distance or density measure. The structure of the new mass model enables different parts of a cluster to be identified and merged without expensive evaluations. The characteristics of the new clustering method are: (i) it can identify arbitrary-shape clusters, (ii) it is significantly faster than existing density-based or distance-based methods, and (iii) it is noise-tolerant.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Mass estimation, mass-based clustering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533916</person_id>
				<author_profile_id><![CDATA[81100367824]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kai]]></first_name>
				<middle_name><![CDATA[Ming]]></middle_name>
				<last_name><![CDATA[Ting]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533917</person_id>
				<author_profile_id><![CDATA[81438595313]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Wells]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934655</article_id>
		<sort_key>650</sort_key>
		<display_label>Pages</display_label>
		<pages>521-530</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>62</seq_no>
		<title><![CDATA[minCEntropy]]></title>
		<subtitle><![CDATA[A Novel Information Theoretic Approach for the Generation of Alternative Clusterings]]></subtitle>
		<page_from>521</page_from>
		<page_to>530</page_to>
		<doi_number>10.1109/ICDM.2010.24</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934655</url>
		<abstract>
			<par><![CDATA[Traditional clustering has focused on creating a single good clustering solution, while modern, high dimensional data can often be interpreted, and hence clustered, in different ways. Alternative clustering aims at creating multiple clustering solutions that are both of high quality and distinctive from each other. Methods for alternative clustering can be divided into objective-function-oriented and data-transformation-oriented approaches. This paper presents a novel information theoretic-based, objective-function-oriented approach to generate alternative clusterings, in either an unsupervised or semi-supervised manner. We employ the conditional entropy measure for quantifying both clustering quality and distinctiveness, resulting in an analytically consistent combined criterion. Our approach employs a computationally efficient nonparametric entropy estimator, which does not impose any assumption on the probability distributions. We propose a partitional clustering algorithm, named minCEntropy, to concurrently optimize both clustering quality and distinctiveness. minCEntropy requires setting only some rather intuitive parameters, and performs competitively with existing methods for alternative clustering.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[clustering, alternative clustering, transformation, multi-objective optimization, information theoretic clustering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533918</person_id>
				<author_profile_id><![CDATA[81435606662]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nguyen]]></first_name>
				<middle_name><![CDATA[Xuan]]></middle_name>
				<last_name><![CDATA[Vinh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533919</person_id>
				<author_profile_id><![CDATA[81339498387]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Julien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Epps]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934656</article_id>
		<sort_key>660</sort_key>
		<display_label>Pages</display_label>
		<pages>531-540</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>63</seq_no>
		<title><![CDATA[A Conscience On-line Learning Approach for Kernel-Based Clustering]]></title>
		<page_from>531</page_from>
		<page_to>540</page_to>
		<doi_number>10.1109/ICDM.2010.57</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934656</url>
		<abstract>
			<par><![CDATA[Kernel-based clustering is one of the most popular methods for partitioning nonlinearly separable dataset. However, exhaustive search for the global optimum is NP-hard. Iterative procedure such as k-means can be used to seek one of the local minima. Unfortunately, it is easily trapped into degenerate local minima when the prototypes of clusters are ill-initialized. In this paper, we restate the optimization problem of kernel-based clustering in an on-line learning framework, whereby a conscience mechanism is easily integrated to tackle the ill-initialization problem and faster convergence rate is achieved. Thus, we propose a novel approach termed conscience on-line learning (COLL). For each randomly taken data point, our method selects the winning prototype based on the conscience mechanism to bias the ill-initialized prototype to avoid degenerate local minima, and efficiently updates the winner by the on-line learning rule. Therefore, it can more efficiently obtain smaller distortion error than k-means with the same initialization. Experimental results on synthetic and large-scale real-world datasets, as well as that in the application of video clustering, have demonstrated the significant improvement over existing kernel clustering methods.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[kernel-based clustering, conscience mechanism, on-line learning, k-means]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531774</person_id>
				<author_profile_id><![CDATA[81479653207]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chang-Dong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531775</person_id>
				<author_profile_id><![CDATA[81100543893]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jian-Huang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531776</person_id>
				<author_profile_id><![CDATA[81490654336]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jun-Yong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934657</article_id>
		<sort_key>670</sort_key>
		<display_label>Pages</display_label>
		<pages>541-550</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>64</seq_no>
		<title><![CDATA[Weighted Feature Subset Non-negative Matrix Factorization and Its Applications to Document Understanding]]></title>
		<page_from>541</page_from>
		<page_to>550</page_to>
		<doi_number>10.1109/ICDM.2010.47</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934657</url>
		<abstract>
			<par><![CDATA[Keyword (Feature) selection enhances and improves many Information Retrieval (IR) tasks such as document categorization, automatic topic discovery, etc. The problem of keyword selection is usually solved using supervised algorithms. In this paper, we propose an unsupervised approach that combines keyword selection and document clustering (topic discovery) together. The proposed approach extends non-negative matrix factorization (NMF) by incorporating a weight matrix to indicate the importance of the keywords. The proposed approach is further extended to a weighted version in which each document is also assigned a weight to assess its importance in the cluster. This work considers both theoretical and empirical weighted feature subset selection for NMF and draws the connection between unsupervised feature selection and data clustering. We apply our proposed approaches to various document understanding tasks including document clustering, summarization, and visualization. Experimental results demonstrate the effectiveness of our approach for these tasks.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Non-negative matrix factorization, feature selection, weighted feature subset non-negative matrix factorization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532326</person_id>
				<author_profile_id><![CDATA[81363591802]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dingding]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532327</person_id>
				<author_profile_id><![CDATA[81100475528]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532328</person_id>
				<author_profile_id><![CDATA[81100136610]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ding]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934495</article_id>
		<sort_key>680</sort_key>
		<display_label>Pages</display_label>
		<pages>551-560</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>65</seq_no>
		<title><![CDATA[Learning a Bi-Stochastic Data Similarity Matrix]]></title>
		<page_from>551</page_from>
		<page_to>560</page_to>
		<doi_number>10.1109/ICDM.2010.141</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934495</url>
		<abstract>
			<par><![CDATA[An idealized clustering algorithm seeks to learn a cluster-adjacency matrix such that, if two data points belong to the same cluster, the corresponding entry would be 1, otherwise the entry would be 0. This integer (1/0) constraint makes it difficult to find the optimal solution. We propose a relaxation on the cluster-adjacency matrix, by deriving a bi-stochastic matrix from a data similarity (e.g., kernel) matrix according to the Bregman divergence. Our general method is named the {\em Bregmanian Bi-Stochastication} (BBS) algorithm. We focus on two popular choices of the Bregman divergence: the Euclidian distance and the KL divergence. Interestingly, the BBS algorithm using the KL divergence is equivalent to the Sinkhorn-Knopp (SK) algorithm for deriving bi-stochastic matrices. We show that the BBS algorithm using the Euclidian distance is closely related to the relaxed $k$-means clustering and can often produce noticeably superior clustering results than the SK algorithm (and other algorithms such as Normalized Cut), through extensive experiments on public data sets.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532878</person_id>
				<author_profile_id><![CDATA[81479641348]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532879</person_id>
				<author_profile_id><![CDATA[81479648386]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ping]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532880</person_id>
				<author_profile_id><![CDATA[81100484292]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Arnd]]></first_name>
				<middle_name><![CDATA[Christian]]></middle_name>
				<last_name><![CDATA[Konig]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934496</article_id>
		<sort_key>690</sort_key>
		<display_label>Pages</display_label>
		<pages>561-568</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>66</seq_no>
		<title><![CDATA[Active Spectral Clustering]]></title>
		<page_from>561</page_from>
		<page_to>568</page_to>
		<doi_number>10.1109/ICDM.2010.119</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934496</url>
		<abstract>
			<par><![CDATA[The technique of spectral clustering is widely used to segment a range of data from graphs to images. Our work marks a natural progression of spectral clustering from the original passive unsupervised formulation to our active semi-supervised formulation. We follow the widely used area of constrained clustering and allow supervision in the form of pair wise relations between two nodes: Must-Link and Cannot-Link. Unlike most previous constrained clustering work, our constraints are specified incrementally by querying an oracle (domain expert). Since in practice, each query comes with a cost, our goal is to maximally improve the result with as few queries as possible. The advantages of our approach include: 1) it is principled by querying the constraints which maximally reduce the expected error, 2) it can incorporate both hard and soft constraints which are prevalent in practice. We empirically show that our method significantly outperforms the baseline approach, namely constrained spectral clustering with randomly selected constraints, on UCI benchmark data sets.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[spectral clustering, active learning, constrained clustering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533441</person_id>
				<author_profile_id><![CDATA[81479644300]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xiang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533442</person_id>
				<author_profile_id><![CDATA[81100099431]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Davidson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934497</article_id>
		<sort_key>700</sort_key>
		<display_label>Pages</display_label>
		<pages>569-578</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>67</seq_no>
		<title><![CDATA[Discovering Overlapping Groups in Social Media]]></title>
		<page_from>569</page_from>
		<page_to>578</page_to>
		<doi_number>10.1109/ICDM.2010.48</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934497</url>
		<abstract>
			<par><![CDATA[The increasing popularity of social media is shortening the distance between people. Social activities, e.g., tagging in Flickr, book marking in Delicious, twittering in Twitter, etc. are reshaping people&#8217;s social life and redefining their social roles. People with shared interests tend to form their groups in social media, and users within the same community likely exhibit similar social behavior (e.g., going for the same movies, having similar political viewpoints), which in turn reinforces the community structure. The multiple interactions in social activities entail that the community structures are often overlapping, i.e., one person is involved in several communities. We propose a novel co-clustering framework, which takes advantage of networking information between users and tags in social media, to discover these overlapping communities. In our method, users are connected via tags and tags are connected to users. This explicit representation of users and tags is useful for understanding group evolution by looking at who is interested in what. The efficacy of our method is supported by empirical evaluation in both synthetic and online social networking data.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Community Detection, Overlapping, Social Media, Co-Clustering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532377</person_id>
				<author_profile_id><![CDATA[81484643431]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xufei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532378</person_id>
				<author_profile_id><![CDATA[81363600542]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Lei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532379</person_id>
				<author_profile_id><![CDATA[81466643026]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Huiji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532380</person_id>
				<author_profile_id><![CDATA[81367594306]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Huan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934498</article_id>
		<sort_key>710</sort_key>
		<display_label>Pages</display_label>
		<pages>579-588</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>68</seq_no>
		<title><![CDATA[Adaptive Distances on Sets of Vectors]]></title>
		<page_from>579</page_from>
		<page_to>588</page_to>
		<doi_number>10.1109/ICDM.2010.45</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934498</url>
		<abstract>
			<par><![CDATA[Recently, there has been a growing interest in learning distances directly from training data. While the previous works focused mainly on adapting distance measures over vectorial data, it is a well-known fact that many real-world data could not be easily represented as fixed length tuples of constants. In this paper we address this limitation and propose a novel class of distance learning techniques for learning problems in which instances are set of vectors, examples of such problems include, among others, automatic image annotation and graph classification. We investigate the behavior of the adaptive set distances on a number of artificial and real-world problems and demonstrate that they improve over the standard set distances.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[distance learning, adaptive distances, complex objects, sets, graphs]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531805</person_id>
				<author_profile_id><![CDATA[81321499830]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Adam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Woznica]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531806</person_id>
				<author_profile_id><![CDATA[81100543229]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alexandros]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kalousis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934499</article_id>
		<sort_key>720</sort_key>
		<display_label>Pages</display_label>
		<pages>589-598</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>69</seq_no>
		<title><![CDATA[SMILE]]></title>
		<subtitle><![CDATA[A Similarity-Based Approach for Multiple Instance Learning]]></subtitle>
		<page_from>589</page_from>
		<page_to>598</page_to>
		<doi_number>10.1109/ICDM.2010.126</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934499</url>
		<abstract>
			<par><![CDATA[Multiple instance learning (MIL) is a generalization of supervised learning which attempts to learn useful information from bags of instances. In MIL, the true labels of the instances in positive bags are not always available for training. This leads to a critical challenge, namely, handling the ambiguity of instance labels in positive bags. To address this issue, this paper proposes a novel MIL method named SMILE (Similarity-based Multiple Instance LEarning). It introduces a similarity weight to each instance in positive bag, which represents the instance similarity towards the positive and negative classes. The instances in positive bags, together with their similarity weights, are thereafter incorporated into the learning phase to build an extended SVM-based predictive classifier. Experiments on three real-world datasets consisting of 12 subsets show that SMILE achieves markedly better classification accuracy than state-of-the-art MIL methods.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Multiple Instance Learning]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530236</person_id>
				<author_profile_id><![CDATA[81461654846]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yanshan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xiao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530237</person_id>
				<author_profile_id><![CDATA[81479654571]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530238</person_id>
				<author_profile_id><![CDATA[81479644224]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Longbing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530239</person_id>
				<author_profile_id><![CDATA[81479658992]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530240</person_id>
				<author_profile_id><![CDATA[81452596585]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Xindong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934506</article_id>
		<sort_key>730</sort_key>
		<display_label>Pages</display_label>
		<pages>599-608</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>70</seq_no>
		<title><![CDATA[Modeling Information Diffusion in Implicit Networks]]></title>
		<page_from>599</page_from>
		<page_to>608</page_to>
		<doi_number>10.1109/ICDM.2010.22</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934506</url>
		<abstract>
			<par><![CDATA[Social media forms a central domain for the production and dissemination of real-time information. Even though such flows of information have traditionally been thought of as diffusion processes over social networks, the underlying phenomena are the result of a complex web of interactions among numerous participants. Here we develop the Linear Influence Model where rather than requiring the knowledge of the social network and then modeling the diffusion by predicting which node will influence which other nodes in the network, we focus on modeling the global influence of a node on the rate of diffusion through the (implicit) network. We model the number of newly infected nodes as a function of which other nodes got infected in the past. For each node we estimate an influence function that quantifies how many subsequent infections can be attributed to the influence of that node over time. A nonparametric formulation of the model leads to a simple least squares problem that can be solved on large datasets. We validate our model on a set of 500 million tweets and a set of 170 million news articles and blog posts. We show that the Linear Influence Model accurately models influences of nodes and reliably predicts the temporal dynamics of information diffusion. We find that patterns of influence of individual participants differ significantly depending on the type of the node and the topic of the information.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Information diffusion, Online media, Node influence]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532910</person_id>
				<author_profile_id><![CDATA[81479641186]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jaewon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532911</person_id>
				<author_profile_id><![CDATA[81367595814]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jure]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Leskovec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934500</article_id>
		<sort_key>740</sort_key>
		<display_label>Pages</display_label>
		<pages>609-618</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>71</seq_no>
		<title><![CDATA[Term Filtering with Bounded Error]]></title>
		<page_from>609</page_from>
		<page_to>618</page_to>
		<doi_number>10.1109/ICDM.2010.131</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934500</url>
		<abstract>
			<par><![CDATA[In this paper, we consider a novel problem referred to as term filtering with bounded error to reduce the term (feature) space by eliminating terms without (or with bounded) information loss. Different from existing works, the obtained term space provides a complete view of the original term space. More interestingly, several important questions can be answered such as: 1) how different terms interact with each other and 2) how the filtered terms can be represented by the other terms. We perform a theoretical investigation of the term filtering problem and link it to the Geometric Covering By Discs problem, and prove its NP-hardness. We present two novel approaches for both loss less and lossy term filtering with bounds on the introduced error. Experimental results on multiple text mining tasks validate the effectiveness of the proposed approaches.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532381</person_id>
				<author_profile_id><![CDATA[81479652756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Zi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532382</person_id>
				<author_profile_id><![CDATA[81479654119]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532383</person_id>
				<author_profile_id><![CDATA[81479640540]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532384</person_id>
				<author_profile_id><![CDATA[81479658705]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Juanzi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934507</article_id>
		<sort_key>750</sort_key>
		<display_label>Pages</display_label>
		<pages>619-628</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>72</seq_no>
		<title><![CDATA[Exploiting Unlabeled Data to Enhance Ensemble Diversity]]></title>
		<page_from>619</page_from>
		<page_to>628</page_to>
		<doi_number>10.1109/ICDM.2010.12</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934507</url>
		<abstract>
			<par><![CDATA[Ensemble learning aims to improve generalization ability by using multiple base learners. It is well-known that to construct a good ensemble, the base learners should be accurate as well as diverse. In this paper, unlabeled data is exploited to facilitate ensemble learning by helping augment the diversity among the base learners. Specifically, a semi-supervised ensemble method named UDEED is proposed. Unlike existing semi-supervised ensemble methods where error-prone pseudo-labels are estimated for unlabeled data to enlarge the labeled data to improve accuracy, UDEED works by maximizing accuracies of base learners on labeled data while maximizing diversity among them on unlabeled data. Experiments show that UDEED can effectively utilize unlabeled data for ensemble learning and is highly competitive to well-established semi-supervised ensemble methods.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[ensemble learning, unlabeled data, diversity]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2529626</person_id>
				<author_profile_id><![CDATA[81423595920]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Min-Ling]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529627</person_id>
				<author_profile_id><![CDATA[81451593001]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Zhi-Hua]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934508</article_id>
		<sort_key>760</sort_key>
		<display_label>Pages</display_label>
		<pages>629-638</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>73</seq_no>
		<title><![CDATA[Constraint Based Dimension Correlation and Distance Divergence for Clustering High-Dimensional Data]]></title>
		<page_from>629</page_from>
		<page_to>638</page_to>
		<doi_number>10.1109/ICDM.2010.15</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934508</url>
		<abstract>
			<par><![CDATA[Clusters are hidden in subspaces of high dimensional data, i.e., only a subset of features is relevant for each cluster. Subspace clustering is challenging since the search for the relevant features of each cluster and the detection of the final clusters are circular dependent and should be solved simultaneously. In this paper, we point out that feature correlation and distance divergence are important to subspace clustering, but both have not been considered in previous works. Feature correlation groups correlated features independently thus helps to reduce the search space for the relevant features search problem. Distance divergence distinguishes distances on different dimensions and helps to find the final clusters accurately. We tackle the two problems with the aid of a small amount domain knowledge in the form of must-links and cannot-links. We then devise a semi-supervised subspace clustering algorithm CDCDD. CDCDD integrates our solutions of the feature correlation and distance divergence problems, and uses an adaptive dimension voting scheme, which is derived from a previous unsupervised subspace clustering algorithm FINDIT. Experimental results on both synthetic data sets and real data sets show that the proposed CDCDD algorithm outperforms FINDIT in terms of accuracy, and outperforms the other constraint based algorithm SCMINER in terms of both accuracy and efficiency.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[high-dimensional data, subspace clustering, semi-supervised learning, pair-wise constraint]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531312</person_id>
				<author_profile_id><![CDATA[81479659214]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xianchao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531313</person_id>
				<author_profile_id><![CDATA[81479649713]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531314</person_id>
				<author_profile_id><![CDATA[81479652811]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Qiu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934509</article_id>
		<sort_key>770</sort_key>
		<display_label>Pages</display_label>
		<pages>639-648</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>74</seq_no>
		<title><![CDATA[Active Learning from Multiple Noisy Labelers with Varied Costs]]></title>
		<page_from>639</page_from>
		<page_to>648</page_to>
		<doi_number>10.1109/ICDM.2010.147</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934509</url>
		<abstract>
			<par><![CDATA[In active learning, where a learning algorithm has to purchase the labels of its training examples, it is often assumed that there is only one labeler available to label examples, and that this labeler is noise-free. In reality, it is possible that there are multiple labelers available (such as human labelers in the online annotation tool Amazon Mechanical Turk) and that each such labeler has a different cost and accuracy. We address the active learning problem with multiple labelers where each labeler has a different (known) cost and a different (unknown) accuracy. Our approach uses the idea of {\em adjusted cost}, which allows labelers with different costs and accuracies to be directly compared. This allows our algorithm to find low-cost combinations of labelers that result in high-accuracy labelings of instances. Our algorithm further reduces costs by pruning under performing labelers from the set under consideration, and by halting the process of estimating the accuracy of the labelers as early as it can. We found that our algorithm often outperforms, and is always competitive with, other algorithms in the literature.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[active learning, multiple labelers, noisy labelers, algorithms, adjusted cost]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532912</person_id>
				<author_profile_id><![CDATA[81375603070]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yaling]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zheng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532913</person_id>
				<author_profile_id><![CDATA[81479644459]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Scott]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532914</person_id>
				<author_profile_id><![CDATA[81367591353]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Deng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934510</article_id>
		<sort_key>780</sort_key>
		<display_label>Pages</display_label>
		<pages>649-658</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>75</seq_no>
		<title><![CDATA[A Novel Contrast Co-learning Framework for Generating High Quality Training Data]]></title>
		<page_from>649</page_from>
		<page_to>658</page_to>
		<doi_number>10.1109/ICDM.2010.23</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934510</url>
		<abstract>
			<par><![CDATA[The good performances of most classical learning algorithms are generally founded on high quality training data, which are clean and unbiased. The availability of such data is however becoming much harder than ever in many real world problems due to the difficulties in collecting large scale unbiased data and precisely labeling them for training. In this paper, we propose a general Contrast Co-learning (CCL) framework to refine the biased and noisy training data when an unbiased yet unlabeled data pool is available. CCL starts with multiple sets of probably biased and noisy training data and trains a set of classifiers individually. Then under the assumption that the confidently classified data samples may have higher probabilities to be correctly classified, CCL iteratively and automatically filtering out possible data noises as well as adding those confidently classified samples from the unlabeled data pool to correct the bias. Through this process, we can generate a cleaner and unbiased training dataset with theoretical guarantees. Extensive experiments on two public text datasets clearly show that CCL consistently improves the algorithmic classification performance on biased and noisy training data compared with several state-of-the-art classical algorithms.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Noisy training data, Training data bias, Contrast Classifier, Co-learning]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531315</person_id>
				<author_profile_id><![CDATA[81479643068]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Zeyu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zheng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531316</person_id>
				<author_profile_id><![CDATA[81100045080]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531317</person_id>
				<author_profile_id><![CDATA[81100044797]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shuicheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531318</person_id>
				<author_profile_id><![CDATA[81392606688]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ning]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531319</person_id>
				<author_profile_id><![CDATA[81416601059]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Zheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531320</person_id>
				<author_profile_id><![CDATA[81479657404]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Ming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934516</article_id>
		<sort_key>790</sort_key>
		<display_label>Pages</display_label>
		<pages>659-668</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>76</seq_no>
		<title><![CDATA[Network Simplification with Minimal Loss of Connectivity]]></title>
		<page_from>659</page_from>
		<page_to>668</page_to>
		<doi_number>10.1109/ICDM.2010.133</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934516</url>
		<abstract>
			<par><![CDATA[We propose a novel problem to simplify weighted graphs by pruning least important edges from them. Simplified graphs can be used to improve visualization of a network, to extract its main structure, or as a pre-processing step for other data mining algorithms. We define a graph connectivity function based on the best paths between all pairs of nodes. Given the number of edges to be pruned, the problem is then to select a subset of edges that best maintains the overall graph connectivity. Our model is applicable to a wide range of settings, including probabilistic graphs, flow graphs and distance graphs, since the path quality function that is used to find best paths can be defined by the user. We analyze the problem, and give lower bounds for the effect of individual edge removal in the case where the path quality function has a natural recursive property. We then propose a range of algorithms and report on experimental results on real networks derived from public biological databases. The results show that a large fraction of edges can be removed quite fast and with minimal effect on the overall graph connectivity. A rough semantic analysis of the removed edges indicates that few important edges were removed, and that the proposed approach could be a valuable tool in aiding users to view or explore weighted graphs.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[graph mining, network simplification, connectivity]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532434</person_id>
				<author_profile_id><![CDATA[81488648429]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532435</person_id>
				<author_profile_id><![CDATA[81479660319]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sebastien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Malher]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532436</person_id>
				<author_profile_id><![CDATA[81100609333]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hannu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Toivonen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934517</article_id>
		<sort_key>800</sort_key>
		<display_label>Pages</display_label>
		<pages>669-678</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>77</seq_no>
		<title><![CDATA[Improving Kernel Methods through Complex Data Mapping]]></title>
		<page_from>669</page_from>
		<page_to>678</page_to>
		<doi_number>10.1109/ICDM.2010.33</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934517</url>
		<abstract>
			<par><![CDATA[This paper introduces a simple yet powerful data transformation strategy for kernel machines. Instead of adapting the parameters of the kernel function w.r.t. the given data (as in conventional methods), we adjust both the kernel hyper-parameters and the given data itself. Using this approach, the input data is transformed to be more representative of the assumptions encoded in the kernel function. A novel complex mapping is proposed to nonlinearly adjust the data. Optimization of the data transformation parameters is performed in two different manners. Firstly, the complex data mapping parameters and kernel hyper-parameters are selected separately, with the former guided by frequency metrics and the latter under the Bayesian framework. Next, the complex data mapping parameters and kernel hyper-parameters are optimized simultaneously in a Bayesian formulation by creating a new category of "integrated kernel" with the complex data mapping embedded. Experiments using Gaussian Process learning have shown that both methods improve the learning accuracy in either classification or regression tasks, with the complex mapping embedded kernel approach outperforming the separate complex mapping one.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[complex mapping, kernel methods, Gaussian Process, frequency domain]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532437</person_id>
				<author_profile_id><![CDATA[81479649046]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532438</person_id>
				<author_profile_id><![CDATA[81340492287]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Fabio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ramos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532439</person_id>
				<author_profile_id><![CDATA[81447602318]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nettleton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934518</article_id>
		<sort_key>810</sort_key>
		<display_label>Pages</display_label>
		<pages>679-688</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>78</seq_no>
		<title><![CDATA[NESVM]]></title>
		<subtitle><![CDATA[A Fast Gradient Method for Support Vector Machines]]></subtitle>
		<page_from>679</page_from>
		<page_to>688</page_to>
		<doi_number>10.1109/ICDM.2010.135</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934518</url>
		<abstract>
			<par><![CDATA[Support vector machines (SVMs) are invaluable tools for many practical applications in artificial intelligence, e.g., classification and event recognition. However, popular SVM solvers are not sufficiently efficient for applications with a great deal of samples as well as a large number of features. In this paper, thus, we present NESVM, a fast gradient SVM solver that can optimize various SVM models, e.g., classical SVM, linear programming SVM and least square SVM. Compared against SVM-Perf \cite{SVM_Perf}\cite{PerfML} (whose convergence rate in solving the dual SVM is upper bounded by $\mathcal O(1/\sqrt{k})$ where $k$ is the number of iterations) and Pegasos \cite{Pegasos} (online SVM that converges at rate $\mathcal O(1/k)$ for the primal SVM), NESVM achieves the optimal convergence rate at $\mathcal O(1/k^{2})$ and a linear time complexity. In particular, NESVM smoothes the non-differentiable hinge loss and $\ell_1$-norm in the primal SVM. Then the optimal gradient method without any line search is adopted to solve the optimization. In each iteration round, the current gradient and historical gradients are combined to determine the descent direction, while the Lipschitz constant determines the step size. Only two matrix-vector multiplications are required in each iteration round. Therefore, NESVM is more efficient than existing SVM solvers. In addition, NESVM is available for both linear and nonlinear kernels. We also propose ``homotopy NESVM'' to accelerate NESVM by dynamically decreasing the smooth parameter and using the continuation method. Our experiments on census income categorization, indoor/outdoor scene classification, event recognition and scene recognition suggest the efficiency and the effectiveness of NESVM. The MATLAB code of NESVM will be available on our website for further assessment.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Support vector machines, smooth, hinge loss, $\ell_1$ norm, Nesterov's method, continuation method]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531846</person_id>
				<author_profile_id><![CDATA[81479641399]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tianyi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531847</person_id>
				<author_profile_id><![CDATA[81100159571]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dacheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531848</person_id>
				<author_profile_id><![CDATA[81452596585]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Xindong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934519</article_id>
		<sort_key>820</sort_key>
		<display_label>Pages</display_label>
		<pages>689-698</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>79</seq_no>
		<title><![CDATA[Clustering Large Attributed Graphs]]></title>
		<subtitle><![CDATA[An Efficient Incremental Approach]]></subtitle>
		<page_from>689</page_from>
		<page_to>698</page_to>
		<doi_number>10.1109/ICDM.2010.41</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934519</url>
		<abstract>
			<par><![CDATA[In recent years, many networks have become available for analysis, including social networks, sensor networks, biological networks, etc. Graph clustering has shown its effectiveness in analyzing and visualizing large networks. The goal of graph clustering is to partition vertices in a large graph into clusters based on various criteria such as vertex connectivity or neighborhood similarity. Many existing graph clustering methods mainly focus on the topological structures, but largely ignore the vertex properties which are often heterogeneous. Recently, a new graph clustering algorithm, SA-Cluster, has been proposed which combines structural and attribute similarities through a unified distance measure. SA-Cluster performs matrix multiplication to calculate the random walk distances between graph vertices. As the edge weights are iteratively adjusted to balance the importance between structural and attribute similarities, matrix multiplication is repeated in each iteration of the clustering process to recalculate the random walk distances which are affected by the edge weight update. In order to improve the efficiency and scalability of SA-Cluster, in this paper, we propose an efficient algorithm Inc-Cluster to incrementally update the random walk distances given the edge weight increments. Complexity analysis is provided to estimate how much runtime cost Inc-Cluster can save. Experimental results demonstrate that Inc-Cluster achieves significant speedup over SA-Cluster on large graphs, while achieving exactly the same clustering quality in terms of intra-cluster structural cohesiveness and attribute value homogeneity.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[graph clustering, incremental computation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530311</person_id>
				<author_profile_id><![CDATA[81438595405]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530312</person_id>
				<author_profile_id><![CDATA[81351596071]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cheng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530313</person_id>
				<author_profile_id><![CDATA[81447600785]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jeffrey]]></first_name>
				<middle_name><![CDATA[Xu]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934525</article_id>
		<sort_key>830</sort_key>
		<display_label>Pages</display_label>
		<pages>699-708</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>80</seq_no>
		<title><![CDATA[Mother Fugger]]></title>
		<subtitle><![CDATA[Mining Historical Manuscripts with Local Color Patches]]></subtitle>
		<page_from>699</page_from>
		<page_to>708</page_to>
		<doi_number>10.1109/ICDM.2010.11</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934525</url>
		<abstract>
			<par><![CDATA[Initiatives such as the Google Print Library Project and the Million Book Project have already archived more than ten million books in digital format, and within the next decade the majority of world&#8217;s books will be online. Although most of the data will naturally be text, there will also be tens of millions of pages of images, many in color. While there is an active research community pursuing data mining of text from historical manuscripts, there has been very little work that exploits the rich color information which is often present. In this work we introduce a simple color measure which both addresses and exploits typical features of historical manuscripts. To enable the efficient mining of massive archives, we propose a tight lower bound to the measure. Beyond the fast similarity search, we show how this lower bound allows us to build several higher-level data mining tools, including motif discovery and link analyses. We demonstrate our ideas in several data mining tasks on manuscripts dating back to the fifteenth century.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Historical Manuscripts, Color Indexing]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531905</person_id>
				<author_profile_id><![CDATA[81479653155]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Qiang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531906</person_id>
				<author_profile_id><![CDATA[81493650377]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eamonn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Keogh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934526</article_id>
		<sort_key>840</sort_key>
		<display_label>Pages</display_label>
		<pages>709-718</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>81</seq_no>
		<title><![CDATA[D-LDA]]></title>
		<subtitle><![CDATA[A Topic Modeling Approach without Constraint Generation for Semi-defined Classification]]></subtitle>
		<page_from>709</page_from>
		<page_to>718</page_to>
		<doi_number>10.1109/ICDM.2010.13</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934526</url>
		<abstract>
			<par><![CDATA[We study what we call semi-defined classification, which deals with the categorization tasks where the taxonomy of the data is not well defined in advance. It is motivated by the real-world applications, where the unlabeled data may also come from some other unknown classes besides the known classes for the labeled data. Given the unlabeled data, our goal is to not only identify the instances belonging to the known classes, but also cluster the remaining data into other meaningful groups. It differs from traditional semi-supervised clustering in the sense that in semi-supervised clustering the supervision knowledge is too far from being representative of a target classification, while in semi-defined classification the labeled data may be enough to supervise the learning on the known classes. In this paper we propose the model of Double-latent-layered LDA (D-LDA for short) for this problem. Compared with LDA with only one latent variable y for word topics, D-LDA contains another latent variable z for (known and unknown) document classes. With this double latent layers consisting of y and z and the dependency between them, D-LDA directly injects the class labels into z to supervise the exploiting of word topics in y. Thus, the semi-supervised learning in D-LDA does not need the generation of pair wise constraints, which is required in most of the previous semi-supervised clustering approaches. We present the experimental results on ten different data sets for semi-defined classification. Our results are either comparable to (on one data sets), or significantly better (on the other nine data set) than the six compared methods, including the state-of-the-art semi-supervised clustering methods.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Semi-defined classification, Topic modeling, Gibbs Sampling, Semi-supervised clustering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530341</person_id>
				<author_profile_id><![CDATA[81387604343]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fuzhen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhuang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530342</person_id>
				<author_profile_id><![CDATA[81100416055]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ping]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Luo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530343</person_id>
				<author_profile_id><![CDATA[81392598013]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Zhiyong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530344</person_id>
				<author_profile_id><![CDATA[81320490313]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Qing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[He]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530345</person_id>
				<author_profile_id><![CDATA[81548030167]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Yuhong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xiong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530346</person_id>
				<author_profile_id><![CDATA[81460654425]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Zhongzhi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934527</article_id>
		<sort_key>850</sort_key>
		<display_label>Pages</display_label>
		<pages>719-724</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>82</seq_no>
		<title><![CDATA[SONNET]]></title>
		<subtitle><![CDATA[Efficient Approximate Nearest Neighbor Using Multi-core]]></subtitle>
		<page_from>719</page_from>
		<page_to>724</page_to>
		<doi_number>10.1109/ICDM.2010.157</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934527</url>
		<abstract>
			<par><![CDATA[Approximate Nearest Neighbor search over high dimensional data is an important problem with a wide range of practical applications. In this paper, we propose SONNET, a simple multi-core friendly approximate nearest neighbor algorithm that is based on rank aggregation. SONNET is particularly suitable for very high dimensional data, its performance gets better as the dimension increases, whereas the majority of the existing algorithms show a reverse trend. Furthermore, most of the existing algorithms are hard to parallelize either due to the sequential nature of the algorithm or due to the inherent complexity of the algorithm. On the other hand, SONNET has inherent parallelism embedded in the core concept of the algorithm, which earns it almost a linear speed-up as the number of cores increases. Finally, SONNET is very easy to implement and it has an approximation parameter which is intuitively simple.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[nearest neighbors, approximate nearest neighbors, rank aggregation, early termination]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531907</person_id>
				<author_profile_id><![CDATA[84459832857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mohammad]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Al Hasan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531908</person_id>
				<author_profile_id><![CDATA[81381602507]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hilmi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yildirim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531909</person_id>
				<author_profile_id><![CDATA[81479656538]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Abhirup]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chakraborty]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934528</article_id>
		<sort_key>860</sort_key>
		<display_label>Pages</display_label>
		<pages>725-730</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>83</seq_no>
		<title><![CDATA[Two of a Kind or the Ratings Game? Adaptive Pairwise Preferences and Latent Factor Models]]></title>
		<page_from>725</page_from>
		<page_to>730</page_to>
		<doi_number>10.1109/ICDM.2010.149</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934528</url>
		<abstract>
			<par><![CDATA[While latent factor models are built using ratings data, which is typically assumed static, the ability to incorporate different kinds of subsequent user feedback is an important asset. For instance, the user might want to provide additional information to the system in order to improve his personal recommendations. To this end, we examine a novel scheme for efficiently learning (or refining) user parameters from such feedback. We propose a scheme where users are presented with a sequence of pair wise preference questions: "Do you prefer item A over B?". User parameters are updated based on their response, and subsequent questions are chosen adaptively after incorporating the feedback. We operate in a Bayesian framework and the choice of questions is based on an information gain criterion. We validate the scheme on the Netflix movie ratings data set. A user study and automated experiments validate our findings.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Recommender Systems, Latent factor models, Pairwise preferences, Active Learning]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530833</person_id>
				<author_profile_id><![CDATA[81375604623]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Suhrid]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Balakrishnan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530834</person_id>
				<author_profile_id><![CDATA[81100090708]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sumit]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chopra]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934529</article_id>
		<sort_key>870</sort_key>
		<display_label>Pages</display_label>
		<pages>731-736</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>84</seq_no>
		<title><![CDATA[Document Similarity Self-Join with MapReduce]]></title>
		<page_from>731</page_from>
		<page_to>736</page_to>
		<doi_number>10.1109/ICDM.2010.70</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934529</url>
		<abstract>
			<par><![CDATA[iven a collection of objects, the Similarity Self-Join problem requires to discover all those pairs of objects whose similarity is above a user defined threshold. In this paper we focus on document collections, which are characterized by a sparseness that allows effective pruning strategies. Our contribution is a new parallel algorithm within the MapReduce framework. This work borrows from the state of the art in serial algorithms for similarity join and MapReduce-based techniques for set-similarity join. The proposed algorithm shows that it is possible to leverage a distributed file system to support communication patterns that do not naturally fit the MapReduce framework. Scalability is achieved by introducing a partitioning strategy able to overcome memory bottlenecks. Experimental evidence on real world data shows that our algorithm outperforms the state of the art by a factor 4.5.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Similarity Self-Join, MapReduce, Web Information Retrieval]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530347</person_id>
				<author_profile_id><![CDATA[81100237305]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ranieri]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baraglia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530348</person_id>
				<author_profile_id><![CDATA[81479663507]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gianmarco]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[De Francisci Morales]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530349</person_id>
				<author_profile_id><![CDATA[81324491771]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Claudio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lucchese]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934546</article_id>
		<sort_key>880</sort_key>
		<display_label>Pages</display_label>
		<pages>737-742</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>85</seq_no>
		<title><![CDATA[Quantification via Probability Estimators]]></title>
		<page_from>737</page_from>
		<page_to>742</page_to>
		<doi_number>10.1109/ICDM.2010.75</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934546</url>
		<abstract>
			<par><![CDATA[Quantification is the name given to a novel machine learning task which deals with correctly estimating the number of elements of one class in a set of examples. The output of a quantifier is a real value, since training instances are the same as a classification problem, a natural approach is to train a classifier and to derive a quantifier from it. Some previous works have shown that just classifying the instances and counting the examples belonging to the class of interest classify count typically yields bad quantifiers, especially when the class distribution may vary between training and test. Hence, adjusted versions of classify count have been developed by using modified thresholds. However, previous works have explicitly discarded (without a deep analysis) any possible approach based on the probability estimations of the classifier. In this paper, we present a method based on averaging the probability estimations of a classifier with a very simple scaling that does perform reasonably well, showing that probability estimators for quantification capture a richer view of the problem than methods based on a threshold.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[quantification, probability estimators, class imbalance, classification]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532995</person_id>
				<author_profile_id><![CDATA[81460654437]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Antonio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bella]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532996</person_id>
				<author_profile_id><![CDATA[81100646460]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Cesar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ferri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532997</person_id>
				<author_profile_id><![CDATA[81337489981]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jose]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hernandez-Orallo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532998</person_id>
				<author_profile_id><![CDATA[81100368982]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Maria]]></first_name>
				<middle_name><![CDATA[Jose]]></middle_name>
				<last_name><![CDATA[Ramirez-Quintana]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934547</article_id>
		<sort_key>890</sort_key>
		<display_label>Pages</display_label>
		<pages>743-748</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>86</seq_no>
		<title><![CDATA[Learning Collaborative Filtering and Its Application to People to People Recommendation in Social Networks]]></title>
		<page_from>743</page_from>
		<page_to>748</page_to>
		<doi_number>10.1109/ICDM.2010.159</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934547</url>
		<abstract>
			<par><![CDATA[Predicting people who other people may like has recently become an important task in many online social networks. Traditional collaborative filtering (CF) approaches are popular in recommender systems to effectively predict user preferences for items. One major problem in CF is computing similarity between users or items. Traditional CF methods often use heuristic methods to combine the ratings given to an item by similar users, which may not reflect the characteristics of the active user and can give unsatisfactory performance. In contrast to heuristic approaches we have developed CollabNet, a novel algorithm that uses gradient descent to learn the relative contributions of similar users or items to the ranking of recommendations produced by a recommender system, using weights to represent the contributions of similar users for each active user. We have applied CollabNet to the challenging problem of people to people recommendation in social networks, where people have a dual role as both "users" and "items", e.g., both initiating and receiving communications, to recommend other users to a given user, based on user similarity in terms of both taste (whom they like) and attractiveness (who likes them). Evaluation of CollabNet recommendations on datasets from a commercial online social network shows improved performance over standard CF.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Data Mining, Machine Learning, Recommender Systems, Collaborative Filtering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530366</person_id>
				<author_profile_id><![CDATA[81384594810]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xiongcai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530367</person_id>
				<author_profile_id><![CDATA[81100044182]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bain]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530368</person_id>
				<author_profile_id><![CDATA[81100228155]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Alfred]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Krzywicki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530369</person_id>
				<author_profile_id><![CDATA[81100372298]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Wayne]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wobcke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530370</person_id>
				<author_profile_id><![CDATA[81387597790]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Yang]]></first_name>
				<middle_name><![CDATA[Sok]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530371</person_id>
				<author_profile_id><![CDATA[81406596439]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Compton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530372</person_id>
				<author_profile_id><![CDATA[81100609135]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Ashesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mahidadia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934548</article_id>
		<sort_key>900</sort_key>
		<display_label>Pages</display_label>
		<pages>749-754</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>87</seq_no>
		<title><![CDATA[Approximation of Frequentness Probability of Itemsets in Uncertain Data]]></title>
		<page_from>749</page_from>
		<page_to>754</page_to>
		<doi_number>10.1109/ICDM.2010.42</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934548</url>
		<abstract>
			<par><![CDATA[Mining frequent item sets from transactional datasets is a well known problem with good algorithmic solutions. Most of these algorithms assume that the input data is free from errors. Real data, however, is often affected by noise. Such noise can be represented by uncertain datasets in which each item has an existence probability. Recently, Bernecker et al. (2009) proposed the frequentness probability, i.e., the probability that a given item set is frequent, to select item sets in an uncertain database. A dynamic programming approach to evaluate this measure was given as well. We argue, however, that for the setting of Bernecker et al. (2009), that assumes independence between the items, already well-known statistical tools exist. We show how the frequentness probability can be approximated extremely accurately using a form of the central limit theorem. We experimentally evaluated our approximation and compared it to the dynamic programming approach. The evaluation shows that our approximation method is extremely accurate even for very small databases while at the same time it has much lower memory overhead and computation time.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530871</person_id>
				<author_profile_id><![CDATA[81100650328]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Toon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Calders]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530872</person_id>
				<author_profile_id><![CDATA[81479664967]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Calin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Garboni]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530873</person_id>
				<author_profile_id><![CDATA[81100529915]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Bart]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goethals]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934549</article_id>
		<sort_key>910</sort_key>
		<display_label>Pages</display_label>
		<pages>755-760</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>88</seq_no>
		<title><![CDATA[On Finding Frequent Patterns in Event Sequences]]></title>
		<page_from>755</page_from>
		<page_to>760</page_to>
		<doi_number>10.1109/ICDM.2010.132</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934549</url>
		<abstract>
			<par><![CDATA[Given a directed a cyclic graph with labeled vertices, we consider the problem of finding the most common label sequences ("traces") among all paths in the graph (of some maximum length m). Since the number of paths can be huge, we propose novel algorithms whose time complexity depends only on the size of the graph, and on the frequency \varepsilon of the most frequent traces. In addition, we apply techniques from streaming algorithms to achieve space usage that depends only on \varepsilon, and not on the number of distinct traces. The abstract problem considered models a variety of tasks concerning finding frequent patterns in event sequences. Our motivation comes from working with a data set of 2 million RFID readings from baggage trolleys at Copenhagen Airport. The question of finding frequent passenger movement patterns is mapped to the above problem. We report on experimental findings for this data set.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[algorithms, graphs, sampling, data mining, patterns discovery]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531953</person_id>
				<author_profile_id><![CDATA[81453661380]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrea]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Campagna]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531954</person_id>
				<author_profile_id><![CDATA[81100651427]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Rasmus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pagh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934550</article_id>
		<sort_key>920</sort_key>
		<display_label>Pages</display_label>
		<pages>761-766</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>89</seq_no>
		<title><![CDATA[Active Improvement of Hierarchical Object Features under Budget Constraints]]></title>
		<page_from>761</page_from>
		<page_to>766</page_to>
		<doi_number>10.1109/ICDM.2010.74</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934550</url>
		<abstract>
			<par><![CDATA[When we think of an object in a supervised learning setting, we usually perceive it as a collection of fixed attribute values. Although this setting may be suited well for many classification tasks, we propose a new object representation and therewith a new challenge in data mining: an object is no longer described by one set of attributes but is represented in a hierarchy of attribute sets in different levels of quality. Obtaining a more detailed representation of an object comes with a cost. This raises the interesting question of which objects we want to enhance under a given budget and cost model. This new setting is very useful whenever resources like computing power, memory or time are limited. We propose a new Active Adaptive Algorithm (AAA) to improve objects in an iterative fashion. We demonstrate how to create a hierarchical object representation and prove the effectiveness of our new selection algorithm on these datasets.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Pattern classification, Active vision]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530874</person_id>
				<author_profile_id><![CDATA[81385591772]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nicolas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cebron]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934555</article_id>
		<sort_key>930</sort_key>
		<display_label>Pages</display_label>
		<pages>767-772</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>90</seq_no>
		<title><![CDATA[Pseudo Conditional Random Fields]]></title>
		<subtitle><![CDATA[Joint Training Approach to Segmenting and Labeling Sequence Data]]></subtitle>
		<page_from>767</page_from>
		<page_to>772</page_to>
		<doi_number>10.1109/ICDM.2010.99</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934555</url>
		<abstract>
			<par><![CDATA[Cascaded approach has been used for a long time to conduct sub-tasks in order to accomplish a major task. We put cascaded approach in a probabilistic framework and analyze possible reasons for cascaded errors. To reduce the occurrence of cascaded errors, we need to add a constraint when performing joint training. We suggest a pseudo Conditional Random Field (pseudo-CRF) approach that models two sub-tasks as two Conditional Random Fields (CRFs). We then present the formulation in the context of a linear chain CRF for solving problems on sequence data. In conducting joint training for a pseudo-CRF, we reuse all existing well-developed efficient inference algorithms for a linear chain CRF, which would otherwise require the use of approximate inference algorithms or simulations that involve long computational time. Our experimental results show an interesting fact that a jointly trained CRF model in a pseudo-CRF may perform worse than a separately trained CRF on a sub-task. However the overall system performance of a pseudo-CRF would outperform that of a cascaded approach. We implement the implicit constraint in the form of a soft constraint such that users can define the penalty cost for violating the constraint. In order to work on large-scale datasets, we further suggest a parallel implementation of the pseudo-CRF approach, which can be implemented on a multi-core CPU or GPU on a graphics card that supports multi-threading. Our experimental results show that it can achieve a 12 times increase in speedup.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[CRF, Sequence Labeling Problem, Cascaded Approach, Noun-phrase Chunking, Joint Training]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530897</person_id>
				<author_profile_id><![CDATA[81479659385]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shing-Kit]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530898</person_id>
				<author_profile_id><![CDATA[81479643973]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934556</article_id>
		<sort_key>940</sort_key>
		<display_label>Pages</display_label>
		<pages>773-778</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>91</seq_no>
		<title><![CDATA[Location and Scatter Matching for Dataset Shift in Text Mining]]></title>
		<page_from>773</page_from>
		<page_to>778</page_to>
		<doi_number>10.1109/ICDM.2010.72</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934556</url>
		<abstract>
			<par><![CDATA[Dataset shift from the training data in a source domain to the data in a target domain poses a great challenge for many statistical learning methods. Most algorithms can be viewed as exploiting only the first-order statistics, namely, the empirical mean discrepancy to evaluate the distribution gap. Intuitively, considering only the empirical mean may not be statistically efficient. In this paper, we propose a non-parametric distance metric with a good property which jointly considers the empirical mean (Location) and sample covariance (Scatter) difference. More specifically, we propose an improved symmetric Stein's loss function which combines the mean and covariance discrepancy into a unified Bregman matrix divergence of which Jensen-Shannon divergence between normal distributions is a particular case. Our target is to find a good feature representation which can reduce the distribution gap between different domains, at the same time, ensure that the new derived representation can encode most discriminative components with respect to the label information. We have conducted extensive experiments on several document classification datasets to demonstrate the effectiveness of our proposed method.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Domain Adaptation, Feature Extraction]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2529818</person_id>
				<author_profile_id><![CDATA[81436594457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529819</person_id>
				<author_profile_id><![CDATA[81423592360]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529820</person_id>
				<author_profile_id><![CDATA[81309487444]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ivor]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tsang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529821</person_id>
				<author_profile_id><![CDATA[81474647120]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tak-Lam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934557</article_id>
		<sort_key>950</sort_key>
		<display_label>Pages</display_label>
		<pages>779-784</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>92</seq_no>
		<title><![CDATA[Learning Preferences with Millions of Parameters by Enforcing Sparsity]]></title>
		<page_from>779</page_from>
		<page_to>784</page_to>
		<doi_number>10.1109/ICDM.2010.67</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934557</url>
		<abstract>
			<par><![CDATA[We study the retrieval task that ranks a set of objects for a given query in the pair wise preference learning framework. Recently researchers found out that raw features (e.g. words for text retrieval) and their pair wise features which describe relationships between two raw features (e.g. word synonymy or polysemy) could greatly improve the retrieval precision. However, most existing methods can not scale up to problems with many raw features (e.g. English vocabulary), due to the prohibitive computational cost on learning and the memory requirement to store a quadratic number of parameters. In this paper, we propose to learn a sparse representation of the pair wise features under the preference learning framework using the L1 regularization. Based on stochastic gradient descent, an online algorithm is devised to enforce the sparsity using a mini-batch shrinkage strategy. On multiple benchmark datasets, we show that our method achieves better performance with fast convergence, and takes much less memory on models with millions of parameters.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[preference learning, sparse model, online learning, learning to rank, text mining]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530899</person_id>
				<author_profile_id><![CDATA[81479650071]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530900</person_id>
				<author_profile_id><![CDATA[81315487782]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530901</person_id>
				<author_profile_id><![CDATA[81447597003]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yanjun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Qi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530902</person_id>
				<author_profile_id><![CDATA[81479655089]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Qihang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530903</person_id>
				<author_profile_id><![CDATA[81452611253]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jaime]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Carbonell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934558</article_id>
		<sort_key>960</sort_key>
		<display_label>Pages</display_label>
		<pages>785-790</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>93</seq_no>
		<title><![CDATA[QMAS]]></title>
		<subtitle><![CDATA[Querying, Mining and Summarization of Multi-modal Databases]]></subtitle>
		<page_from>785</page_from>
		<page_to>790</page_to>
		<doi_number>10.1109/ICDM.2010.150</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934558</url>
		<abstract>
			<par><![CDATA[Given a large collection of images, very few of which have labels, how can we guess the labels of the remaining majority, and how can we spot those images that need brand new labels, different from the existing ones? Current automatic labeling techniques usually scale super linearly with the data size, and/or they fail when only a tiny amount of labeled data is provided. In this paper, we propose QMAS (Querying, Mining And Summarization of Multi-modal Databases), a fast solution to the following problems: (i) low-labor labeling (L3) &#8211; given a collection of images, very few of which are labeled with keywords, find the most suitable labels for the remaining ones, and (ii) mining and attention routing &#8211; in the same setting, find clusters, the top-NO outlier images, and the top-NR representative images. We report experiments on real satellite images, two large sets (1.5GB and 2.25GB) of proprietary images and a smaller set (17MB) of public images. We show that QMAS scales linearly with the data size, being up to 40 times faster than top competitors (GCap), obtaining better or equal accuracy. In contrast to other methods, QMAS does low-labor labeling (L3), that is, it works even with tiny initial label sets. It also solves both presented problems and spots tiles that potentially require new labels.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Automatic labeling, Clustering, Summarization and Multi-modal databases]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530904</person_id>
				<author_profile_id><![CDATA[81479652616]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robson]]></first_name>
				<middle_name><![CDATA[L. F.]]></middle_name>
				<last_name><![CDATA[Cordeiro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530905</person_id>
				<author_profile_id><![CDATA[81479647676]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Fan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530906</person_id>
				<author_profile_id><![CDATA[81479657209]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Donna]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Haverkamp]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530907</person_id>
				<author_profile_id><![CDATA[81479652211]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Horne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530908</person_id>
				<author_profile_id><![CDATA[81479652995]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ellen]]></first_name>
				<middle_name><![CDATA[K.]]></middle_name>
				<last_name><![CDATA[Hughes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530909</person_id>
				<author_profile_id><![CDATA[81479644925]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Gunhee]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530910</person_id>
				<author_profile_id><![CDATA[81416605532]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Agma]]></first_name>
				<middle_name><![CDATA[J. M.]]></middle_name>
				<last_name><![CDATA[Traina]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530911</person_id>
				<author_profile_id><![CDATA[81442619226]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Caetano]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Traina Jr.]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530912</person_id>
				<author_profile_id><![CDATA[81479659309]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Christos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Faloutsos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934564</article_id>
		<sort_key>970</sort_key>
		<display_label>Pages</display_label>
		<pages>791-796</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>94</seq_no>
		<title><![CDATA[Block-GP]]></title>
		<subtitle><![CDATA[Scalable Gaussian Process Regression for Multimodal Data]]></subtitle>
		<page_from>791</page_from>
		<page_to>796</page_to>
		<doi_number>10.1109/ICDM.2010.38</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934564</url>
		<abstract>
			<par><![CDATA[Regression problems on massive data sets are ubiquitous in many application domains including the Internet, earth and space sciences, and finances. In many cases, regression algorithms such as linear regression or neural networks attempt to fit the target variable as a function of the input variables without regard to the underlying joint distribution of the variables. As a result, these global models are not sensitive to variations in the local structure of the input space. Several algorithms, including the mixture of experts model, classification and regression trees (CART), and others have been developed, motivated by the fact that a variability in the local distribution of inputs may be reflective of a significant change in the target variable. While these methods can handle the non-stationarity in the relationships to varying degrees, they are often not scalable and, therefore, not used in large scale data mining applications. In this paper we develop Block-GP, a Gaussian Process regression framework for multimodal data, that can be an order of magnitude more scalable than existing state-of-the-art nonlinear regression algorithms. The framework builds local Gaussian Processes on semantically meaningful partitions of the data and provides higher prediction accuracy than a single global model with very high confidence. The method relies on approximating the covariance matrix of the entire input space by smaller covariance matrices that can be modeled independently, and can therefore be parallelized for faster execution. Theoretical analysis and empirical studies on various synthetic and real data sets show high accuracy and scalability of Block-GP compared to existing nonlinear regression techniques.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Gaussian Process, regression, parallel computation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530430</person_id>
				<author_profile_id><![CDATA[81326488446]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kamalika]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Das]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530431</person_id>
				<author_profile_id><![CDATA[81479652065]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ashok]]></first_name>
				<middle_name><![CDATA[N.]]></middle_name>
				<last_name><![CDATA[Srivastava]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934559</article_id>
		<sort_key>980</sort_key>
		<display_label>Pages</display_label>
		<pages>797-802</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>95</seq_no>
		<title><![CDATA[Active Learning with Human-Like Noisy Oracle]]></title>
		<page_from>797</page_from>
		<page_to>802</page_to>
		<doi_number>10.1109/ICDM.2010.114</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934559</url>
		<abstract>
			<par><![CDATA[When active learning is applied to real-world applications, human experts usually act as oracles to provide labels. However, human make mistakes, thus noise might be introduced during the learning process. Most previous studies simplify the problem by assuming uniformly-distributed noise over the sample space. Such assumption, however, might fail to precisely reflect the human experts' behaviour in real-world situations. In this paper, we therefore study active learning with such human-like oracles, by making a more realistic assumption that the noise is example-dependent (i.e., non-uniformly distributed over the sample space). More specifically, when the human-like oracle is highly confident in labelling examples, it is naturally less likely to provide incorrect answers, whereas when such confidence is low, the noise would be more likely to be introduced. Based on the analysis of such human-like oracle, we propose a generic yet simple active learning algorithm to simultaneously explore the unlabelled data and exploit the labelled data. Empirical study on both synthetic and real-world data sets verifies the superiority of the proposed algorithm, compared with the traditional uncertainty sampling.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[active learning, oracle, noise]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530396</person_id>
				<author_profile_id><![CDATA[81479657939]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Du]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530397</person_id>
				<author_profile_id><![CDATA[81100159332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Charles]]></first_name>
				<middle_name><![CDATA[X.]]></middle_name>
				<last_name><![CDATA[Ling]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934565</article_id>
		<sort_key>990</sort_key>
		<display_label>Pages</display_label>
		<pages>803-808</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>96</seq_no>
		<title><![CDATA[Monotone Relabeling in Ordinal Classification]]></title>
		<page_from>803</page_from>
		<page_to>808</page_to>
		<doi_number>10.1109/ICDM.2010.92</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934565</url>
		<abstract>
			<par><![CDATA[In many applications of data mining we know beforehand that the response variable should be increasing (or decreasing) in the attributes. Such relations between response and attributes are called monotone. In this paper we present a new algorithm to compute an optimal monotone classification of a data set for convex loss functions. Moreover, we show how the algorithm can be extended to compute all optimal monotone classifications with little additional effort. Monotone relabeling is useful for at least two reasons. Firstly, models trained on relabeled data sets often have better predictive performance than models trained on the original data. Secondly, relabeling is an important building block for the construction of monotone classifiers. We apply the new algorithm to investigate the effect on the prediction error of relabeling the training sample for $k$ nearest neighbour classification and classification trees. In contrast to previous work in this area, we consider {\em all} optimal monotone relabelings. The results show that, for small training samples, relabeling the training data results in significantly better predictive performance.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2529865</person_id>
				<author_profile_id><![CDATA[81100162409]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ad]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Feelders]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934566</article_id>
		<sort_key>1000</sort_key>
		<display_label>Pages</display_label>
		<pages>809-814</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>97</seq_no>
		<title><![CDATA[The Effect of History on Modeling Systems' Performance]]></title>
		<subtitle><![CDATA[The Problem of the Demanding Lord]]></subtitle>
		<page_from>809</page_from>
		<page_to>814</page_to>
		<doi_number>10.1109/ICDM.2010.90</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934566</url>
		<abstract>
			<par><![CDATA[In several concept attainment systems, ranging from recommendation systems to information filtering, a sliding window of learning instances has been used in the learning process to allow the learner to follow concepts that change over time. However, no analytic study has been performed on the relation between the size of the sliding window and the performance of a learning system. In this work, we present such an analytic model that describes the effect of the sliding window size on the prediction performance of a learning system based on iterative feedback. Using a signal-to-noise approach to model the learning ability of the underlying machine learning algorithms, we can provide good estimates of the average performance of a modeling system independently of the supervised machine learning algorithm employed. We experimentally validate the effectiveness of the proposed methodology with detailed experiments using synthetic and real datasets, and a variety of learning algorithms, including Support Vector Machines, Naive Bayes, Nearest Neighbor and Decision Trees. The results validate the analysis and indicate very good estimation performance in different settings.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[concept drift, user modeling, adaptive learning, demanding lord problem]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532005</person_id>
				<author_profile_id><![CDATA[81453605685]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Giannakopoulos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532006</person_id>
				<author_profile_id><![CDATA[81331501619]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Themis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Palpanas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934567</article_id>
		<sort_key>1010</sort_key>
		<display_label>Pages</display_label>
		<pages>815-820</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>98</seq_no>
		<title><![CDATA[Resilient K-d Trees]]></title>
		<subtitle><![CDATA[K-Means in Space Revisited]]></subtitle>
		<page_from>815</page_from>
		<page_to>820</page_to>
		<doi_number>10.1109/ICDM.2010.94</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934567</url>
		<abstract>
			<par><![CDATA[We develop a k-d tree variant that is resilient to a pre-described number of memory corruptions while still using only linear space. We show how to use this data structure in the context of clustering in high-radiation environments and demonstrate that our approach leads to a significantly higher resiliency rate compared to previous results.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[k-means, k-d tree, resilient algorithm, clustering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532567</person_id>
				<author_profile_id><![CDATA[81464667939]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fabian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gieseke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532568</person_id>
				<author_profile_id><![CDATA[81367593590]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gabriel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Moruz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532569</person_id>
				<author_profile_id><![CDATA[81100502961]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vahrenhold]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934568</article_id>
		<sort_key>1020</sort_key>
		<display_label>Pages</display_label>
		<pages>821-826</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>99</seq_no>
		<title><![CDATA[Advertising Campaigns Management]]></title>
		<subtitle><![CDATA[Should We Be Greedy?]]></subtitle>
		<page_from>821</page_from>
		<page_to>826</page_to>
		<doi_number>10.1109/ICDM.2010.78</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934568</url>
		<abstract>
			<par><![CDATA[We consider the problem of displaying advertisements on web pages in the "cost per click" model, which necessitates to learn the appeal of visitors for the different advertisements in order to maximize the revenue. In a realistic context, the advertisements have constraints such as a certain number of clicks to draw, as well as a lifetime. This problem is thus inherently dynamic, and intimately combines combinatorial and statistical issues. To set the stage, it is also noteworthy that we deal with very rare events of interest, since the base probability of one click is in the order of 10^-4. We introduce an adaptive policy learning algorithm based on linear programming, and investigate its performance through simulations on a realistic model designed with an important commercial web actor.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Advertisement selection, Optimization, Non-stationary setting, Linear Programming, CTR estimation, Exploration/exploitation trade-off]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2529866</person_id>
				<author_profile_id><![CDATA[81314481978]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sertan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Girgin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529867</person_id>
				<author_profile_id><![CDATA[81100431042]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jeremie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mary]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529868</person_id>
				<author_profile_id><![CDATA[81100596997]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Philippe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Preux]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529869</person_id>
				<author_profile_id><![CDATA[81479663940]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Olivier]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nicol]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934569</article_id>
		<sort_key>1030</sort_key>
		<display_label>Pages</display_label>
		<pages>827-832</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>100</seq_no>
		<title><![CDATA[Accelerating Radius-Margin Parameter Selection for SVMs Using Geometric Bounds]]></title>
		<page_from>827</page_from>
		<page_to>832</page_to>
		<doi_number>10.1109/ICDM.2010.100</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934569</url>
		<abstract>
			<par><![CDATA[By considering the geometric properties of the Support Vector Machine (SVM) and Minimal Enclosing Ball (MEB) optimization problems, we show that upper and lower bounds on the radius-margin ratio of an SVM can be efficiently computed at any point during training. We use these bounds to accelerate radius-margin parameter selection by terminating training routines as early as possible, while still obtaining a guarantee that the parameters minimize the radius-margin ratio. Once an SVM has been partially trained on any set of parameters, we also show that these bounds can be used to evaluate and possibly reject neighboring parameter values with little or no additional training required. Empirical results show that, when selecting two parameter values, this process can reduce the number of training iterations required by a factor of 10 or more, while suffering no loss of precision in minimizing the radius-margin ratio.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[support vector machines, parameter selection, computational geometry]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530962</person_id>
				<author_profile_id><![CDATA[81453643661]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ben]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goodrich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530963</person_id>
				<author_profile_id><![CDATA[81100358366]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Albrecht]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530964</person_id>
				<author_profile_id><![CDATA[81100551242]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tischer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934574</article_id>
		<sort_key>1040</sort_key>
		<display_label>Pages</display_label>
		<pages>833-838</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>101</seq_no>
		<title><![CDATA[Enhancing Single-Objective Projective Clustering Ensembles]]></title>
		<page_from>833</page_from>
		<page_to>838</page_to>
		<doi_number>10.1109/ICDM.2010.138</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934574</url>
		<abstract>
			<par><![CDATA[Projective Clustering Ensembles (PCE) has recently been formulated to solve the problem of deriving a robust projective consensus clustering from an ensemble of projective clustering solutions. PCE is formalized as an optimization problem with either a two-objective or a single-objective function, depending on whether the object-based and the feature-based representations of the clusters in the ensemble are treated separately. A major result in is that single-objective PCE outperforms two-objective PCE in terms of efficiency, at the cost of lower accuracy in consensus clustering. In this paper, we enhance the single-objective PCE formulation, with the ultimate goal of providing more effective formulations capable of reducing the accuracy gap with the two-objective counterpart, while maintaining the efficiency advantages. We provide theoretical insights into the single-objective function, and introduce two heuristics that overcome the major limitations of the previous single-objective PCE formulation. Experimental evidence has demonstrated the significance of our proposed heuristics. In fact, results have not only confirmed a far better efficiency w.r.t. two-objective PCE, but have also shown the claimed improvements in accuracy of the consensus clustering obtained by the new single-objective PCE.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530986</person_id>
				<author_profile_id><![CDATA[81335491254]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Francesco]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gullo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530987</person_id>
				<author_profile_id><![CDATA[81100062921]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Carlotta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Domeniconi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530988</person_id>
				<author_profile_id><![CDATA[81100286971]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Andrea]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tagarelli]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934575</article_id>
		<sort_key>1050</sort_key>
		<display_label>Pages</display_label>
		<pages>839-844</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>102</seq_no>
		<title><![CDATA[Minimizing the Variance of Cluster Mixture Models for Clustering Uncertain Objects]]></title>
		<page_from>839</page_from>
		<page_to>844</page_to>
		<doi_number>10.1109/ICDM.2010.134</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934575</url>
		<abstract>
			<par><![CDATA[The increasing demand for dealing with uncertainty in data has led to the development of effective and efficient approaches in the data management and mining contexts. Clustering uncertain data objects has particularly attracted great attention in the data mining community. Most existing clustering methods however have urgently to come up with a number of issues, some of which are related to a poor efficiency mainly due to an expensive computation of the distance between uncertain objects. In this work, we propose a novel formulation to the problem of clustering uncertain objects, which allows for reaching accurate solutions by minimizing the variance of the mixture models that represent the clusters to be identified. We define a heuristic, MMVar, which exploits some analytical properties about the computation of variance for mixture models to compute local minima of the objective function at the basis of the proposed formulation. This characteristic allows MMVar to discard any distance measure between uncertain objects and, therefore, to achieve high efficiency. Experiments have shown that MMVar outperforms state-of-the-art algorithms from an efficiency viewpoint, while achieving better average performance in terms of accuracy.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532050</person_id>
				<author_profile_id><![CDATA[81335491254]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Francesco]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gullo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532051</person_id>
				<author_profile_id><![CDATA[81335496174]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Giovanni]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ponti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532052</person_id>
				<author_profile_id><![CDATA[81100286971]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Andrea]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tagarelli]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934576</article_id>
		<sort_key>1060</sort_key>
		<display_label>Pages</display_label>
		<pages>845-850</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>103</seq_no>
		<title><![CDATA[Subspace Clustering Meets Dense Subgraph Mining]]></title>
		<subtitle><![CDATA[A Synthesis of Two Paradigms]]></subtitle>
		<page_from>845</page_from>
		<page_to>850</page_to>
		<doi_number>10.1109/ICDM.2010.95</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934576</url>
		<abstract>
			<par><![CDATA[Today's applications deal with multiple types of information: graph data to represent the relations between objects and attribute data to characterize single objects. Analyzing both data sources simultaneously can increase the quality of mining methods. Recently, combined clustering approaches were introduced, which detect densely connected node sets within one large graph that also show high similarity according to all of their attribute values. However, for attribute data it is known that this full-space clustering often leads to poor clustering results. Thus, subspace clustering was introduced to identify locally relevant subsets of attributes for each cluster. In this work, we propose a method for finding homogeneous groups by joining the paradigms of subspace clustering and dense sub graph mining, i.e. we determine sets of nodes that show high similarity in subsets of their dimensions and that are as well densely connected within the given graph. Our twofold clusters are optimized according to their density, size, and number of relevant dimensions. Our developed redundancy model confines the clustering to a manageable size of only the most interesting clusters. We introduce the algorithm Gamer for the efficient calculation of our clustering. In thorough experiments on synthetic and real world data we show that Gamer achieves low runtimes and high clustering qualities.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[subspace clustering, dense subgraph mining, attribute data, graph data, combined clustering approach, redundancy removal]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533653</person_id>
				<author_profile_id><![CDATA[81447604694]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stephan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gunnemann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533654</person_id>
				<author_profile_id><![CDATA[81447603438]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ines]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Farber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533655</person_id>
				<author_profile_id><![CDATA[81479663668]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Brigitte]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Boden]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533656</person_id>
				<author_profile_id><![CDATA[81100145971]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seidl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934577</article_id>
		<sort_key>1070</sort_key>
		<display_label>Pages</display_label>
		<pages>851-856</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>104</seq_no>
		<title><![CDATA[Multi-stream Join Answering for Mining Significant Cross-Stream Correlations]]></title>
		<page_from>851</page_from>
		<page_to>856</page_to>
		<doi_number>10.1109/ICDM.2010.167</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934577</url>
		<abstract>
			<par><![CDATA[Sliding-window multi-stream join (SWMJ) is a fundamental operation for correlating information from different streams. We provide a solution to the problem of assessing significance of the SWMJ result by focusing on the relative frequency of windows satisfying a given equijoin predicate as the most important parameter of the SWMJ result. In particular, we derive an analytic formula for computing the average relative frequency of windows satisfying a given equijoin predicate that can be evaluated in quadratic time in the window size given a probabilistic model of the multi-stream. In experiments we demonstrated remarkable accuracy of our method, which confirmed our theoretical analysis.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2529921</person_id>
				<author_profile_id><![CDATA[81343493912]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gwadera]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934578</article_id>
		<sort_key>1080</sort_key>
		<display_label>Pages</display_label>
		<pages>857-862</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>105</seq_no>
		<title><![CDATA[Category Mining by Heterogeneous Data Fusion Using PdLSI Model in a Retail Service]]></title>
		<page_from>857</page_from>
		<page_to>862</page_to>
		<doi_number>10.1109/ICDM.2010.83</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934578</url>
		<abstract>
			<par><![CDATA[This paper describes an appropriate category discovery method that simultaneously involves a customer's lifestyle category and item category for the sustainable management of retail services, designated as ``category mining''. Category mining is realized using a large-scale ID-POS data and customer's questionnaire responses with respect to their lifestyle. For the heterogeneous data fusion, we propose a probabilistic double-latent semantic indexing (PdLSI) model that is an extension of PLSI model. In the PdLSI model, customers and items are classified probabilistically into some latent lifestyle categories and latent item category. Then, understanding of relation between the latent categories and various purchased situations is realized using Bayesian network modeling. This method provides useful knowledge based on a large-scale data for efficient customer relationship management and category management, and can be applicable for other service industries.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[service engineering, topic model, large-scale ID-POS data, heterogeneous data fusion, Bayesian network]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532602</person_id>
				<author_profile_id><![CDATA[81416605172]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tsukasa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ishigaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532603</person_id>
				<author_profile_id><![CDATA[81479643455]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takenaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532604</person_id>
				<author_profile_id><![CDATA[81416598233]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yoichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Motomura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934584</article_id>
		<sort_key>1090</sort_key>
		<display_label>Pages</display_label>
		<pages>863-868</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>106</seq_no>
		<title><![CDATA[Content-Based Methods for Predicting Web-Site Demographic Attributes]]></title>
		<page_from>863</page_from>
		<page_to>868</page_to>
		<doi_number>10.1109/ICDM.2010.97</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934584</url>
		<abstract>
			<par><![CDATA[Demographic information plays an important role in gaining valuable insights about a web-site's user-base and is used extensively to target online advertisements and promotions. This paper investigates machine-learning approaches for predicting the demographic attributes of web-sites using information derived from their content and their hyper linked structure and not relying on any information directly or indirectly obtained from the web-site's users. Such methods are important because users are becoming increasingly more concerned about sharing their personal and behavioral information on the Internet. Regression-based approaches are developed and studied for predicting demographic attributes that utilize different content-derived features, different ways of building the prediction models, and different ways of aggregating web-page level predictions that take into account the web's hyper linked structure. In addition, a matrix-approximation based approach is developed for coupling the predictions of individual regression models into a model designed to predict the probability mass function of the attribute. Extensive experiments show that these methods are able to achieve an RMSE of 8-10% and provide insights on how to best train and apply such models.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Demographic Attribute Prediction, Content Based Models, Regression, Inlink Count, Probability Mass Function]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533127</person_id>
				<author_profile_id><![CDATA[81479663048]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Santosh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kabbur]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533128</person_id>
				<author_profile_id><![CDATA[81452618273]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eui-Hong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Han]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533129</person_id>
				<author_profile_id><![CDATA[81548023798]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Karypis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934585</article_id>
		<sort_key>1100</sort_key>
		<display_label>Pages</display_label>
		<pages>869-874</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>107</seq_no>
		<title><![CDATA[Discrimination Aware Decision Tree Learning]]></title>
		<page_from>869</page_from>
		<page_to>874</page_to>
		<doi_number>10.1109/ICDM.2010.50</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934585</url>
		<abstract>
			<par><![CDATA[Recently, the following discrimination aware classification problem was introduced: given a labeled dataset and an attribute B, find a classifier with high predictive accuracy that at the same time does not discriminate on the basis of the given attribute B. This problem is motivated by the fact that often available historic data is biased due to discrimination, e.g., when B denotes ethnicity. Using the standard learners on this data may lead to wrongfully biased classifiers, even if the attribute B is removed from training data. Existing solutions for this problem consist in &#8220;cleaning away&#8221; the discrimination from the dataset before a classifier is learned. In this paper we study an alternative approach in which the non-discrimination constraint is pushed deeply into a decision tree learner by changing its splitting criterion and pruning strategy. Experimental evaluation shows that the proposed approach advances the state-of-the-art in the sense that the learned decision trees have a lower discrimination than models provided by previous methods, with little loss in accuracy.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Data Mining, Classification, Discrimination Aware Data Mining]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2529962</person_id>
				<author_profile_id><![CDATA[81453661983]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Faisal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kamiran]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529963</person_id>
				<author_profile_id><![CDATA[81100650328]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Toon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Calders]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529964</person_id>
				<author_profile_id><![CDATA[81100136358]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mykola]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pechenizkiy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934586</article_id>
		<sort_key>1110</sort_key>
		<display_label>Pages</display_label>
		<pages>875-880</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>108</seq_no>
		<title><![CDATA[Patterns on the Connected Components of Terabyte-Scale Graphs]]></title>
		<page_from>875</page_from>
		<page_to>880</page_to>
		<doi_number>10.1109/ICDM.2010.121</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934586</url>
		<abstract>
			<par><![CDATA[How do connected components evolve? What are the regularities that govern the dynamic growth process and the static snapshot of the connected components? In this work, we study patterns in connected components of large, real-world graphs. First, we study one of the largest static Web graphs with billions of nodes and edges and analyze the regularities among the connected components using GFD(Graph Fractal Dimension) as our main tool. Second, we study several time evolving graphs and find dynamic patterns and rules that govern the dynamics of connected components. We analyze the growth rates of top connected components and study their relation over time. We also study the probability that a newcomer absorbs to disconnected components as a function of the current portion of the disconnected components and the degree of the newcomer. Finally, we propose a generative model that explains both the dynamic growth process and the static regularities of connected components.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Evolution of Connected Components, CommunityConnection Model, Graph Mining]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532643</person_id>
				<author_profile_id><![CDATA[81100492259]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[U.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532644</person_id>
				<author_profile_id><![CDATA[81367599088]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mary]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McGlohon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532645</person_id>
				<author_profile_id><![CDATA[81414592499]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Leman]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Akoglu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532646</person_id>
				<author_profile_id><![CDATA[81548006169]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Christos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Faloutsos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934587</article_id>
		<sort_key>1120</sort_key>
		<display_label>Pages</display_label>
		<pages>881-886</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>109</seq_no>
		<title><![CDATA[Attribution of Conversion Events to Multi-channel Media]]></title>
		<page_from>881</page_from>
		<page_to>886</page_to>
		<doi_number>10.1109/ICDM.2010.161</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934587</url>
		<abstract>
			<par><![CDATA[This paper presents a practical method for measuring the impact of multiple marketing events on sales, including marketing events that are not traditionally trackable. The technique infers which of several competing media events are likely to have caused a given conversion. We test the method using hold-out sets, and also a live media experiment in which we test whether the method can accurately predict television-generated web conversions.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[credit assignment, attribution, advertising]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531534</person_id>
				<author_profile_id><![CDATA[81100654107]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Brendan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kitts]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531535</person_id>
				<author_profile_id><![CDATA[81479645358]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Liang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wei]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531536</person_id>
				<author_profile_id><![CDATA[81479645635]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Dyng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Au]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531537</person_id>
				<author_profile_id><![CDATA[81479660513]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Amanda]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Powter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531538</person_id>
				<author_profile_id><![CDATA[81479654349]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Burdick]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934588</article_id>
		<sort_key>1130</sort_key>
		<display_label>Pages</display_label>
		<pages>887-892</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>110</seq_no>
		<title><![CDATA[Mining Public Transport Usage for Personalised Intelligent Transport Systems]]></title>
		<page_from>887</page_from>
		<page_to>892</page_to>
		<doi_number>10.1109/ICDM.2010.46</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934588</url>
		<abstract>
			<par><![CDATA[Traveller information, route planning, and service updates have become essential components of public transport systems: they help people navigate built environments by providing access to information regarding delays and service disruptions. However, one aspect that these systems lack is a way of tailoring the information they offer in order to provide personalised trip time estimates and relevant notifications to each traveller. Mining each user&#8217;s travel history, collected by automated ticketing systems, has the potential to address this gap. In this work, we analyse one such dataset of travel history on the London underground. We then propose and evaluate methods to (a) predict personalised trip times for the system users and (b) rank stations based on future mobility patterns, in order to identify the subset of stations that are of greatest interest to the user and thus provide useful travel updates.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Intelligent Transport Systems, Personalization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531539</person_id>
				<author_profile_id><![CDATA[81350596151]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Neal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lathia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531540</person_id>
				<author_profile_id><![CDATA[81331492504]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Froehlich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531541</person_id>
				<author_profile_id><![CDATA[81100419515]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Licia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Capra]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934589</article_id>
		<sort_key>1140</sort_key>
		<display_label>Pages</display_label>
		<pages>893-898</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>111</seq_no>
		<title><![CDATA[Micro-blogging Sentiment Detection by Collaborative Online Learning]]></title>
		<page_from>893</page_from>
		<page_to>898</page_to>
		<doi_number>10.1109/ICDM.2010.139</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934589</url>
		<abstract>
			<par><![CDATA[We study the online micro-blog sentiment detection problem, which aims to determine whether a micro-blog post expresses emotions. This problem is challenging because a micro-blog post is very short and individuals have distinct ways of expressing emotions. A single classification model trained on the entire corpus may fail to capture characteristics unique to each user. On the other hand, a personalized model for each user may be inaccurate due to the scarcity of training data, especially at the very beginning where users have just posted a few entries. To overcome these challenges, we propose learning a global model over all micro-bloggers, which is then leveraged to continuously refine the individual models through a collaborative online learning way. We evaluate our algorithm on a real-life micro-blog dataset collected from the popular micro-blog site &#8211; Twitter. Results show that our algorithm is effective and efficient for timely sentiment detection in real micro-blogging applications.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[data mining, classification, mining methods and algorithms]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532647</person_id>
				<author_profile_id><![CDATA[81548029260]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Guangxia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532648</person_id>
				<author_profile_id><![CDATA[81501646172]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Steven]]></first_name>
				<middle_name><![CDATA[C. H.]]></middle_name>
				<last_name><![CDATA[Hoi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532649</person_id>
				<author_profile_id><![CDATA[81335488797]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kuiyu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532650</person_id>
				<author_profile_id><![CDATA[81479656469]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jain]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934595</article_id>
		<sort_key>1150</sort_key>
		<display_label>Pages</display_label>
		<pages>899-904</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>112</seq_no>
		<title><![CDATA[Enforcing Vocabulary k-Anonymity by Semantic Similarity Based Clustering]]></title>
		<page_from>899</page_from>
		<page_to>904</page_to>
		<doi_number>10.1109/ICDM.2010.59</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934595</url>
		<abstract>
			<par><![CDATA[Web query logs provide a rich wealth of information, but also present serious privacy risks. We consider publishing vocabularies, bags of query-terms extracted from web query logs, which has a variety of applications. We aim at preventing identity disclosure of such bag-valued data. The key feature of such data is the extreme sparsity, which renders conventional anonymization techniques not working well in retaining enough utility. We propose a semantic similarity based clustering approach to address the issue. We measure the semantic similarity between two vocabularies by a weighted bipartite matching and present a greedy algorithm to cluster vocabularies by the semantic similarities. Extensive experiments on the AOL query log show that our approach retains more data utility than existing approaches.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Anonymity, privacy, bag-valued data, web query logs]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532130</person_id>
				<author_profile_id><![CDATA[81479656026]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Junqiang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532131</person_id>
				<author_profile_id><![CDATA[81479644366]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934596</article_id>
		<sort_key>1160</sort_key>
		<display_label>Pages</display_label>
		<pages>905-910</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>113</seq_no>
		<title><![CDATA[Efficient Probabilistic Latent Semantic Analysis with Sparsity Control]]></title>
		<page_from>905</page_from>
		<page_to>910</page_to>
		<doi_number>10.1109/ICDM.2010.136</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934596</url>
		<abstract>
			<par><![CDATA[Probabilistic latent semantic analysis is a topic modeling technique to discover the hidden structure in binary and count data. As a mixture model, it performs a probabilistic mixture decomposition on the co-occurrence matrix, which produces two matrices assigned with probabilistic explanations. However, the factorized matrices may be rather smooth, which means we may obtain global feature and topic representations rather than expected local ones. To resolve this problem, one of the solutions is to revise the decomposition process with considerations of sparsity. In this paper, we present an approach that provides direct control over sparsity during the expectation maximization process. Furthermore, by using the log penalty function as sparsity measurement instead of the widely used L2 norm, we can approximate the re-estimation of parameters in linear time, as same as original PLSA does, while many other approaches require much more time. Experiments on face databases are reported to show visual representations on obtaining local features, and detailed improvements in clustering tasks compared with the original process.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[unsupervised learning, opic model, plsa, sparsity, data-adaptive representations]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2529996</person_id>
				<author_profile_id><![CDATA[81479641999]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529997</person_id>
				<author_profile_id><![CDATA[81479642046]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chaolun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529998</person_id>
				<author_profile_id><![CDATA[81479655559]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Xiaohong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jiang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934597</article_id>
		<sort_key>1170</sort_key>
		<display_label>Pages</display_label>
		<pages>911-916</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>114</seq_no>
		<title><![CDATA[Understanding of Internal Clustering Validation Measures]]></title>
		<page_from>911</page_from>
		<page_to>916</page_to>
		<doi_number>10.1109/ICDM.2010.35</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934597</url>
		<abstract>
			<par><![CDATA[Clustering validation has long been recognized as one of the vital issues essential to the success of clustering applications. In general, clustering validation can be categorized into two classes, external clustering validation and internal clustering validation. In this paper, we focus on internal clustering validation and present a detailed study of 11 widely used internal clustering validation measures for crisp clustering. From five conventional aspects of clustering, we investigate their validation properties. Experiment results show that S\_Dbw is the only internal validation measure which performs well in all five aspects, while other measures have certain limitations in different application scenarios.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2529999</person_id>
				<author_profile_id><![CDATA[81479649388]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yanchi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530000</person_id>
				<author_profile_id><![CDATA[81479642616]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Zhongmou]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530001</person_id>
				<author_profile_id><![CDATA[81451596433]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xiong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530002</person_id>
				<author_profile_id><![CDATA[81387598500]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Xuedong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530003</person_id>
				<author_profile_id><![CDATA[81375612119]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Junjie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934598</article_id>
		<sort_key>1180</sort_key>
		<display_label>Pages</display_label>
		<pages>917-922</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>115</seq_no>
		<title><![CDATA[Transfer Learning via Cluster Correspondence Inference]]></title>
		<page_from>917</page_from>
		<page_to>922</page_to>
		<doi_number>10.1109/ICDM.2010.146</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934598</url>
		<abstract>
			<par><![CDATA[Transfer learning targets to leverage knowledge from one domain for tasks in a new domain. It finds abundant applications, such as text/sentiment classification. Many previous works are based on cluster analysis, which assume some common clusters shared by both domains. They mainly focus on the one-to-one cluster correspondence to bridge different domains. However, such a correspondence scheme might be too strong for real applications where each cluster in one domain corresponds to many clusters in the other domain. In this paper, we propose a Cluster Correspondence Inference (CCI) method to iteratively infer many-to-many correspondence among clusters from different domains. Specifically, word clusters and document clusters are exploited for each domain using nonnegative matrix factorization, then the word clusters from different domains are corresponded in a many-to-many scheme, with the help of shared word space as a bridge. These two steps are run iteratively and label information is transferred from source domain to target domain through the inferred cluster correspondence. Experiments on various real data sets demonstrate that our method outperforms several state-of-the-art approaches for cross-domain text classification.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Transfer Learning, Text Classification, Cluster Correspondence Inference]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533713</person_id>
				<author_profile_id><![CDATA[81479660110]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mingsheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Long]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533714</person_id>
				<author_profile_id><![CDATA[81479656986]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cheng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533715</person_id>
				<author_profile_id><![CDATA[81479646138]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Xiaoming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533716</person_id>
				<author_profile_id><![CDATA[81479650260]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jianmin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533717</person_id>
				<author_profile_id><![CDATA[81479657019]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Dou]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934599</article_id>
		<sort_key>1190</sort_key>
		<display_label>Pages</display_label>
		<pages>923-928</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>116</seq_no>
		<title><![CDATA[Supervised Link Prediction Using Multiple Sources]]></title>
		<page_from>923</page_from>
		<page_to>928</page_to>
		<doi_number>10.1109/ICDM.2010.112</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934599</url>
		<abstract>
			<par><![CDATA[Link prediction is a fundamental problem in social network analysis and modern-day commercial applications such as Face book and My space. Most existing research approaches this problem by exploring the topological structure of a social network using only one source of information. However, in many application domains, in addition to the social network of interest, there are a number of auxiliary social networks and/or derived proximity networks available. The contribution of the paper is twofold: (1) a supervised learning framework that can effectively and efficiently learn the dynamics of social networks in the presence of auxiliary networks, (2) a feature design scheme for constructing a rich variety of path-based features using multiple sources, and an effective feature selection strategy based on structured sparsity. Extensive experiments on three real-world collaboration networks show that our model can effectively learn to predict new links using multiple sources, yielding higher prediction accuracy than unsupervised and single-source supervised models.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[social network, link prediction, multiple sources, supervised learning]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533718</person_id>
				<author_profile_id><![CDATA[81435594802]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Zhengdong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533719</person_id>
				<author_profile_id><![CDATA[81323495827]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Berkant]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Savas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533720</person_id>
				<author_profile_id><![CDATA[81453633414]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Wei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533721</person_id>
				<author_profile_id><![CDATA[81100098715]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Inderjit]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Dhillon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934606</article_id>
		<sort_key>1200</sort_key>
		<display_label>Pages</display_label>
		<pages>929-934</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>117</seq_no>
		<title><![CDATA[Addressing Concept-Evolution in Concept-Drifting Data Streams]]></title>
		<page_from>929</page_from>
		<page_to>934</page_to>
		<doi_number>10.1109/ICDM.2010.160</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934606</url>
		<abstract>
			<par><![CDATA[The problem of data stream classification is challenging because of many practical aspects associated with efficient processing and temporal behavior of the stream. Two such well studied aspects are infinite length and concept-drift. Since a data stream may be considered a continuous process, which is theoretically infinite in length, it is impractical to store and use all the historical data for training. Data streams also frequently experience concept-drift as a result of changes in the underlying concepts. However, another important characteristic of data streams, namely, concept-evolution is rarely addressed in the literature. Concept-evolution occurs as a result of new classes evolving in the stream. This paper addresses concept-evolution in addition to the existing challenges of infinite-length and concept-drift. In this paper, the concept-evolution phenomenon is studied, and the insights are used to construct superior novel class detection techniques. First, we propose an adaptive threshold for outlier detection, which is a vital part of novel class detection. Second, we propose a probabilistic approach for novel class detection using discrete Gini Coefficient, and prove its effectiveness both theoretically and empirically. Finally, we address the issue of simultaneous multiple novel class occurrence, and provide an elegant solution to detect more than one novel classes at the same time. We also consider feature-evolution in text data streams, which occurs because new features (i.e., words) evolve in the stream. Comparison with state-of-the-art data stream classification techniques establishes the effectiveness of the proposed approach.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[data stream, concept-evolution, novel class, outlier]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533204</person_id>
				<author_profile_id><![CDATA[81350592477]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mohammad]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Masud]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533205</person_id>
				<author_profile_id><![CDATA[81474689636]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Qing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533206</person_id>
				<author_profile_id><![CDATA[81100344538]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Latifur]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Khan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533207</person_id>
				<author_profile_id><![CDATA[81350594201]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Charu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aggarwal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533208</person_id>
				<author_profile_id><![CDATA[81314494134]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533209</person_id>
				<author_profile_id><![CDATA[81479644692]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Jiawei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Han]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533210</person_id>
				<author_profile_id><![CDATA[81100621824]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Bhavani]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thuraisingham]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934600</article_id>
		<sort_key>1210</sort_key>
		<display_label>Pages</display_label>
		<pages>935-940</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>118</seq_no>
		<title><![CDATA[Sparse Boolean Matrix Factorizations]]></title>
		<page_from>935</page_from>
		<page_to>940</page_to>
		<doi_number>10.1109/ICDM.2010.93</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934600</url>
		<abstract>
			<par><![CDATA[Matrix factorizations are commonly used methods in data mining. When the input data is Boolean, replacing the standard matrix multiplication with Boolean matrix multiplication can yield more intuitive results. Unfortunately, finding a good Boolean decomposition is known to be computationally hard, with even many sub-problems being hard to approximate. Many real-world data sets are sparse, and it is often required that also the factor matrices are sparse. This requirement has motivated many new matrix decomposition methods and many modifications of the existing methods. This paper studies how Boolean matrix factorizations behave with sparse data: can we assume some sparsity on the factor matrices, and does the sparsity help with the computationally hard problems. The answer to these problems is shown to be positive.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Matrix decompositions, Boolean rank, approximation algorithms]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532659</person_id>
				<author_profile_id><![CDATA[81367594364]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pauli]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miettinen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934607</article_id>
		<sort_key>1220</sort_key>
		<display_label>Pages</display_label>
		<pages>941-946</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>119</seq_no>
		<title><![CDATA[On the Computation of Stochastic Search Variable Selection in Linear Regression with UDFs]]></title>
		<page_from>941</page_from>
		<page_to>946</page_to>
		<doi_number>10.1109/ICDM.2010.79</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934607</url>
		<abstract>
			<par><![CDATA[Computing Bayesian statistics with traditional techniques is extremely slow, specially when large data has to be exported from a relational DBMS. We propose algorithms for large scale processing of stochastic search variable selection (SSVS) for linear regression that can work entirely inside a DBMS. The traditional SSVS algorithm requires multiple scans of the input data in order to compute a regression model. Due to our optimizations, SSVS can be done in either one scan over the input table for large number of records with sufficient statistics, or one scan per iteration for high-dimensional data. We consider storage layouts which efficiently exploit DBMS parallel processing of aggregate functions. Experimental results demonstrate correctness, convergence and performance of our algorithms. Finally, the algorithms show good scalability for data with a very large number of records, or a very high number of dimensions.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Bayesian statistics, variable selection, UDF]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531607</person_id>
				<author_profile_id><![CDATA[81440595732]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mario]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Navas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531608</person_id>
				<author_profile_id><![CDATA[81100618426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Carlos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ordonez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531609</person_id>
				<author_profile_id><![CDATA[81479664832]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Veerabhadran]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baladandayuthapani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934608</article_id>
		<sort_key>1230</sort_key>
		<display_label>Pages</display_label>
		<pages>947-952</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>120</seq_no>
		<title><![CDATA[Data Editing Techniques to Allow the Application of Distance-Based Outlier Detection to Streams]]></title>
		<page_from>947</page_from>
		<page_to>952</page_to>
		<doi_number>10.1109/ICDM.2010.56</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934608</url>
		<abstract>
			<par><![CDATA[The problem of finding outliers in data has broad applications in areas as diverse as data cleaning, fraud detection, network monitoring, invasive species monitoring, etc. While there are dozens of techniques that have been proposed to solve this problem for static data collections, very simple distance-based outlier detection methods are known to be competitive or superior to more complex methods. However, distance-based methods have time and space complexities that make them impractical for streaming data and/or resource limited sensors. In this work, we show that simple data-editing techniques can make distance-based outlier detection practical for very fast streams and resource limited sensors. Our technique generalizes to produce two algorithms, which, relative to the original algorithm, can guarantee to produce no false positives, or guarantee to produce no false negatives. Our methods are independent of both data type and distance measure, and are thus broadly applicable.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Data editing, Anomaly detection, Data stream]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531111</person_id>
				<author_profile_id><![CDATA[81321496288]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Vit]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Niennattrakul]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531112</person_id>
				<author_profile_id><![CDATA[81493650377]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eamonn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Keogh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531113</person_id>
				<author_profile_id><![CDATA[81100182188]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chotirat]]></first_name>
				<middle_name><![CDATA[Ann]]></middle_name>
				<last_name><![CDATA[Ratanamahatana]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934609</article_id>
		<sort_key>1240</sort_key>
		<display_label>Pages</display_label>
		<pages>953-958</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>121</seq_no>
		<title><![CDATA[Anomaly Detection Using an Ensemble of Feature Models]]></title>
		<page_from>953</page_from>
		<page_to>958</page_to>
		<doi_number>10.1109/ICDM.2010.140</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934609</url>
		<abstract>
			<par><![CDATA[We present a new approach to semi-supervised anomaly detection. Given a set of training examples believed to come from the same distribution or class, the task is to learn a model that will be able to distinguish examples in the future that do not belong to the same class. Traditional approaches typically compare the position of a new data point to the set of ``normal'' training data points in a chosen representation of the feature space. For some data sets, the normal data may not have discernible positions in feature space, but do have consistent relationships among some features that fail to appear in the anomalous examples. Our approach learns to predict the values of training set features from the values of other features. After we have formed an ensemble of predictors, we apply this ensemble to new data points. To combine the contribution of each predictor in our ensemble, we have developed a novel, information-theoretic anomaly measure that our experimental results show selects against noisy and irrelevant features. Our results on 47 data sets show that for most data sets, this approach significantly improves performance over current state-of-the-art feature space distance and density-based approaches.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[anomaly detection, machine learning, feature selection]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533211</person_id>
				<author_profile_id><![CDATA[81361605759]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Keith]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Noto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533212</person_id>
				<author_profile_id><![CDATA[86058699557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Carly]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Brodley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533213</person_id>
				<author_profile_id><![CDATA[81100045698]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Donna]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Slonim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934610</article_id>
		<sort_key>1250</sort_key>
		<display_label>Pages</display_label>
		<pages>959-964</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>122</seq_no>
		<title><![CDATA[Assessing Data Mining Results on Matrices with Randomization]]></title>
		<page_from>959</page_from>
		<page_to>964</page_to>
		<doi_number>10.1109/ICDM.2010.20</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934610</url>
		<abstract>
			<par><![CDATA[Randomization is a general technique for evaluating the significance of data analysis results. In randomization-based significance testing, a result is considered to be interesting if it is unlikely to obtain as good result on random data sharing some basic properties with the original data. Recently, the randomization approach has been applied to assess data mining results on binary matrices and limited types of real-valued matrices. In these works, the row and column value distributions are approximately preserved in randomization. However, the previous approaches suffer from various technical and practical shortcomings. In this paper, we give solutions to these problems and introduce a new practical algorithm for randomizing various types of matrices while preserving the row and column value distributions more accurately. We propose a new approach for randomizing matrices containing features measured in different scales. Compared to previous work, our approach can be applied to assess data mining results on different types of real-life matrices containing dissimilar features, nominal values, non-Gaussian value distributions, missing values and sparse structure. We provide an easily usable implementation that does not need problematic manual tuning as theoretically justified parameter values are given. We perform extensive experiments on various real-life datasets showing that our approach produces reasonable results on practically all types of matrices while being easy and fast to use.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533214</person_id>
				<author_profile_id><![CDATA[81436594498]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Markus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ojala]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934615</article_id>
		<sort_key>1260</sort_key>
		<display_label>Pages</display_label>
		<pages>965-970</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>123</seq_no>
		<title><![CDATA[A Generalized Linear Threshold Model for Multiple Cascades]]></title>
		<page_from>965</page_from>
		<page_to>970</page_to>
		<doi_number>10.1109/ICDM.2010.153</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934615</url>
		<abstract>
			<par><![CDATA[This paper presents a generalized version of the linear threshold model for simulating multiple cascades on a network while allowing nodes to switch between them. The proposed model is shown to be a rapidly mixing Markov chain and the corresponding steady state distribution is used to estimate highly likely states of the cascades' spread in the network. Results on a variety of real world networks demonstrate the high quality of the estimated solution.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[network diffusion, cascading processes, social networks, rapidly mixing markov chains, graph theory]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531632</person_id>
				<author_profile_id><![CDATA[81100402948]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nishith]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pathak]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531633</person_id>
				<author_profile_id><![CDATA[81100144629]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Arindam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Banerjee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531634</person_id>
				<author_profile_id><![CDATA[81474657118]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jaideep]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Srivastava]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934616</article_id>
		<sort_key>1270</sort_key>
		<display_label>Pages</display_label>
		<pages>971-976</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>124</seq_no>
		<title><![CDATA[Recommending Social Events from Mobile Phone Location Data]]></title>
		<page_from>971</page_from>
		<page_to>976</page_to>
		<doi_number>10.1109/ICDM.2010.152</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934616</url>
		<abstract>
			<par><![CDATA[A city offers thousands of social events a day, and it is difficult for dwellers to make choices. The combination of mobile phones and recommender systems can change the way one deals with such abundance. Mobile phones with positioning technology are now widely available, making it easy for people to broadcast their whereabouts, recommender systems can now identify patterns in people&#8217;s movements in order to, for example, recommend events. To do so, the system relies on having mobile users who share their attendance at a large number of social events: cold-start users, who have no location history, cannot receive recommendations. We set out to address the mobile cold-start problem by answering the following research question: how can social events be recommended to a cold-start user based only on his home location? To answer this question, we carry out a study of the relationship between preferences for social events and geography, the first of its kind in a large metropolitan area. We sample location estimations of one million mobile phone users in Greater Boston, combine the sample with social events in the same area, and infer the social events attended by 2,519 residents. Upon this data, we test a variety of algorithms for recommending social events. We find that the most effective algorithm recommends events that are popular among residents of an area. The least effective, instead, recommends events that are geographically close to the area. This last result has interesting implications for location-based services that emphasize recommending nearby events.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[mobile, recommender systems, web 2.0]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533248</person_id>
				<author_profile_id><![CDATA[81100232264]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniele]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Quercia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533249</person_id>
				<author_profile_id><![CDATA[81350596151]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Neal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lathia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533250</person_id>
				<author_profile_id><![CDATA[81310487997]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Francesco]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Calabrese]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533251</person_id>
				<author_profile_id><![CDATA[81436602950]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Giusy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Di Lorenzo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533252</person_id>
				<author_profile_id><![CDATA[81100034222]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Crowcroft]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934617</article_id>
		<sort_key>1280</sort_key>
		<display_label>Pages</display_label>
		<pages>977-982</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>125</seq_no>
		<title><![CDATA[On Normalizing Fuzzy Coincidence Matrices to Compare Fuzzy and/or Possibilistic Partitions with the Rand Index]]></title>
		<page_from>977</page_from>
		<page_to>982</page_to>
		<doi_number>10.1109/ICDM.2010.130</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934617</url>
		<abstract>
			<par><![CDATA[Most already existing indices used to compare two strict partitions with different number of clusters are based on coincidence matrices. To extend such indices to fuzzy partitions, one can define fuzzy coincidence matrices by means of triangular norms. It has been shown this can require some kind of normalization to reinforce the corresponding indices. We propose in this paper a generic solution to perform this normalization considering the generators of the used triangular norms. Although the solution is not index-dependant, we focus on the Rand index and some of its fuzzy counterparts.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[coincidence matrix, fuzzy partition, triangular norms, Rand index]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533784</person_id>
				<author_profile_id><![CDATA[81479652240]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[R.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Quere]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533785</person_id>
				<author_profile_id><![CDATA[81456638998]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[H.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Le Capitaine]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533786</person_id>
				<author_profile_id><![CDATA[81479664788]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[N.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fraisseix]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533787</person_id>
				<author_profile_id><![CDATA[81479656051]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[C.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Frelicot]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934618</article_id>
		<sort_key>1290</sort_key>
		<display_label>Pages</display_label>
		<pages>983-988</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>126</seq_no>
		<title><![CDATA[Financial Forecasting with Gompertz Multiple Kernel Learning]]></title>
		<page_from>983</page_from>
		<page_to>988</page_to>
		<doi_number>10.1109/ICDM.2010.68</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934618</url>
		<abstract>
			<par><![CDATA[Financial forecasting is the basis for budgeting activities and estimating future financing needs. Applying machine learning and data mining models to financial forecasting is both effective and efficient. Among different kinds of machine learning models, kernel methods are well accepted since they are more robust and accurate than traditional models, such as neural networks. However, learning from multiple data sources is still one of the main challenges in the financial forecasting area. In this paper, we focus on applying the multiple kernel learning models to the multiple major international stock indexes. Our experiment results indicate that applying multiple kernel learning to the financial forecasting problem suffers from both the short training period problem and non-stationary problem. Therefore we propose a novel multiple kernel learning model to address the challenge by introducing the Gompertz model and considering a non-linear combination of different kernel matrices. The experiment results show that our Gompertz multiple kernel learning model addresses the challenges and achieves better performance than the original multiple kernel learning model and single SVM models.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[financial forecasting, multiple kernel learning, non-linear kernel combination]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532198</person_id>
				<author_profile_id><![CDATA[81479649452]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Han]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Qin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532199</person_id>
				<author_profile_id><![CDATA[81100541628]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dejing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532200</person_id>
				<author_profile_id><![CDATA[81479651652]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yue]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934619</article_id>
		<sort_key>1300</sort_key>
		<display_label>Pages</display_label>
		<pages>989-994</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>127</seq_no>
		<title><![CDATA[Leveraging D-Separation for Relational Data Sets]]></title>
		<page_from>989</page_from>
		<page_to>994</page_to>
		<doi_number>10.1109/ICDM.2010.142</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934619</url>
		<abstract>
			<par><![CDATA[Testing for marginal and conditional independence is a common task in machine learning and knowledge discovery applications. Prior work has demonstrated that conventional independence tests suffer from dramatically increased rates of Type I errors when naively applied to relational data. We use graphical models to specify the conditions under which these errors occur, and use those models to devise novel and accurate conditional independence tests.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530071</person_id>
				<author_profile_id><![CDATA[81100417672]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[J. H.]]></middle_name>
				<last_name><![CDATA[Rattigan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530072</person_id>
				<author_profile_id><![CDATA[81100640362]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934620</article_id>
		<sort_key>1310</sort_key>
		<display_label>Pages</display_label>
		<pages>995-1000</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>128</seq_no>
		<title><![CDATA[Factorization Machines]]></title>
		<page_from>995</page_from>
		<page_to>1000</page_to>
		<doi_number>10.1109/ICDM.2010.127</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934620</url>
		<abstract>
			<par><![CDATA[In this paper, we introduce Factorization Machines (FM) which are a new model class that combines the advantages of Support Vector Machines (SVM) with factorization models. Like SVMs, FMs are a general predictor working with any real valued feature vector. In contrast to SVMs, FMs model all interactions between variables using factorized parameters. Thus they are able to estimate interactions even in problems with huge sparsity (like recommender systems) where SVMs fail. We show that the model equation of FMs can be calculated in linear time and thus FMs can be optimized directly. So unlike nonlinear SVMs, a transformation in the dual form is not necessary and the model parameters can be estimated directly without the need of any support vector in the solution. We show the relationship to SVMs and the advantages of FMs for parameter estimation in sparse settings. On the other hand there are many different factorization models like matrix factorization, parallel factor analysis or specialized models like SVD++, PITF or FPMC. The drawback of these models is that they are not applicable for general prediction tasks but work only with special input data. Furthermore their model equations and optimization algorithms are derived individually for each task. We show that FMs can mimic these models just by specifying the input data (i.e. the feature vectors). This makes FMs easily applicable even for users without expert knowledge in factorization models.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[factorization machine, sparse data, tensor factorization, support vector machine]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532716</person_id>
				<author_profile_id><![CDATA[81321497327]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Steffen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rendle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934626</article_id>
		<sort_key>1320</sort_key>
		<display_label>Pages</display_label>
		<pages>1001-1006</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>129</seq_no>
		<title><![CDATA[Accelerating Dynamic Time Warping Subsequence Search with GPUs and FPGAs]]></title>
		<page_from>1001</page_from>
		<page_to>1006</page_to>
		<doi_number>10.1109/ICDM.2010.21</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934626</url>
		<abstract>
			<par><![CDATA[Many time series data mining problems require subsequence similarity search as a subroutine. Dozens of similarity/distance measures have been proposed in the last decade and there is increasing evidence that Dynamic Time Warping (DTW) is the best measure across a wide range of domains. Given DTW&#8217;s usefulness and ubiquity, there has been a large community-wide effort to mitigate its relative lethargy. Proposed speedup techniques include early abandoning strategies, lower-bound based pruning, indexing and embedding. In this work we argue that we are now close to exhausting all possible speedup from software, and that we must turn to hardware-based solutions. With this motivation, we investigate both GPU (Graphics Processing Unit) and FPGA (Field Programmable Gate Array) based acceleration of subsequence similarity search under the DTW measure. As we shall show, our novel algorithms allow GPUs to achieve two orders of magnitude speedup and FPGAs to produce four orders of magnitude speedup. We conduct detailed case studies on the classification of astronomical observations and demonstrate that our ideas allow us to tackle problems that would be untenable otherwise.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[time series, similarity search, dynamic time warping, FPGA, GPU]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533298</person_id>
				<author_profile_id><![CDATA[81479664169]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Doruk]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sart]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533299</person_id>
				<author_profile_id><![CDATA[81453650813]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Abdullah]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mueen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533300</person_id>
				<author_profile_id><![CDATA[81479654335]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Walid]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Najjar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533301</person_id>
				<author_profile_id><![CDATA[81100209161]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Eamonn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Keogh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533302</person_id>
				<author_profile_id><![CDATA[81321496288]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Vit]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Niennattrakul]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934627</article_id>
		<sort_key>1330</sort_key>
		<display_label>Pages</display_label>
		<pages>1007-1012</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>130</seq_no>
		<title><![CDATA[An Approach for Automatic Sleep Stage Scoring and Apnea-Hypopnea Detection]]></title>
		<page_from>1007</page_from>
		<page_to>1012</page_to>
		<doi_number>10.1109/ICDM.2010.60</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934627</url>
		<abstract>
			<par><![CDATA[This paper presents an application of data mining to the medical domain sleep research, i.e. an approach for automatic sleep stage scoring and apnea-hypopnea detection. By several combined techniques (Fourier and wavelet transform, DDTW and waveform recognition), our approach extracts meaningful features (frequencies and special patterns) from EEG, ECG, EOG and EMG data, on which a decision trees classifier is built for classifying epochs into their sleep stages (according to the rules by Rechtschaffen and Kales) and annotating occurrences of apnea-hypopnea (total or partial cessation of respiration). After that, case-based reasoning is applied to improve quality. We evaluated our approach on 3 large public databases from PhysioBank, which showed an overall accuracy of 95.2% for sleep stage scoring and 94.5% for classifying apneic/non-apneic minutes.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Time series, Data processing, Feature extraction, Pattern classification, Biomedical signal processing, Sleep]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532222</person_id>
				<author_profile_id><![CDATA[81381605290]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schluter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532223</person_id>
				<author_profile_id><![CDATA[81100033996]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Stefan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Conrad]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934628</article_id>
		<sort_key>1340</sort_key>
		<display_label>Pages</display_label>
		<pages>1013-1018</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>131</seq_no>
		<title><![CDATA[Bonsai]]></title>
		<subtitle><![CDATA[Growing Interesting Small Trees]]></subtitle>
		<page_from>1013</page_from>
		<page_to>1018</page_to>
		<doi_number>10.1109/ICDM.2010.86</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934628</url>
		<abstract>
			<par><![CDATA[Graphs are increasingly used to model a variety of loosely structured data such as biological or social networks and entity-relationships. Given this profusion of large-scale graph data, efficiently discovering interesting substructures buried within is essential. These substructures are typically used in determining subsequent actions, such as conducting visual analytics by humans or designing expensive biomedical experiments. In such settings, it is often desirable to constrain the size of the discovered results in order to directly control the associated costs. In this paper, we address the problem of finding cardinality-constrained connected sub trees in large node-weighted graphs that maximize the sum of weights of selected nodes. We provide an efficient constant-factor approximation algorithm for this strongly NP-hard problem. Our techniques can be applied in a wide variety of application settings, for example in differential analysis of graphs, a problem that frequently arises in bioinformatics but also has applications on the web.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Graph Mining, Subgraph Discovery, Visual Analytics, Cardinalty Constrained Subgraphs]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533303</person_id>
				<author_profile_id><![CDATA[81460649177]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stephan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seufert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533304</person_id>
				<author_profile_id><![CDATA[81320487534]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Srikanta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bedathur]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533305</person_id>
				<author_profile_id><![CDATA[81318490291]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Julian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mestre]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533306</person_id>
				<author_profile_id><![CDATA[81100576787]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Gerhard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Weikum]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934629</article_id>
		<sort_key>1350</sort_key>
		<display_label>Pages</display_label>
		<pages>1019-1024</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>132</seq_no>
		<title><![CDATA[Mixed-Membership Stochastic Block-Models for Transactional Networks]]></title>
		<page_from>1019</page_from>
		<page_to>1024</page_to>
		<doi_number>10.1109/ICDM.2010.88</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934629</url>
		<abstract>
			<par><![CDATA[Transactional network data can be thought of as a list of one-to-many communications (e.g., email) between nodes in a social network. Most social network models convert this type of data into binary relations between pairs of nodes. We develop a latent mixed membership model capable of modeling richer forms of transactional network data, including relations between more than two nodes. The model can cluster nodes and predict transactions. The block-model nature of the model implies that groups can be characterized in very general ways. This flexible notion of group structure enables discovery of rich structure in transactional networks. Estimation and inference are accomplished via a variational EM algorithm. Simulations indicate that the learning algorithm can recover the correct generative model. Interesting structure is discovered in the Enron email dataset and another dataset extracted from the Reddit website. Analysis of the Reddit data is facilitated by a novel performance measure for comparing two soft clusterings. The new model is superior at discovering mixed membership in groups and in predicting transactions.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Social Network Analysis, Clustering, Mixedmembership, Variational EM, Email Data]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530108</person_id>
				<author_profile_id><![CDATA[81435594226]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mahdi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shafiei]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530109</person_id>
				<author_profile_id><![CDATA[81436595145]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hugh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chipman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934630</article_id>
		<sort_key>1360</sort_key>
		<display_label>Pages</display_label>
		<pages>1025-1030</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>133</seq_no>
		<title><![CDATA[Generalized Probabilistic Matrix Factorizations for Collaborative Filtering]]></title>
		<page_from>1025</page_from>
		<page_to>1030</page_to>
		<doi_number>10.1109/ICDM.2010.116</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934630</url>
		<abstract>
			<par><![CDATA[Probabilistic matrix factorization (PMF) methods have shown great promise in collaborative filtering. In this paper, we consider several variants and generalizations of PMF framework inspired by three broad questions: Are the prior distributions used in existing PMF models suitable, or can one get better predictive performance with different priors? Are there suitable extensions to leverage side information? Are there benefits to taking into account row and column biases? We develop new families of PMF models to address these questions along with efficient approximate inference algorithms for learning and prediction. Through extensive experiments on movie recommendation datasets, we illustrate that simpler models directly capturing correlations among latent factors can outperform existing PMF models, side information can benefit prediction accuracy, and accounting for row/column biases leads to improvements in predictive performance.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[probabilistic matrix factorization, topic models, variational inference]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530110</person_id>
				<author_profile_id><![CDATA[81375593685]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hanhuai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530111</person_id>
				<author_profile_id><![CDATA[81100144629]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Arindam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Banerjee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934631</article_id>
		<sort_key>1370</sort_key>
		<display_label>Pages</display_label>
		<pages>1031-1036</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>134</seq_no>
		<title><![CDATA[Topic Modeling Ensembles]]></title>
		<page_from>1031</page_from>
		<page_to>1036</page_to>
		<doi_number>10.1109/ICDM.2010.113</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934631</url>
		<abstract>
			<par><![CDATA[In this paper we propose a framework of topic modeling ensembles, a novel solution to combine the models learned by topic modeling over each partition of the whole corpus. It has the potentials for applications such as distributed topic modeling for large corpora, and incremental topic modeling for rapidly growing corpora. Since only the base models, not the original documents, are required in the ensemble, all these applications can be performed in a privacy preserving manner. We explore the theoretical foundation of the proposed framework, give its geometric interpretation, and implement it for both PLSA and LDA. The evaluation of the implementations over the synthetic and real-life data sets shows that the proposed framework is much more efficient than modeling the original corpus directly while achieves comparable effectiveness in terms of perplexity and classification accuracy.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Topic model, Ensemble]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533307</person_id>
				<author_profile_id><![CDATA[81392598013]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Zhiyong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533308</person_id>
				<author_profile_id><![CDATA[81100416055]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ping]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Luo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533309</person_id>
				<author_profile_id><![CDATA[81479653830]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shengwen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533310</person_id>
				<author_profile_id><![CDATA[81479659003]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Xukun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934638</article_id>
		<sort_key>1380</sort_key>
		<display_label>Pages</display_label>
		<pages>1037-1042</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>135</seq_no>
		<title><![CDATA[Interval-valued Matrix Factorization with Applications]]></title>
		<page_from>1037</page_from>
		<page_to>1042</page_to>
		<doi_number>10.1109/ICDM.2010.115</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934638</url>
		<abstract>
			<par><![CDATA[In this paper, we propose the Interval-valued Matrix Factorization (IMF) framework. Matrix Factorization (MF) is a fundamental building block of data mining. MF techniques, such as Nonnegative Matrix Factorization (NMF) and Probabilistic Matrix Factorization (PMF), are widely used in applications of data mining. For example, NMF has shown its advantage in Face Analysis (FA) while PMF has been successfully applied to Collaborative Filtering (CF). In this paper, we analyze the data approximation in FA as well as CF applications and construct interval-valued matrices to capture these approximation phenomenons. We adapt basic NMF and PMF models to the interval-valued matrices and propose Interval-valued NMF (I-NMF) as well as Interval-valued PMF (I-PMF). We conduct extensive experiments to show that proposed I-NMF and I-PMF significantly outperform their single-valued counterparts in FA and CF applications.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Matrix factorization, uncertainty]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532246</person_id>
				<author_profile_id><![CDATA[81479652668]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Zhiyong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532247</person_id>
				<author_profile_id><![CDATA[81501672769]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Liang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Du]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532248</person_id>
				<author_profile_id><![CDATA[81479659002]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Xukun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532249</person_id>
				<author_profile_id><![CDATA[81450594087]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yidong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934639</article_id>
		<sort_key>1390</sort_key>
		<display_label>Pages</display_label>
		<pages>1043-1048</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>136</seq_no>
		<title><![CDATA[Efficient Semi-supervised Spectral Co-clustering with Constraints]]></title>
		<page_from>1043</page_from>
		<page_to>1048</page_to>
		<doi_number>10.1109/ICDM.2010.64</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934639</url>
		<abstract>
			<par><![CDATA[Co-clustering was proposed to simultaneously cluster objects and features to explore inter-correlated patterns. For example, by analyzing the blog click-through data, one finds the group of users who are interested in a specific group of blogs in order to perform applications such as recommendations. However, it is usually very difficult to achieve good co-clustering quality by just analyzing the object-feature correlation data due to the sparsity of the data and the noise. Meanwhile, one may have some prior knowledge that indicates the internal structure of the co-clusters. For instance, one may find user cluster information from the social network system, and the blog-blog similarity from the social tags or contents. This prior information provides some supervision toward the co-cluster structures, and may help reduce the effect of sparsity and noise. However, most co-clustering algorithms do not use this information and may produce unmeaningful results. In this paper we study the problem of finding the optimal co-clusters when some objects and features are believed to be in the same cluster a priori. A matrix decomposition based approach is proposed to formulate as a trace minimization problem, and solve it efficiently with the selected eigenvectors. The asymptotic complexity of the proposed approach is the same as co-clustering without constraints. Experiments include graph-pattern co-clustering and document-word co-clustering. For instance, in graph-pattern data set, the proposed model can improve the normalized mutual information by as much as 5.5 times and 10 times faster than two naive solutions that expand the edges and vertices in the graphs.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Spectral, Semi-supervised Learning, Co-clustering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532767</person_id>
				<author_profile_id><![CDATA[81479648819]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xiaoxiao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532768</person_id>
				<author_profile_id><![CDATA[81367591181]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532769</person_id>
				<author_profile_id><![CDATA[81350576309]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Philip]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934640</article_id>
		<sort_key>1400</sort_key>
		<display_label>Pages</display_label>
		<pages>1049-1054</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>137</seq_no>
		<title><![CDATA[Transfer Learning on Heterogenous Feature Spaces via Spectral Transformation]]></title>
		<page_from>1049</page_from>
		<page_to>1054</page_to>
		<doi_number>10.1109/ICDM.2010.65</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934640</url>
		<abstract>
			<par><![CDATA[Labeled examples are often expensive and time-consuming to obtain. One practically important problem is: can the labeled data from other related sources help predict the target task, even if they have (a) different feature spaces (e.g., image vs. text data), (b) different data distributions, and (c) different output spaces? This paper proposes a solution and discusses the conditions where this is possible and highly likely to produce better results. It works by first using spectral embedding to unify the different feature spaces of the target and source data sets, even when they have completely different feature spaces. The principle is to cast into an optimization objective that preserves the original structure of the data, while at the same time, maximizes the similarity between the two. Second, a judicious sample selection strategy is applied to select only those related source examples. At last, a Bayesian-based approach is applied to model the relationship between different output spaces. The three steps can bridge related heterogeneous sources in order to learn the target task. Among the 12 experiment data sets, for example, the images with wavelet-transformed-based features are used to predict another set of images whose features are constructed from color-histogram space. By using these extracted examples from heterogeneous sources, the models can reduce the error rate by as much as~50\%, compared with the methods using only the examples from the target task.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[transfer learning, heterogeneous, spectral, feature space]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530144</person_id>
				<author_profile_id><![CDATA[81479648819]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xiaoxiao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530145</person_id>
				<author_profile_id><![CDATA[81479655171]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Qi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530146</person_id>
				<author_profile_id><![CDATA[81367591181]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Wei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530147</person_id>
				<author_profile_id><![CDATA[81350576309]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Philip]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530148</person_id>
				<author_profile_id><![CDATA[81479658217]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ruixin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934641</article_id>
		<sort_key>1410</sort_key>
		<display_label>Pages</display_label>
		<pages>1055-1060</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>138</seq_no>
		<title><![CDATA[One-Class Matrix Completion with Low-Density Factorizations]]></title>
		<page_from>1055</page_from>
		<page_to>1060</page_to>
		<doi_number>10.1109/ICDM.2010.164</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934641</url>
		<abstract>
			<par><![CDATA[Consider a typical recommendation problem. A company has historical records of products sold to a large customer base. These records may be compactly represented as a sparse customer-times-product ``who-bought-what" binary matrix. Given this matrix, the goal is to build a model that provides recommendations for which products should be sold next to the existing customer base. Such problems may naturally be formulated as collaborative filtering tasks. However, this is a {\it one-class} setting, that is, the only known entries in the matrix are one-valued. If a customer has not bought a product yet, it does not imply that the customer has a low propensity to {\it potentially} be interested in that product. In the absence of entries explicitly labeled as negative examples, one may resort to considering unobserved customer-product pairs as either missing data or as surrogate negative instances. In this paper, we propose an approach to explicitly deal with this kind of ambiguity by instead treating the unobserved entries as optimization variables. These variables are optimized in conjunction with learning a weighted, low-rank non-negative matrix factorization (NMF) of the customer-product matrix, similar to how Transductive SVMs implement the low-density separation principle for semi-supervised learning. Experimental results show that our approach gives significantly better recommendations in comparison to various competing alternatives on one-class collaborative filtering tasks.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Collaborative Filtering, Recommender Systems, Non-negative Matrix Factorizations, Implicit Feedback, Matrix Completion]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531224</person_id>
				<author_profile_id><![CDATA[81309499649]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Vikas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sindhwani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531225</person_id>
				<author_profile_id><![CDATA[81416599889]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Serhat]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Bucak]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531226</person_id>
				<author_profile_id><![CDATA[81479658108]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jianying]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531227</person_id>
				<author_profile_id><![CDATA[81100344997]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Aleksandra]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mojsilovic]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934647</article_id>
		<sort_key>1420</sort_key>
		<display_label>Pages</display_label>
		<pages>1061-1066</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>139</seq_no>
		<title><![CDATA[A System for Mining Temporal Physiological Data Streams for Advanced Prognostic Decision Support]]></title>
		<page_from>1061</page_from>
		<page_to>1066</page_to>
		<doi_number>10.1109/ICDM.2010.102</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934647</url>
		<abstract>
			<par><![CDATA[We present a mining system that can predict the future health status of the patient using the temporal trajectories of health status of a set of similar patients. The main novelties of this system are its use of stream processing technology for handling the incoming physiological time series data and incorporating domain knowledge in learning the similarity metric between patients represented by their temporal data. The proposed approach and system were tested using the MIMIC II database, which consists of physiological waveforms, and accompanying clinical data obtained for ICU patients. The study was carried out on 1500 patients from this database. In the experiments we report the efficiency and throughput of the stream processing unit for feature extraction, the effectiveness of the supervised similarity measure both in the context of classification and retrieval tasks compared to unsupervised approaches, and the accuracy of the temporal projections of the patient data.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Patient similarity, Physiological streams]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531247</person_id>
				<author_profile_id><![CDATA[81455605573]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jimeng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531248</person_id>
				<author_profile_id><![CDATA[81474692696]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daby]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sow]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531249</person_id>
				<author_profile_id><![CDATA[81100255366]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jianying]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531250</person_id>
				<author_profile_id><![CDATA[81100531658]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shahram]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ebadollahi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934648</article_id>
		<sort_key>1430</sort_key>
		<display_label>Pages</display_label>
		<pages>1067-1072</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>140</seq_no>
		<title><![CDATA[Averaged Stochastic Gradient Descent with Feedback]]></title>
		<subtitle><![CDATA[An Accurate, Robust, and Fast Training Method]]></subtitle>
		<page_from>1067</page_from>
		<page_to>1072</page_to>
		<doi_number>10.1109/ICDM.2010.26</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934648</url>
		<abstract>
			<par><![CDATA[On large datasets, the popular training approach has been stochastic gradient descent (SGD). This paper proposes a modification of SGD, called averaged SGD with feedback (ASF), that significantly improves the performance (robustness, accuracy, and training speed) over the traditional SGD. The proposal is based on three simple ideas: averaging the weight vectors across SGD iterations, feeding the averaged weights back into the SGD update process, and deciding when to perform the feedback (linearly slowing down feedback). Theoretically, we demonstrate the reasonable convergence properties of the ASF. Empirically, the ASF outperforms several strong baselines in terms of accuracy, robustness over the noise, and the training speed. To our knowledge, this is the first study of ``feedback'' in stochastic gradient learning. Although we choose latent conditional models for verifying the ASF in this paper, the ASF is a general purpose technique just like SGD, and can be directly applied to other models.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530662</person_id>
				<author_profile_id><![CDATA[81442596203]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530663</person_id>
				<author_profile_id><![CDATA[81100105757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hisashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kashima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530664</person_id>
				<author_profile_id><![CDATA[81544764356]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takuya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matsuzaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530665</person_id>
				<author_profile_id><![CDATA[81100364194]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Naonori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ueda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934642</article_id>
		<sort_key>1440</sort_key>
		<display_label>Pages</display_label>
		<pages>1073-1078</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>141</seq_no>
		<title><![CDATA[Visualizing Graphs Using Minimum Spanning Dendrograms]]></title>
		<page_from>1073</page_from>
		<page_to>1078</page_to>
		<doi_number>10.1109/ICDM.2010.71</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934642</url>
		<abstract>
			<par><![CDATA[We present a novel visualization methodology for graphs and high-dimensional data which combines the neighborhood preservation characteristics of the minimum spanning trees, with the grouping properties of dendrograms. We call the method `minimum spanning dendrogram'. We highlight the ability of the mapping to accurately capture both neighborhood and cluster structures. The technique accommodates the interactive cluster formation at progressively more granular levels, allowing the user to explore data relationships at different resolutions. We also compare our work with other visualization methodologies, such as ISOMAP, and highlight the distinct merits of our approach.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[minimum spanning tree, single linkage hierarchical clustering, dendrogram, distance preservation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533339</person_id>
				<author_profile_id><![CDATA[81479662652]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Svonava]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533340</person_id>
				<author_profile_id><![CDATA[81479649225]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michail]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vlachos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934649</article_id>
		<sort_key>1450</sort_key>
		<display_label>Pages</display_label>
		<pages>1079-1084</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>142</seq_no>
		<title><![CDATA[Tru-Alarm]]></title>
		<subtitle><![CDATA[Trustworthiness Analysis of Sensor Networks in Cyber-Physical Systems]]></subtitle>
		<page_from>1079</page_from>
		<page_to>1084</page_to>
		<doi_number>10.1109/ICDM.2010.63</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934649</url>
		<abstract>
			<par><![CDATA[A Cyber-Physical System (CPS) integrates physical devices (e.g., sensors, cameras) with cyber (or informational)components to form a situation-integrated analytical system that responds intelligently to dynamic changes of the real-world scenarios. One key issue in CPS research is trustworthiness analysis of the observed data: Due to technology limitations and environmental influences, the CPS data are inherently noisy that may trigger many false alarms. It is highly desirable to sift meaningful information from a large volume of noisy data. In this paper, we propose a method called Tru-Alarm which finds out trustworthy alarms and increases the feasibility of CPS. Tru-Alarm estimates the locations of objects causing alarms, constructs an object-alarm graph and carries out trustworthiness inferences based on linked information in the graph. Extensive experiments show that Tru-Alarm filters out noises and false information efficiently and guarantees not missing any meaningful alarms.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Cyber Physical System, Alarm Detection]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531730</person_id>
				<author_profile_id><![CDATA[81375605800]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lu-An]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531731</person_id>
				<author_profile_id><![CDATA[81486653339]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Xiao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531732</person_id>
				<author_profile_id><![CDATA[81436592737]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sangkyum]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531733</person_id>
				<author_profile_id><![CDATA[81351593425]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jiawei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Han]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531734</person_id>
				<author_profile_id><![CDATA[81331494508]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Chih-Chieh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hung]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531735</person_id>
				<author_profile_id><![CDATA[81100022878]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Wen-Chih]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Peng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934650</article_id>
		<sort_key>1460</sort_key>
		<display_label>Pages</display_label>
		<pages>1085-1090</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>143</seq_no>
		<title><![CDATA[Node Similarities from Spreading Activation]]></title>
		<page_from>1085</page_from>
		<page_to>1090</page_to>
		<doi_number>10.1109/ICDM.2010.108</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934650</url>
		<abstract>
			<par><![CDATA[In this paper we propose two methods to derive two different kinds of node similarities in a network based on their neighborhood. The first similarity measure focuses on the overlap of direct and indirect neighbors. The second similarity compares nodes based on the structure of their - possibly also very distant - neighborhoods. Instead of using standard node measures, both similarities are derived from spreading activation patterns over time. Whereas in the first method the activation patterns are directly compared, in the second method the relative change of activation over time is compared. We apply both methods to a real-world graph dataset and discuss the results.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[spreading activation, graph analysis, node similarities, node signatures]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533884</person_id>
				<author_profile_id><![CDATA[81447602400]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kilian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thiel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533885</person_id>
				<author_profile_id><![CDATA[81100130304]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Berthold]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934651</article_id>
		<sort_key>1470</sort_key>
		<display_label>Pages</display_label>
		<pages>1091-1096</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>144</seq_no>
		<title><![CDATA[On the Vulnerability of Large Graphs]]></title>
		<page_from>1091</page_from>
		<page_to>1096</page_to>
		<doi_number>10.1109/ICDM.2010.54</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934651</url>
		<abstract>
			<par><![CDATA[Given a large graph, like a computer network, which k nodes should we immunize (or monitor, or remove), to make it as robust as possible against a computer virus attack? We need (a) a measure of the &#8216;Vulnerability&#8217; of a given network, b) a measure of the &#8216;Shield-value&#8217; of a specific set of k nodes and (c) a fast algorithm to choose the best such k nodes. We answer all these three questions: we give the justification behind our choices, we show that they agree with intuition as well as recent results in immunology. Moreover, we propose Net Shield, a fast and scalable algorithm. Finally, we give experiments on large real graphs, where Net Shield achieves tremendous speed savings exceeding 7 orders of magnitude, against straightforward competitors.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Vulnerability, immunization, graph mining, scalability]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533390</person_id>
				<author_profile_id><![CDATA[81337494052]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hanghang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533391</person_id>
				<author_profile_id><![CDATA[81414613593]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[B.]]></first_name>
				<middle_name><![CDATA[Aditya]]></middle_name>
				<last_name><![CDATA[Prakash]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533392</person_id>
				<author_profile_id><![CDATA[81384619692]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Charalampos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tsourakakis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533393</person_id>
				<author_profile_id><![CDATA[81100597717]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eliassi-Rad]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533394</person_id>
				<author_profile_id><![CDATA[81100373169]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Christos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Faloutsos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533395</person_id>
				<author_profile_id><![CDATA[81311482767]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Duen]]></first_name>
				<middle_name><![CDATA[Horng]]></middle_name>
				<last_name><![CDATA[Chau]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934652</article_id>
		<sort_key>1480</sort_key>
		<display_label>Pages</display_label>
		<pages>1097-1102</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>145</seq_no>
		<title><![CDATA[Testing the Significance of Patterns in Data with Cluster Structure]]></title>
		<page_from>1097</page_from>
		<page_to>1102</page_to>
		<doi_number>10.1109/ICDM.2010.61</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934652</url>
		<abstract>
			<par><![CDATA[Clustering is one of the basic operations in data analysis, and the cluster structure of a dataset often has a marked effect on observed patterns in data. Testing whether a data mining result is implied by the cluster structure can give substantial information on the formation of the dataset. We propose a new method for empirically testing the statistical significance of patterns in real-valued data in relation to the cluster structure. The method relies on principal component analysis and is based on the general idea of decomposing the data for the purpose of isolating the null model. We evaluate the performance of the method and the information it provides on various real datasets. Our results show that the proposed method is robust and provides nontrivial information about the origin of patterns in data, such as the source of classification accuracy and the observed correlations between attributes.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[significance testing, randomization, clustering, principal component analysis]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532309</person_id>
				<author_profile_id><![CDATA[81436602669]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Niko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vuokko]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532310</person_id>
				<author_profile_id><![CDATA[81479645188]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Petteri]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kaski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934658</article_id>
		<sort_key>1490</sort_key>
		<display_label>Pages</display_label>
		<pages>1103-1108</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>146</seq_no>
		<title><![CDATA[Compressed Nonnegative Sparse Coding]]></title>
		<page_from>1103</page_from>
		<page_to>1108</page_to>
		<doi_number>10.1109/ICDM.2010.162</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934658</url>
		<abstract>
			<par><![CDATA[Sparse Coding (SC), which models the data vectors as sparse linear combinations over basis vectors, has been widely applied in machine learning, signal processing and neuroscience. In this paper, we propose a dual random projection method to provide an efficient solution to Nonnegative Sparse Coding (NSC) using small memory. Experiments on real world data demonstrate the effectiveness of the proposed method.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530699</person_id>
				<author_profile_id><![CDATA[81479641355]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530700</person_id>
				<author_profile_id><![CDATA[81479648387]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ping]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934659</article_id>
		<sort_key>1500</sort_key>
		<display_label>Pages</display_label>
		<pages>1109-1114</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>147</seq_no>
		<title><![CDATA[Anonymizing Temporal Data]]></title>
		<page_from>1109</page_from>
		<page_to>1114</page_to>
		<doi_number>10.1109/ICDM.2010.96</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934659</url>
		<abstract>
			<par><![CDATA[Temporal data are time-critical in that the snapshot at each timestamp must be made available to researchers in a timely fashion. However, due to the limited data, each snapshot likely has a skewed distribution on sensitive values, which renders classical anonymization methods not possible. In this work, we propose the &#8220;reposition model&#8221; to allow a record to be published within a close proximity of original timestamp. We show that reposition over a small proximity of timestamp is sufficient for reducing the skewness of a snapshot, therefore, minimizing the impact on window queries. We formalize the optimal reposition problem and present a linear-time solution. The contribution of this work is that it enables classical methods on temporal data.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Privacy, Anonymization, Temporal Data]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532821</person_id>
				<author_profile_id><![CDATA[81350574174]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532822</person_id>
				<author_profile_id><![CDATA[81479649629]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yabo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532823</person_id>
				<author_profile_id><![CDATA[81406592097]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Raymond]]></first_name>
				<middle_name><![CDATA[Chi-Wing]]></middle_name>
				<last_name><![CDATA[Wong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532824</person_id>
				<author_profile_id><![CDATA[81451592510]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ada]]></first_name>
				<middle_name><![CDATA[Wai-Chee]]></middle_name>
				<last_name><![CDATA[Fu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934660</article_id>
		<sort_key>1510</sort_key>
		<display_label>Pages</display_label>
		<pages>1115-1120</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>148</seq_no>
		<title><![CDATA[Homotopy Regularization for Boosting]]></title>
		<page_from>1115</page_from>
		<page_to>1120</page_to>
		<doi_number>10.1109/ICDM.2010.14</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934660</url>
		<abstract>
			<par><![CDATA[In this paper, we present a homotopy regularization algorithm for boosting. We introduce a regularization term with adaptive weight into the boosting framework and compose a homotopy objective function. Optimization of this objective approximately composes a solution path for the regularized boosting. Following this path, we can find suitable solution efficiently using early stopping. Experiments show that this adaptive regularization method gives a more efficient parameter selection strategy than regularized boosting and semi supervised boosting algorithms, and significantly improves the performances of traditional AdaBoost and related methods.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[homotopy regularization, boosting]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530701</person_id>
				<author_profile_id><![CDATA[81474674324]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Zheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530702</person_id>
				<author_profile_id><![CDATA[81479651248]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yangqiu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Song]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530703</person_id>
				<author_profile_id><![CDATA[81372592002]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Changshui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934661</article_id>
		<sort_key>1520</sort_key>
		<display_label>Pages</display_label>
		<pages>1121-1126</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>149</seq_no>
		<title><![CDATA[What Do People Want in Microblogs? Measuring Interestingness of Hashtags in Twitter]]></title>
		<page_from>1121</page_from>
		<page_to>1126</page_to>
		<doi_number>10.1109/ICDM.2010.34</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934661</url>
		<abstract>
			<par><![CDATA[When micro logging becomes a very popular social media, finding interesting posts from high volume stream of user posts is a challenging research problem. To organize large number of posts, users can assign tags to posts so that these posts can be navigated and searched by tag. In this paper, we focus on modeling the interestingness of hash tags in Twitter, the largest and most active micro logging site. We propose to first construct communities based on both follow links and tagged interactions. We then measure the dispersion and divergence of users and tweets using hash tags among the constructed communities. The interestingness of hash tags are then derived from these community-based dispersion and divergence features. We further introduce a supervised approach to rank hash tags by interestingness. Our experiments on a Twitter dataset show that the proposed approach achieves a fairly good performance.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Twitter, hashtag, interestingness, ranking]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530704</person_id>
				<author_profile_id><![CDATA[81479656267]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jianshu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Weng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530705</person_id>
				<author_profile_id><![CDATA[81100399142]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ee-Peng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530706</person_id>
				<author_profile_id><![CDATA[81100331201]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Qi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[He]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530707</person_id>
				<author_profile_id><![CDATA[81337490928]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Cane]]></first_name>
				<middle_name><![CDATA[Wing-Ki]]></middle_name>
				<last_name><![CDATA[Leung]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934662</article_id>
		<sort_key>1530</sort_key>
		<display_label>Pages</display_label>
		<pages>1127-1132</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>150</seq_no>
		<title><![CDATA[Probabilistic Inference Protection on Anonymized Data]]></title>
		<page_from>1127</page_from>
		<page_to>1132</page_to>
		<doi_number>10.1109/ICDM.2010.18</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934662</url>
		<abstract>
			<par><![CDATA[Background knowledge is an important factor in privacy preserving data publishing. Probabilistic distribution-based background knowledge is a powerful kind of background knowledge which is easily accessible to adversaries. However, to the best of our knowledge, there is no existing work that can provide a privacy guarantee under adversary attack with such background knowledge. The difficulty of the problem lies in the high complexity of the probability computation and the non-monotone nature of the privacy condition. The only solution known to us relies on approximate algorithms with no known error bound. In this paper, we propose a new bounding condition that overcomes the difficulties of the problem and gives a privacy guarantee. This condition is based on probability deviations in the anonymized data groups, which is much easier to compute and which is a monotone function on the grouping sizes.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[privacy preserving data publishing, k-anonymity, l-diversity]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530708</person_id>
				<author_profile_id><![CDATA[81406592097]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Raymond]]></first_name>
				<middle_name><![CDATA[Chi-Wing]]></middle_name>
				<last_name><![CDATA[Wong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530709</person_id>
				<author_profile_id><![CDATA[81451592510]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ada]]></first_name>
				<middle_name><![CDATA[Wai-Chee]]></middle_name>
				<last_name><![CDATA[Fu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530710</person_id>
				<author_profile_id><![CDATA[81350574174]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530711</person_id>
				<author_profile_id><![CDATA[81384612266]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yabo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530712</person_id>
				<author_profile_id><![CDATA[81100323054]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pei]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530713</person_id>
				<author_profile_id><![CDATA[81350576309]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Philip]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934663</article_id>
		<sort_key>1540</sort_key>
		<display_label>Pages</display_label>
		<pages>1133-1138</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>151</seq_no>
		<title><![CDATA[Collaborative Learning between Visual Content and Hidden Semantic for Image Retrieval]]></title>
		<page_from>1133</page_from>
		<page_to>1138</page_to>
		<doi_number>10.1109/ICDM.2010.27</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934663</url>
		<abstract>
			<par><![CDATA[Similarity measure is a critical component in image retrieval systems, and learning similarity measure from the relevance feedback has become a promising way to enhance retrieval performance. Existing approaches mainly focus on learning the visual similarity measure from online feedbacks or constructing the semantic similarity measure depended on historical feedbacks log. However, there is still a big room to elevate the retrieval performance, because few works take the relationship between the visual similarity and the semantic similarity into account. This paper proposes the collaborative learning similarity measure, CoSim, which focuses on the collaborative learning between the visual content of images and the hidden semantic in log. Concretely, the semantic similarity is first learned from log data and serves as prior knowledge. Then, the visual similarity is learned from a mixture of labeled and unlabeled images. In particular, unlabeled images are exploited for the relevant and irrelevant classes in different ways. Finally, the collaborative learning similarity is produced by integrating the visual similarity and the semantic similarity in a nonlinear way. An empirical study shows that the proposed CoSim is significantly more effective than some existing approaches.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[image retrieval, relevance feedback, short-term learning, long-term learning, collaborative learning]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533920</person_id>
				<author_profile_id><![CDATA[81466645769]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533921</person_id>
				<author_profile_id><![CDATA[81466642965]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ming-Yu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533922</person_id>
				<author_profile_id><![CDATA[81474658141]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chun-Li]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934664</article_id>
		<sort_key>1550</sort_key>
		<display_label>Pages</display_label>
		<pages>1139-1144</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>152</seq_no>
		<title><![CDATA[Max-Clique]]></title>
		<subtitle><![CDATA[A Top-Down Graph-Based Approach to Frequent Pattern Mining]]></subtitle>
		<page_from>1139</page_from>
		<page_to>1144</page_to>
		<doi_number>10.1109/ICDM.2010.73</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934664</url>
		<abstract>
			<par><![CDATA[Frequent pattern mining is a fundamental problem in data mining research. We note that almost all state-of-the art algorithms may not be able to mine very long patterns in a large database with a huge set of frequent patterns. In this paper, we point our research to solve this difficult problem from a different perspective: we focus on mining top-k long maximal frequent patterns because long patterns are in general more interesting ones. Different from traditional level-wise mining or tree-growth strategies, our method works in a top-down manner. We pull large maximal cliques from a pattern graph constructed after some fast initial processing, and directly use such large-sized maximal cliques as promising candidates for long frequent patterns. A separate refinement stage is needed to further transform these candidates into true maximal patterns.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[frequent pattern mining, top-down, pattern graph]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532329</person_id>
				<author_profile_id><![CDATA[81479655737]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532330</person_id>
				<author_profile_id><![CDATA[81350576309]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Philip]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934501</article_id>
		<sort_key>1560</sort_key>
		<display_label>Pages</display_label>
		<pages>1145-1150</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>153</seq_no>
		<title><![CDATA[Personalizing Web Page Recommendation via Collaborative Filtering and Topic-Aware Markov Model]]></title>
		<page_from>1145</page_from>
		<page_to>1150</page_to>
		<doi_number>10.1109/ICDM.2010.28</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934501</url>
		<abstract>
			<par><![CDATA[Web-page recommendation is to predict the next request of pages that Web users are potentially interested in when surfing the Web. This technique can guide Web users to find more useful pages without asking for them explicitly and has attracted much attention in the community of Web mining. However, few studies on Web page recommendation consider personalization, which is an indispensable feature to meet various preferences of users. In this paper, we propose a personalized Web page recommendation model called PIGEON (abbr. for PersonalIzed web paGe rEcommendatiON) via collaborative filtering and a topic-aware Markov model. We propose a graph-based iteration algorithm to discover users' interested topics, based on which user similarities are measured. To recommend topically coherent pages, we propose a topic-aware Markov model to learn users' navigation patterns which capture both temporal and topical relevance of pages. A thorough experimental evaluation conducted on a large real dataset demonstrates PIGEON's effectiveness and efficiency.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Web Page Clustering, Personalized Recommendation, Collaborative Filtering, Markov model]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531807</person_id>
				<author_profile_id><![CDATA[81479650186]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Qingyan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531808</person_id>
				<author_profile_id><![CDATA[81479642901]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ju]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531809</person_id>
				<author_profile_id><![CDATA[81479650257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jianyong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531810</person_id>
				<author_profile_id><![CDATA[81479656781]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Lizhu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934502</article_id>
		<sort_key>1570</sort_key>
		<display_label>Pages</display_label>
		<pages>1151-1156</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>154</seq_no>
		<title><![CDATA[Passive Sampling for Regression]]></title>
		<page_from>1151</page_from>
		<page_to>1156</page_to>
		<doi_number>10.1109/ICDM.2010.9</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934502</url>
		<abstract>
			<par><![CDATA[Active sampling (also called active learning or selective sampling) has been extensively researched for classification and rank learning methods, which is to select the most informative samples from unlabeled data such that, once the samples are labeled, the accuracy of the function learned from the samples is maximized. While active sampling methods require learning a function at each iteration to find the most informative samples, this paper proposes passive sampling techniques for regression, which find the informative samples not based on the learned function but based on the samples' geometric characteristics in the feature space. Passive sampling is more efficient than active sampling, as it does not require, at each iteration, learning and validating the regression functions and evaluating the unlabeled data using the function. For regression, passive sampling is also more effective, Active sampling for regression suffers from serious performance fluctuations in practice, because it selects the samples of highest regression errors and such samples are likely noisy. Passive sampling, on the other hand, shows more stable performance. We observe from our extensive experiments that our passive sampling methods perform even better than the ``omniscient'' active sampling that knows the labels of unlabeled data.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[selective sampling, passive sampling, active learning, active sampling, regression]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532385</person_id>
				<author_profile_id><![CDATA[81479656917]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hwanjo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532386</person_id>
				<author_profile_id><![CDATA[81447593052]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sungchul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934503</article_id>
		<sort_key>1580</sort_key>
		<display_label>Pages</display_label>
		<pages>1157-1162</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>155</seq_no>
		<title><![CDATA[Modeling Experts and Novices in Citizen Science Data for Species Distribution Modeling]]></title>
		<page_from>1157</page_from>
		<page_to>1162</page_to>
		<doi_number>10.1109/ICDM.2010.103</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934503</url>
		<abstract>
			<par><![CDATA[Citizen scientists, who are volunteers from the community that participate as field assistants in scientific studies [3], enable research to be performed at much larger spatial and temporal scales than trained scientists can cover. Species distribution modeling [6], which involves understanding species-habitat relationships, is a research area that can benefit greatly from citizen science. The eBird project [16] is one of the largest citizen science programs in existence. By allowing birders to upload observations of bird species to an online database, eBird can provide useful data for species distribution modeling. However, since birders vary in their levels of expertise, the quality of data submitted to eBird is often questioned. In this paper, we develop a probabilistic model called the Occupancy-Detection-Expertise (ODE) model that incorporates the expertise of birders submitting data to eBird. We show that modeling the expertise of birders can improve the accuracy of predicting observations of a bird species at a site. In addition, we can use the ODE model for two other tasks: predicting birder expertise given their history of eBird checklists and identifying bird species that are difficult for novices to detect.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Applications, Species Distribution Modeling, Citizen Science, Graphical Models, Contrast Mining]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2533443</person_id>
				<author_profile_id><![CDATA[81479657898]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533444</person_id>
				<author_profile_id><![CDATA[81479655588]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Weng-Keen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2533445</person_id>
				<author_profile_id><![CDATA[81430672641]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Rebecca]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Hutchinson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934504</article_id>
		<sort_key>1590</sort_key>
		<display_label>Pages</display_label>
		<pages>1163-1168</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>156</seq_no>
		<title><![CDATA[Causal Discovery from Streaming Features]]></title>
		<page_from>1163</page_from>
		<page_to>1168</page_to>
		<doi_number>10.1109/ICDM.2010.82</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934504</url>
		<abstract>
			<par><![CDATA[In this paper, we study a new research problem of causal discovery from streaming features. A unique characteristic of streaming features is that not all features can be available before learning begins. Feature generation and selection often have to be interleaved. Managing streaming features has been extensively studied in classification, but little attention has been paid to the problem of causal discovery from streaming features. To this end, we propose a novel algorithm to solve this challenging problem, denoted as CDFSF (Causal Discovery From Streaming Features) which consists of two phases: growing and shrinking. In the growing phase, CDFSF finds candidate parents or children for each feature seen so far, while in the shrinking phase the algorithm dynamically removes false positives from the current sets of candidate parents and children. In order to improve the efficiency of CDFSF, we present S-CDFSF, a faster version of CDFSF, using two symmetry theorems. Experimental results validate our algorithms in comparison with other state-of-art algorithms of causal discovery.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[causal discovery, streaming features, Bayesian networks]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532881</person_id>
				<author_profile_id><![CDATA[81460649624]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532882</person_id>
				<author_profile_id><![CDATA[81452596585]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Xindong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532883</person_id>
				<author_profile_id><![CDATA[81438598366]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532884</person_id>
				<author_profile_id><![CDATA[81408597754]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Wei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ding]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934505</article_id>
		<sort_key>1600</sort_key>
		<display_label>Pages</display_label>
		<pages>1169-1174</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>157</seq_no>
		<title><![CDATA[ABS]]></title>
		<subtitle><![CDATA[The Anti Bouncing Model for Usage Data Streams]]></subtitle>
		<page_from>1169</page_from>
		<page_to>1174</page_to>
		<doi_number>10.1109/ICDM.2010.91</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934505</url>
		<abstract>
			<par><![CDATA[Usage data mining is an important research area with applications in various fields. However, usage data is usually considered streaming, due to its high volumes and rates. Because of these characteristics, we only have access, at any point in time, to a small fraction of the stream. When the data is observed through such a limited window, it is challenging to give a reliable description of the recent usage data. We study the important consequences of these constraints, through the &#8220;bounce rate&#8221; problem and the clustering of usage data streams. Then, we propose the ABS (Anti-Bouncing Stream) model which combines the advantages of previous models but discards their drawbacks. First, under the same resource constraints as existing models in the literature, ABS can better model the recent data. Second, owing to its simple but effective management approach, the data in ABS is available at any time for analysis. We demonstrate its superiority through a theoretical study and experiments on two real-world data sets.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[data streams, usage, clustering, bounce rate]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531279</person_id>
				<author_profile_id><![CDATA[81548028172]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chongsheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531280</person_id>
				<author_profile_id><![CDATA[81100234721]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Florent]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Masseglia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531281</person_id>
				<author_profile_id><![CDATA[81323492609]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yves]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lechevallier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934511</article_id>
		<sort_key>1610</sort_key>
		<display_label>Pages</display_label>
		<pages>1175-1180</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>158</seq_no>
		<title><![CDATA[Classifier and Cluster Ensembles for Mining Concept Drifting Data Streams]]></title>
		<page_from>1175</page_from>
		<page_to>1180</page_to>
		<doi_number>10.1109/ICDM.2010.125</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934511</url>
		<abstract>
			<par><![CDATA[Ensemble learning is a commonly used tool for building prediction models from data streams, due to its intrinsic merits of handling large volumes stream data. Despite of its extraordinary successes in stream data mining, existing ensemble models, in stream data environments, mainly fall into the ensemble classifiers category, without realizing that building classifiers requires labor intensive labeling process, and it is often the case that we may have a small number of labeled samples to train a few classifiers, but a large number of unlabeled samples are available to build clusters from data streams. Accordingly, in this paper, we propose a new ensemble model which combines both classifiers and clusters together for mining data streams. We argue that the main challenges of this new ensemble model include (1) clusters formulated from data streams only carry cluster IDs, with no genuine class label information, and (2) concept drifting underlying data streams makes it even harder to combine clusters and classifiers into one ensemble framework. To handle challenge (1), we present a label propagation method to infer each cluster's class label by making full use of both class label information from classifiers, and internal structure information from clusters. To handle challenge (2), we present a new weighting schema to weight all base models according to their consistencies with the up-to-date base model. As a result, all classifiers and clusters can be combined together, through a weighted average mechanism, for prediction. Experiments on real-world data streams demonstrate that our method outperforms simple classifier ensemble and cluster ensemble for stream data mining.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Data Stream Mining, Classification, Ensemble Learning, Concept Drifting]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2529628</person_id>
				<author_profile_id><![CDATA[81384603947]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Peng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529629</person_id>
				<author_profile_id><![CDATA[81452600756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Xingquan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529630</person_id>
				<author_profile_id><![CDATA[81461653571]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jianlong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2529631</person_id>
				<author_profile_id><![CDATA[81325488381]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Li]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934512</article_id>
		<sort_key>1620</sort_key>
		<display_label>Pages</display_label>
		<pages>1181-1186</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>159</seq_no>
		<title><![CDATA[Graph-Based Semi-supervised Learning with Adaptive Similarity Estimation]]></title>
		<page_from>1181</page_from>
		<page_to>1186</page_to>
		<doi_number>10.1109/ICDM.2010.30</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934512</url>
		<abstract>
			<par><![CDATA[Graph-based semi-supervised learning algorithms have attracted a lot of attention. Constructing a good graph is playing an essential role for all these algorithms. Many existing graph construction methods(e.g. Gaussian Kernel etc.) require user input parameter, which is hard to configure manually. In this paper, we propose a parameter-free similarity measure Adaptive Similarity Estimation (ASE), which constructs the graph by adaptively optimizing linear combination of its neighbors. Experimental results show the effectiveness of our proposed method.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[semi-supervised learning, classification, adaptive similarity estimation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530263</person_id>
				<author_profile_id><![CDATA[81479659216]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xianchao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530264</person_id>
				<author_profile_id><![CDATA[81479656825]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yansheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jiang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530265</person_id>
				<author_profile_id><![CDATA[81479641565]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Wenxin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530266</person_id>
				<author_profile_id><![CDATA[81479649240]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Xin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Han]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934513</article_id>
		<sort_key>1630</sort_key>
		<display_label>Pages</display_label>
		<pages>1187-1192</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>160</seq_no>
		<title><![CDATA[K-AP]]></title>
		<subtitle><![CDATA[Generating Specified K Clusters by Efficient Affinity Propagation]]></subtitle>
		<page_from>1187</page_from>
		<page_to>1192</page_to>
		<doi_number>10.1109/ICDM.2010.107</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934513</url>
		<abstract>
			<par><![CDATA[The Affinity Propagation (AP) clustering algorithm proposed by Frey and Dueck (2007) provides an understandable, nearly optimal summary of a data set. However, it suffers two major shortcomings: i) the number of clusters is vague with the user-defined parameter called self-confidence, and ii) the quadratic computational complexity. When aiming at a given number of clusters due to prior knowledge, AP has to be launched many times until an appropriate setting of self-confidence is found. The re-launched AP increases the computational cost by one order of magnitude. In this paper, we propose an algorithm, called K-AP, to exploit the immediate results of K clusters by introducing a constraint in the process of message passing. Through theoretical analysis and experimental validation, K-AP was shown to be able to directly generate K clusters as user defined, with a negligible increase of computational cost compared to AP. In the meanwhile, KAP preserves the clustering quality as AP in terms of the distortion. K-AP is more effective than k-medoids w.r.t. the distortion minimization and higher clustering purity.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[clustering, affinity propagation, k-medoids]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530267</person_id>
				<author_profile_id><![CDATA[81436599318]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xiangliang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530268</person_id>
				<author_profile_id><![CDATA[81423592797]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530269</person_id>
				<author_profile_id><![CDATA[81758776157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kjetil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Norvag]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530270</person_id>
				<author_profile_id><![CDATA[81100614693]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Michele]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sebag]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934514</article_id>
		<sort_key>1640</sort_key>
		<display_label>Pages</display_label>
		<pages>1193-1198</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>161</seq_no>
		<title><![CDATA[MoodCast]]></title>
		<subtitle><![CDATA[Emotion Prediction via Dynamic Continuous Factor Graph Model]]></subtitle>
		<page_from>1193</page_from>
		<page_to>1198</page_to>
		<doi_number>10.1109/ICDM.2010.105</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934514</url>
		<abstract>
			<par><![CDATA[Human emotion is one important underlying force affecting and affected by the dynamics of social networks. An interesting question is &#8220;can we predict a person&#8217;s mood based on his historic emotion log and his social network?&#8221;. In this paper, we propose a Mood Cast method based on a dynamic continuous factor graph model for modeling and predicting users&#8217; emotions in a social network. Mood Cast incorporates users&#8217; dynamic status information (e.g., locations, activities, and attributes) and social influence from users&#8217; friends into a unified model. Based on the historical information (e.g., network structure and users&#8217; status from time 0 to t&#8722;1), Mood Cast learns a discriminative model for predicting users&#8217; emotion status at time t. To the best of our knowledge, this work takes the first step in designing a principled model for emotion prediction in social networks. Our experimental results on both real social network and virtual web-based network show that we can accurately predict emotion status of more than 62% of users and 8+% improvement than the baseline methods.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Social network, Predictive model, Emotion dynamics, Social influence]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530271</person_id>
				<author_profile_id><![CDATA[81447599559]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530272</person_id>
				<author_profile_id><![CDATA[81548005696]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530273</person_id>
				<author_profile_id><![CDATA[81479656771]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jimeng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530274</person_id>
				<author_profile_id><![CDATA[81479654173]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yiran]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530275</person_id>
				<author_profile_id><![CDATA[81474662556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jinghai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934515</article_id>
		<sort_key>1650</sort_key>
		<display_label>Pages</display_label>
		<pages>1199-1204</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>162</seq_no>
		<title><![CDATA[Hierarchical Ensemble Clustering]]></title>
		<page_from>1199</page_from>
		<page_to>1204</page_to>
		<doi_number>10.1109/ICDM.2010.98</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934515</url>
		<abstract>
			<par><![CDATA[Ensemble clustering has emerged as an important elaboration of the classical clustering problems. Ensemble clustering refers to the situation in which a number of different (input) clusterings have been obtained for a particular dataset and it is desired to find a single (consensus) clustering which is a better fit in some sense than the existing clusterings. Many approaches have been developed to solve ensemble clustering problems over the last few years. However, most of these ensemble techniques are designed for partitional clustering methods. Few research efforts have been reported for ensemble hierarchical clustering methods. In this paper, we propose a hierarchical ensemble clustering framework which can naturally combine both partitional clustering and hierarchical clustering results. We notice the importance of ultra-metric distance for hierarchical clustering and propose a novel method for learning the ultra-metric distance from the aggregated distance matrices and generating final hierarchical clustering with enhanced cluster separation. Experimental results demonstrate the effectiveness of our proposed approaches.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Hierarchical ensemble clustering, Ultra-metric]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532418</person_id>
				<author_profile_id><![CDATA[81538091556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Li]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zheng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532419</person_id>
				<author_profile_id><![CDATA[81100475528]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532420</person_id>
				<author_profile_id><![CDATA[81100136610]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ding]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934520</article_id>
		<sort_key>1660</sort_key>
		<display_label>Pages</display_label>
		<pages>1205-1210</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>163</seq_no>
		<title><![CDATA[Frequent Instruction Sequential Pattern Mining in Hardware Sample Data]]></title>
		<page_from>1205</page_from>
		<page_to>1210</page_to>
		<doi_number>10.1109/ICDM.2010.123</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934520</url>
		<abstract>
			<par><![CDATA[When parallelism and heterogeneity has become the trend for computer system design, both the size and the complexity of the hardware sample data generated by Performance Monitoring Unit (PMU) increase fast, thus automatic analysis methods, i.e. data mining methods, are urgently needed to accelerate hardware sample data analysis. We are the first to study instruction sequential pattern mining for hardware sample data. It is a challenging task due to the implicit sequential relationship contained in the data and due to the importance of low frequency patterns. As a solution, we i) provide a novel algorithm ProfSpan, ii) adapt two existing algorithms, which are based on candidate generation and projected database generation, to hardware sample data. Our evaluation results show ProfSpan can reduce up to 75% and 80% of execution time compared with other two algorithms. Particularly, up to 50% of frequent patterns mined by ProfSpan in hardware sample data are crossing basic block boundaries and can not be found by existing methods for source code or disassembly code. We also analyze three example patterns identified by ProfSpan: consecutive loads, JIT entry sequence, and conditional code dependency sequence, to illustrate how ProfSpan can benefit performance analysis. Finally, we apply patterns to module classification and obtain promising results.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[sequential patter mining, hardware sample data, performance analysis]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2531849</person_id>
				<author_profile_id><![CDATA[81479651373]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531850</person_id>
				<author_profile_id><![CDATA[81479649109]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xiao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531851</person_id>
				<author_profile_id><![CDATA[81556991056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Rui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2531852</person_id>
				<author_profile_id><![CDATA[81479648640]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yanqi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934521</article_id>
		<sort_key>1670</sort_key>
		<display_label>Pages</display_label>
		<pages>1211-1216</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>164</seq_no>
		<title><![CDATA[Efficient Episode Mining with Minimal and Non-overlapping Occurrences]]></title>
		<page_from>1211</page_from>
		<page_to>1216</page_to>
		<doi_number>10.1109/ICDM.2010.25</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934521</url>
		<abstract>
			<par><![CDATA[Frequent serial episodes within an event sequence describe the behavior of users or systems about the application. Existing mining algorithms calculate the frequency of an episode based on overlapping or non-minimal occurrences, which is prone to over-counting the support of long episodes or poorly characterizing the followed-by-closely relationship over event types. In addition, due to utilizing the Apriori-style level wise approach, these algorithms are computationally expensive. In this paper, we propose an efficient algorithm MANEPI (Minimal And Non-overlapping EPIsode) for mining more interesting frequent episodes within the given event sequence. The proposed frequency measure takes both minimal and non-overlapping occurrences of an episode into consideration and ensures better mining quality. The introduced depth first search strategy with the Apriori Property for performing episode growth greatly improves the efficiency of mining long episodes because of scanning the given sequence only once and not generating candidate episodes. Moreover, an optimization technique is presented to narrow down search space and speed up the mining process. Experimental evaluation on both synthetic and real-world datasets demonstrates that our algorithms are more efficient and effective.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Data mining, Event sequence, Frequent episode, Prefix tree, Minimal and non-overlapping occurrences]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2530803</person_id>
				<author_profile_id><![CDATA[81479649064]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Huisheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530804</person_id>
				<author_profile_id><![CDATA[81451596194]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Peng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530805</person_id>
				<author_profile_id><![CDATA[81488652681]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Xianmang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[He]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530806</person_id>
				<author_profile_id><![CDATA[81485648826]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yujia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530807</person_id>
				<author_profile_id><![CDATA[81416594540]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Wei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2530808</person_id>
				<author_profile_id><![CDATA[81479650779]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Baile]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934522</article_id>
		<sort_key>1680</sort_key>
		<display_label>Page</display_label>
		<pages>1217</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>165</seq_no>
		<title><![CDATA[Spatial and Spatio-temporal Data Mining]]></title>
		<page_from>1217</page_from>
		<doi_number>10.1109/ICDM.2010.166</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934522</url>
		<abstract>
			<par><![CDATA[The recent advances and price reduction of technologies for collecting spatial and spatio-temporal data like Satellite Images, Cellular Phones, Sensor Networks, and GPS devices has facilitated the collection of data referenced in space and time. These huge collections of data often hide interesting information which conventional systems and classical data mining techniques are unable to discover. Spatial and spatio-temporal data are embedded in continuous space, whereas classical datasets (e.g. transactions) are often discrete. Spatial and spatio-temporal data require complex data preprocessing, transformation, data mining, and post-processing techniques to extract novel, useful, and understandable patterns. The importance of spatial and spatio-temporal data mining is growing with the increasing incidence and importance of large geo-spatial datasets such as maps, repositories of remote-sensing images, trajectories of moving objects generated by mobile devices, etc. Applications include Mobile-commerce industry (location-based services), climatologically effects of El Nino, land-use classification and global change using satellite imagery, finding crime hot spots, local instability in traffic, migration of birds, fishing control, pedestrian behavior analysis, and so on. Thus, new methods are needed to analyze spatial and spatio-temporal data to extract interesting, useful, and non-trivial patterns. The main goal of this tutorial is to disseminate this research field, giving an overview of the current state of the art and the main methodologies and algorithms for spatial and spatio-temporal data mining. This tutorial is directed to researches and practitioners, experts in data mining, analysts of spatial and spatio-temporal data, as well as knowledge engineers and domain experts from different application areas.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[spatial data mining, trajectory data mining, semantic trajectory data mining, semantic trajectory pattern mining]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532440</person_id>
				<author_profile_id><![CDATA[81320488492]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Vania]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bogorny]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532441</person_id>
				<author_profile_id><![CDATA[81100610476]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shekhar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934523</article_id>
		<sort_key>1690</sort_key>
		<display_label>Page</display_label>
		<pages>1218</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>166</seq_no>
		<title><![CDATA[Knowledge Discovery in Academic Drug Discovery Programs]]></title>
		<subtitle><![CDATA[Opportunities and Challenges]]></subtitle>
		<page_from>1218</page_from>
		<doi_number>10.1109/ICDM.2010.163</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934523</url>
		<abstract>
			<par><![CDATA[In United State several universities and research institutes including the national health institute (NIH) recently started programs aiming for drug discovery. With the initiatives, huge volumes of data have been collected and shared with public free of charge. Those initiatives provide an unprecedented opportunity for data miner and machine learner to study knowledge discovery problems associated with drug design. In this tutorial, the presenter will review the knowledge discovery and management needs in the drug discovery process. Latest methodology development, primarily those from data mining, machine learning, and statistical learning will be discussed.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2529697</person_id>
				<author_profile_id><![CDATA[81479644488]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934524</article_id>
		<sort_key>1700</sort_key>
		<display_label>Page</display_label>
		<pages>1219</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>167</seq_no>
		<title><![CDATA[How to Do Good Data Mining Research and Get it Published in Top Venues]]></title>
		<page_from>1219</page_from>
		<doi_number>10.1109/ICDM.2010.165</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934524</url>
		<abstract>
			<par><![CDATA[While ICDM has traditionally enjoyed an unusually high quality of reviewing, there is no doubt that publishing in ICDM is very challenging. In this tutorial Dr. Keogh will demonstrate some simple ideas to enhance the probability of success in getting your paper published in a top data mining conference, and after the work is published, getting it highly cited.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Research, Experimentation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2529698</person_id>
				<author_profile_id><![CDATA[81493650377]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eamonn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Keogh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934530</article_id>
		<sort_key>1710</sort_key>
		<display_label>Page</display_label>
		<pages>1220</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>168</seq_no>
		<title><![CDATA[Discovering Multiple Clustering Solutions]]></title>
		<subtitle><![CDATA[Grouping Objects in Different Views of the Data]]></subtitle>
		<page_from>1220</page_from>
		<doi_number>10.1109/ICDM.2010.85</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934530</url>
		<abstract>
			<par><![CDATA[Traditional clustering algorithms identify just a single clustering of the data. Today's complex data, however, allow multiple interpretations leading to several valid groupings hidden in different views of the database. Each of these multiple clustering solutions is valuable and interesting as different perspectives on the same data and several meaningful groupings for each object are given. Especially for high dimensional data where each object is described by multiple attributes, alternative clusters in different attribute subsets are of major interest. In this tutorial, we describe several real world application scenarios for multiple clustering solutions. We abstract from these scenarios and provide the general challenges in this emerging research area. We describe state-of-the-art paradigms, we highlight specific techniques, and we give an overview of this topic by providing a taxonomy of the existing methods. By focusing on open challenges, we try to attract young researchers for participating in this emerging research field.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[data mining, subspace clustering, orthogonal clustering, alternative clustering, multiple perspectives]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2532970</person_id>
				<author_profile_id><![CDATA[81350600194]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Emmanuel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Muller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532971</person_id>
				<author_profile_id><![CDATA[81447604694]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Stephan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gunnemann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532972</person_id>
				<author_profile_id><![CDATA[81447603438]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ines]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Farber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2532973</person_id>
				<author_profile_id><![CDATA[81100145971]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seidl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934531</article_id>
		<sort_key>1720</sort_key>
		<display_label>Pages</display_label>
		<pages>1221-1226</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>169</seq_no>
		<title><![CDATA[Author Index]]></title>
		<page_from>1221</page_from>
		<page_to>1226</page_to>
		<doi_number>10.1109/ICDM.2010.168</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934531</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1934532</article_id>
		<sort_key>1730</sort_key>
		<display_label>Page</display_label>
		<pages>1228</pages>
		<article_publication_date>12-13-2010</article_publication_date>
		<seq_no>170</seq_no>
		<title><![CDATA[Publisher Information]]></title>
		<page_from>1228</page_from>
		<doi_number>10.1109/ICDM.2010.169</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1934532</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
	</article_rec>
</content>
</proceeding>
