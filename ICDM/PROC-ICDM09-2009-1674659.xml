<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>12-06-2009</start_date>
		<end_date>12-09-2009</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[]]></city>
		<state></state>
		<country></country>
	</conference_loc>
	<conference_url>http://computer.org/proceedings/icdm/2009/3895</conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES11036</series_id>
		<series_title><![CDATA[ICDM]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>1674659</proc_id>
	<acronym>ICDM '09</acronym>
	<proc_desc>Proceedings of the 2009 Ninth IEEE International Conference on Data Mining</proc_desc>
	<conference_number></conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn13>978-0-7695-3895-2</isbn13>
	<issn>1550-4786</issn>
	<eissn></eissn>
	<copyright_year>2009</copyright_year>
	<publication_date>12-06-2009</publication_date>
	<pages></pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<publisher>
		<publisher_id>PUB769</publisher_id>
		<publisher_code>IEEEW</publisher_code>
		<publisher_name>IEEE Computer Society</publisher_name>
		<publisher_address>1730 Massachusetts Ave., NW             Washington, DC</publisher_address>
		<publisher_city></publisher_city>
		<publisher_state></publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>20036-1992</publisher_zip_code>
		<publisher_contact></publisher_contact>
		<publisher_phone></publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url></publisher_url>
	</publisher>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
</proceeding_rec>
<content>
	<article_rec>
		<article_id>1677005</article_id>
		<sort_key>10</sort_key>
		<display_label>Page</display_label>
		<pages>C4,C1</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Cover Art]]></title>
		<page_from>C1</page_from>
		<doi_number>10.1109/ICDM.2009.152</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677005</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677006</article_id>
		<sort_key>20</sort_key>
		<display_label>Page</display_label>
		<pages>i</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Title Page i]]></title>
		<page_from>i</page_from>
		<doi_number>10.1109/ICDM.2009.1</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677006</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677007</article_id>
		<sort_key>30</sort_key>
		<display_label>Page</display_label>
		<pages>iii</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Title Page iii]]></title>
		<page_from>iii</page_from>
		<doi_number>10.1109/ICDM.2009.2</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677007</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677008</article_id>
		<sort_key>40</sort_key>
		<display_label>Page</display_label>
		<pages>iv</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Copyright Page]]></title>
		<page_from>iv</page_from>
		<doi_number>10.1109/ICDM.2009.3</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677008</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677009</article_id>
		<sort_key>60</sort_key>
		<display_label>Page</display_label>
		<pages>xiv</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Message from the General Co-Chairs]]></title>
		<page_from>xiv</page_from>
		<doi_number>10.1109/ICDM.2009.5</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677009</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677010</article_id>
		<sort_key>70</sort_key>
		<display_label>Page</display_label>
		<pages>xv</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Message from the Program Committee Co-Chairs]]></title>
		<page_from>xv</page_from>
		<doi_number>10.1109/ICDM.2009.150</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677010</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677011</article_id>
		<sort_key>110</sort_key>
		<display_label>Pages</display_label>
		<pages>xxv-xxvi</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[ICDM 2009 Program]]></title>
		<page_from>xxv</page_from>
		<page_to>xxvi</page_to>
		<doi_number>10.1109/ICDM.2009.151</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677011</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677012</article_id>
		<sort_key>120</sort_key>
		<display_label>Pages</display_label>
		<pages>1-10</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Explore/Exploit Schemes for Web Content Optimization]]></title>
		<page_from>1</page_from>
		<page_to>10</page_to>
		<doi_number>10.1109/ICDM.2009.52</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677012</url>
		<abstract>
			<par><![CDATA[We propose novel multi-armed bandit (explore/exploit) schemes to maximize total clicks on a content module published regularly on Yahoo! Intuitively, one can ``explore'' each candidate item by displaying it to a small fraction of user visits to estimate the item's click-through rate (CTR), and then ``exploit'' high CTR items in order to maximize clicks. While bandit methods that seek to find the optimal trade-off between explore and exploit have been studied for decades, existing solutions are not satisfactory for web content publishing applications where dynamic set of items with short lifetimes, delayed feedback and non-stationary reward (CTR) distributions are typical. In this paper, we develop a Bayesian solution and extend several existing schemes to our setting. Through extensive evaluation with nine bandit schemes, we show that our Bayesian solution is uniformly better in several scenarios. We also study the empirical characteristics of our schemes and provide useful insights on the strengths and weaknesses of each. Finally, we validate our results with a ``side-by-side'' comparison of schemes through live experiments conducted on a random sample of real user visits to Yahoo!]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[multi-armed bandits, web application, content scheduling, Bayes optimal]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1826783</person_id>
				<author_profile_id><![CDATA[81100632698]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Deepak]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Agarwal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1826784</person_id>
				<author_profile_id><![CDATA[81100108059]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bee-Chung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1826785</person_id>
				<author_profile_id><![CDATA[81418596842]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Pradheep]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Elango]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677013</article_id>
		<sort_key>130</sort_key>
		<display_label>Pages</display_label>
		<pages>11-20</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Connecting Sparsely Distributed Similar Bloggers]]></title>
		<page_from>11</page_from>
		<page_to>20</page_to>
		<doi_number>10.1109/ICDM.2009.38</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677013</url>
		<abstract>
			<par><![CDATA[The nature of the Blogosphere determines that the majority of bloggers are only connected with a small number of fellow bloggers, and similar bloggers can be largely disconnected from each other. Aggregating them allows for cost-effective personalized services, targeted marketing, and exploration of new business opportunities. As most bloggers have only a small number of adjacent bloggers, the problem of aggregating similar bloggers presents challenges that demand novel algorithms of connecting the non-adjacent due to the fragmented distributions of bloggers. In this work, we define the problem, delineate its challenges, and present an approach that uses innovative ways to employ contextual information and collective wisdom to aggregate similar bloggers. A real-world blog directory is used for experiments. We demonstrate the efficacy of our approach, report findings, and discuss related issues and future work.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Blogosphere, similar bloggers, sparse distribution, Long Tail, collective wisdom, clustering, power law, mean average precision (MAP), latent semantic analysis]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1822399</person_id>
				<author_profile_id><![CDATA[81100021279]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nitin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Agarwal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822400</person_id>
				<author_profile_id><![CDATA[81367594306]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Huan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822401</person_id>
				<author_profile_id><![CDATA[81375601817]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shankara]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Subramanya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822402</person_id>
				<author_profile_id><![CDATA[81100299894]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Salerno]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822403</person_id>
				<author_profile_id><![CDATA[81350576309]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Philip]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677014</article_id>
		<sort_key>140</sort_key>
		<display_label>Pages</display_label>
		<pages>21-30</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Rule Ensembles for Multi-target Regression]]></title>
		<page_from>21</page_from>
		<page_to>30</page_to>
		<doi_number>10.1109/ICDM.2009.16</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677014</url>
		<abstract>
			<par><![CDATA[Methods for learning decision rules are being successfully applied to many problem domains, especially where understanding and interpretation of the learned model is necessary. In many real life problems, we would like to predict multiple related (nominal or numeric) target attributes simultaneously. Methods for learning rules that predict multiple targets at once already exist, but are unfortunately based on the covering algorithm, which is not very well suited for regression problems. A better solution for regression problems may be a rule ensemble approach that transcribes an ensemble of decision trees into a large collection of rules. An optimization procedure is then used for selecting the best (and much smaller) subset of these rules, and to determine their weights. Using the rule ensembles approach we have developed a new system for learning rule ensembles for multi-target regression problems. The newly developed method was extensively evaluated and the results show that the accuracy of multi-target regression rule ensembles is better than the accuracy of multi-target regression trees, but somewhat worse than the accuracy of multi-target random forests. The rules are significantly more concise than random forests, and it is also possible to create very small rule sets that are still comparable in accuracy to single regression trees.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Multi-Target Prediction, Rule Learning, Regression]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1829709</person_id>
				<author_profile_id><![CDATA[81387604133]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Timo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1829710</person_id>
				<author_profile_id><![CDATA[81453656556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bernard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[&#142;enko]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1829711</person_id>
				<author_profile_id><![CDATA[81392619100]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sa&#154;o]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[D&#158;eroski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677015</article_id>
		<sort_key>150</sort_key>
		<display_label>Pages</display_label>
		<pages>31-40</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[A Local Scalable Distributed Expectation Maximization Algorithm for Large Peer-to-Peer Networks]]></title>
		<page_from>31</page_from>
		<page_to>40</page_to>
		<doi_number>10.1109/ICDM.2009.45</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677015</url>
		<abstract>
			<par><![CDATA[This paper describes a local and distributed expectation maximization algorithm for learning parameters of Gaussian mixture models (GMM) in large peer-to-peer (P2P) environments. The algorithm can be used for a variety of well-known data mining tasks in distributed environments such as clustering, anomaly detection, target tracking, and density estimation to name a few, necessary for many emerging P2P applications in bioinformatics, webmining and sensor networks. Centralizing all or some of the data to build global models is impractical in such P2P environments because of the large number of data sources, the asynchronous nature of the P2P networks, and dynamic nature of the data/network. The proposed algorithm takes a two-step approach. In the monitoring phase, the algorithm checks if the model &#8216;quality&#8217; is acceptable by using an efficient local algorithm. This is then used as a feedback loop to sample data from the network and rebuild the GMM when it is outdated. We present thorough experimental results to verify our theoretical claims.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[peer-to-peer, local algorithms, expectation maximization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1826786</person_id>
				<author_profile_id><![CDATA[81317491726]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kanishka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bhaduri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1826787</person_id>
				<author_profile_id><![CDATA[81436597130]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ashok]]></first_name>
				<middle_name><![CDATA[N.]]></middle_name>
				<last_name><![CDATA[Srivastava]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677021</article_id>
		<sort_key>160</sort_key>
		<display_label>Pages</display_label>
		<pages>41-50</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Cross-Guided Clustering]]></title>
		<subtitle><![CDATA[Transfer of Relevant Supervision across Domains for Improved Clustering]]></subtitle>
		<page_from>41</page_from>
		<page_to>50</page_to>
		<doi_number>10.1109/ICDM.2009.33</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677021</url>
		<abstract>
			<par><![CDATA[Lack of supervision in clustering algorithms often leads to clusters that are not useful or interesting to human reviewers. We investigate if supervision can be automatically transferred to a clustering task in a target domain, by providing a relevant supervised partitioning of a dataset from a different source domain. The target clustering is made more meaningful for the human user by trading off intrinsic clustering goodness on the target dataset for alignment with relevant supervised partitions in the source dataset, wherever possible. We propose a cross-guided clustering algorithm that builds on traditional k-means by aligning the target clusters with source partitions. The alignment process makes use of a cross-domain similarity measure that discovers hidden relationships across domains with potentially different vocabularies. Using multiple real-world datasets, we show that our approach improves clustering accuracy significantly over traditional k-means.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Clustering methods, Transfer Learning, Relationship Discovery]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1825335</person_id>
				<author_profile_id><![CDATA[81100376279]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Indrajit]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bhattacharya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825336</person_id>
				<author_profile_id><![CDATA[81336489465]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shantanu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Godbole]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825337</person_id>
				<author_profile_id><![CDATA[81453614673]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sachindra]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Joshi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825338</person_id>
				<author_profile_id><![CDATA[81335498908]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ashish]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Verma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677022</article_id>
		<sort_key>170</sort_key>
		<display_label>Pages</display_label>
		<pages>51-60</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Audio Classification of Bird Species]]></title>
		<subtitle><![CDATA[A Statistical Manifold Approach]]></subtitle>
		<page_from>51</page_from>
		<page_to>60</page_to>
		<doi_number>10.1109/ICDM.2009.65</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677022</url>
		<abstract>
			<par><![CDATA[Our goal is to automatically identify which species of bird is present in an audio recording using supervised learning. Devising effective algorithms for bird species classification is a preliminary step toward extracting useful ecological data from recordings collected in the field. We propose a probabilistic model for audio features within a short interval of time, then derive its Bayes risk-minimizing classifier, and show that it is closely approximated by a nearest-neighbor classifier using Kullback-Leibler divergence to compare histograms of features. We note that feature histograms can be viewed as points on a statistical manifold, and KL divergence approximates geodesic distances defined by the Fisher information metric on such manifolds. Motivated by this fact, we propose the use of another approximation to the Fisher information metric, namely the Hellinger metric. The proposed classifiers achieve over 90% accuracy on a data set containing six species of bird, and outperform support vector machines.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[audio, classification, bayes, maximum a-posteriori, map, manifold, geodesic, codebook, nearest neighbor, clustering, mfccs]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1829753</person_id>
				<author_profile_id><![CDATA[81453629345]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Forrest]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Briggs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1829754</person_id>
				<author_profile_id><![CDATA[81416606228]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Raviv]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1829755</person_id>
				<author_profile_id><![CDATA[81100360432]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Xiaoli]]></first_name>
				<middle_name><![CDATA[Z.]]></middle_name>
				<last_name><![CDATA[Fern]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677023</article_id>
		<sort_key>180</sort_key>
		<display_label>Pages</display_label>
		<pages>61-70</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Finding Associations and Computing Similarity via Biased Pair Sampling]]></title>
		<page_from>61</page_from>
		<page_to>70</page_to>
		<doi_number>10.1109/ICDM.2009.35</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677023</url>
		<abstract>
			<par><![CDATA[Sampling-based methods have previously been proposed for the problem of finding interesting associations in data, even for low-support items. While these methods do not guarantee precise results, they can be vastly more efficient than approaches that rely on exact counting. However, for many similarity measures no such methods have been known. In this paper we show how a wide variety of measures can be supported by a simple biased sampling method. The method also extends to find high-confidence association rules. We demonstrate theoretically that our method is superior to exact methods when the threshold for "interesting similarity/confidence" is above the average pairwise similarity/confidence, and the average support is not too low. Our method is particularly good when transactions contain many items. We confirm in experiments on standard association mining benchmarks that this gives a significant speedup on real data sets (sometimes much larger than the theoretical guarantees). Reductions in computation time of over an order of magnitude, and significant savings in space, are observed.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[algorithms, sampling, data mining, association rules]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1823883</person_id>
				<author_profile_id><![CDATA[81453661380]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrea]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Campagna]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1823884</person_id>
				<author_profile_id><![CDATA[81100651427]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Rasmus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pagh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677024</article_id>
		<sort_key>190</sort_key>
		<display_label>Pages</display_label>
		<pages>71-80</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Beyond Banditron]]></title>
		<subtitle><![CDATA[A Conservative and Efficient Reduction for Online Multiclass Prediction with Bandit Setting Model]]></subtitle>
		<page_from>71</page_from>
		<page_to>80</page_to>
		<doi_number>10.1109/ICDM.2009.36</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677024</url>
		<abstract>
			<par><![CDATA[In this paper, we consider a recently proposed supervised learning problem, called online multiclass prediction with bandit setting model. Aiming at learning from partial feedback of online classification results, i.e. &#8220;true&#8221; when the predicting label is right or &#8220;false&#8221; when the predicting label is wrong, this new kind of problems arouses much of researchers&#8217; interest due to its close relations to real world internet applications and human cognitive procedure. While some algorithms have been brought forward, we propose a novel algorithm to deal with such problems. First, we reduce the multiclass prediction problem to binary based on Conservative one-versus-all others Reduction scheme; Then Online Passive-Aggressive Algorithm is embedded as binary learning algorithm to solve the reduced problem. Also we derive a pleasing cumulative mistake bound for our algorithm and a time complexity bound linear to the sample size. Further experimental evaluation on several real world multiclass datasets including RCV1, MNIST, 20 Newsgroups and USPS shows that our method outperforms the existing algorithms with a great improvement.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[online multiclass prediction, bandit setting model, one versus all reduction, passive-aggressive algorithm]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1820891</person_id>
				<author_profile_id><![CDATA[81453629742]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Guangyun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1820892</person_id>
				<author_profile_id><![CDATA[81418592516]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1820893</person_id>
				<author_profile_id><![CDATA[81453614371]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jianwen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1820894</person_id>
				<author_profile_id><![CDATA[81453651723]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shuo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1820895</person_id>
				<author_profile_id><![CDATA[81453613235]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Changshui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677025</article_id>
		<sort_key>200</sort_key>
		<display_label>Pages</display_label>
		<pages>81-90</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[Probabilistic Similarity Query on Dimension Incomplete Data]]></title>
		<page_from>81</page_from>
		<page_to>90</page_to>
		<doi_number>10.1109/ICDM.2009.72</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677025</url>
		<abstract>
			<par><![CDATA[Retrieving similar data has drawn many research efforts in the literature due to its importance in data mining, database and information retrieval. This problem is challenging when the data is incomplete. In previous research, data incompleteness refers to the fact that data values for some dimensions are unknown. However, in many practical applications (e.g., data collection by sensor network under bad environment), not only data values but even data dimension information may also be missing, which will make most similarity query algorithms infeasible. In this work, we propose the novel similarity query problem on dimension incomplete data and adopt a probabilistic framework to model this problem. For this problem, users can give a distance threshold and a probability threshold to specify their retrieval requirements. The distance threshold is used to specify the allowed distance between query and data objects and the probability threshold is used to require that the retrieval results satisfy the distance condition at least with the given probability. Instead of enumerating all possible cases to recover the missed dimensions, we propose an efficient approach to speed up the retrieval process by leveraging the inherent relations between query and dimension incomplete data objects. During the query process, we estimate the lower/upper bounds of the probability that the query is satisfied by a given data object, and utilize these bounds to filter irrelevant data objects efficiently. Furthermore, a probability triangle inequality is proposed to further speed up query processing. According to our experiments on real data sets, the proposed similarity query method is verified to be effective and efficient on dimension incomplete data.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1828272</person_id>
				<author_profile_id><![CDATA[81453646816]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cheng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828273</person_id>
				<author_profile_id><![CDATA[81547418456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Xiaoming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828274</person_id>
				<author_profile_id><![CDATA[81453621246]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jian-Tao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677032</article_id>
		<sort_key>210</sort_key>
		<display_label>Pages</display_label>
		<pages>91-100</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[flowNet]]></title>
		<subtitle><![CDATA[Flow-Based Approach for Efficient Analysis of Complex Biological Networks]]></subtitle>
		<page_from>91</page_from>
		<page_to>100</page_to>
		<doi_number>10.1109/ICDM.2009.39</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677032</url>
		<abstract>
			<par><![CDATA[Biological networks having complex connectivity have been widely studied recently. By characterizing their inherent and structural behaviors in a topological perspective, these studies have attempted to discover hidden knowledge in the systems. However, even though various algorithms with graph-theoretical modeling have provided fundamentals in the network analysis, the availability of practical approaches to efficiently handle the complexity has been limited. In this paper, we present a novel flow-based approach, called flowNet, to efficiently analyze large-sized, complex networks. Our approach is based on the functional influence model that quantifies the influence of a biological component on another. We introduce a dynamic flow simulation algorithm to generate a flow pattern which is a unique characteristic for each component. The set of patterns can be used in identifying functional modules (i.e., clustering). The proposed flow simulation algorithm runs very efficiently in sparse networks. Since our approach uses a weighted network as an input, we also discuss supervised and unsupervised weighting schemes for unweighted biological networks. As experimental results in real applications to the yeast protein interaction network, we demonstrate that our approach outperforms previous graph clustering methods with respect to accuracy.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[flow-based approach, biological networks, graph clustering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1825373</person_id>
				<author_profile_id><![CDATA[81322491009]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Young-Rae]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825374</person_id>
				<author_profile_id><![CDATA[81444597371]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Lei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825375</person_id>
				<author_profile_id><![CDATA[81100115610]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Aidong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677033</article_id>
		<sort_key>220</sort_key>
		<display_label>Pages</display_label>
		<pages>101-109</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[?-Anomica]]></title>
		<subtitle><![CDATA[A Fast Support Vector Based Novelty Detection Technique]]></subtitle>
		<page_from>101</page_from>
		<page_to>109</page_to>
		<doi_number>10.1109/ICDM.2009.42</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677033</url>
		<abstract>
			<par><![CDATA[In this paper we propose &#957;-Anomica, a novel anomaly detection technique that can be trained on huge data sets with much reduced running time compared to the benchmark one-class Support Vector Machines algorithm. In &#957;-Anomica, the idea is to train the machine such that it can provide a close approximation to the exact decision plane using fewer training points and without losing much of the generalization performance of the classical approach. We have tested the proposed algorithm on a variety of continuous data sets under different conditions. We show that under all test conditions the developed procedure closely preserves the accuracy of standard one- class Support Vector Machines while reducing both the training time and the test time by 5 &#8722; 20 times.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Anomaly Detection, Support Vector Machines, Kernel, Optimization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1826854</person_id>
				<author_profile_id><![CDATA[81447595098]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Santanu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Das]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1826855</person_id>
				<author_profile_id><![CDATA[81317491726]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kanishka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bhaduri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1826856</person_id>
				<author_profile_id><![CDATA[81100418152]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nikunj]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Oza]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1826857</person_id>
				<author_profile_id><![CDATA[81436597130]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ashok]]></first_name>
				<middle_name><![CDATA[N.]]></middle_name>
				<last_name><![CDATA[Srivastava]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677034</article_id>
		<sort_key>230</sort_key>
		<display_label>Pages</display_label>
		<pages>110-119</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[Temporal Neighborhood Discovery Using Markov Models]]></title>
		<page_from>110</page_from>
		<page_to>119</page_to>
		<doi_number>10.1109/ICDM.2009.26</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677034</url>
		<abstract>
			<par><![CDATA[Temporal data, which is a sequence of data tuples measured at successive time instances, is typically very large. Hence instead of mining the entire data, we are interested in dividing the huge data into several smaller intervals of interest which we call temporal neighborhoods. In this paper we propose an approach to generate temporal neighborhoods through unequal depth discretization. We describe two novel algorithms (a) Similarity based Merging (SMerg) and, (b) Stationary distribution based Merging (StMerg). These algorithms are based on the robust framework of Markov models and the Markov Stationary distribution respectively. We identify temporal neighborhoods with distinct demarcations based on unequal depth discretization of the data. We discuss detailed experimental results in both synthetic and real world data. Specifically we show (i) the efficacy of our approach through precision and recall of labeled bins, (ii) the ground truth validation in real world datasets and, (iii) knowledge discovery in the temporal neighborhoods such as global anomalies. Our results indicate that we are able to identify valuable knowledge based on our ground truth validation from real world traffic data.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Temporal neighborhoods, Discretization, Markov Model, Stationary Distribution]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1825376</person_id>
				<author_profile_id><![CDATA[81453614461]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sandipan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825377</person_id>
				<author_profile_id><![CDATA[81100287451]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Vandana]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Janeja]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825378</person_id>
				<author_profile_id><![CDATA[81100125231]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Aryya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gangopadhyay]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677035</article_id>
		<sort_key>240</sort_key>
		<display_label>Pages</display_label>
		<pages>120-128</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[Active Learning with Generalized Queries]]></title>
		<page_from>120</page_from>
		<page_to>128</page_to>
		<doi_number>10.1109/ICDM.2009.71</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677035</url>
		<abstract>
			<par><![CDATA[Active learning can actively select or construct examples to label to reduce the number of labeled examples needed for building accurate classifiers. However, previous works of active learning can only ask specific queries. For example, to predict osteoarthritis from a patient dataset with 30 attributes, specific queries always contain values of all these 30 attributes, many of which may be irrelevant. A more natural way is to ask "generalized queries" with don't-care attributes, such as "are people over 50 with knee pain likely to have osteoarthritis?" (with only two attributes: age and type of pain). We assume that the oracle (and human experts) can readily answer those generalized queries by returning probabilistic labels. The power of such generalized queries is that one generalized query may be equivalent to many specific ones. However, overly general queries may receive highly uncertain labels from the oracle, and this makes learning difficult. In this paper, we propose a novel active learning algorithm that asks generalized queries. We demonstrate experimentally that our new method asks significantly fewer queries compared with the previous works of active learning. Our method can be readily deployed in real-world tasks where obtaining labeled examples is costly.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[active learning, generalized queries, supervised learning]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1829789</person_id>
				<author_profile_id><![CDATA[81339497252]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Du]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1829790</person_id>
				<author_profile_id><![CDATA[81100159332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Charles]]></first_name>
				<middle_name><![CDATA[X.]]></middle_name>
				<last_name><![CDATA[Ling]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677036</article_id>
		<sort_key>250</sort_key>
		<display_label>Pages</display_label>
		<pages>129-138</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[Conditional Models for Non-smooth Ranking Loss Functions]]></title>
		<page_from>129</page_from>
		<page_to>138</page_to>
		<doi_number>10.1109/ICDM.2009.49</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677036</url>
		<abstract>
			<par><![CDATA[Learning to rank is an important area at the interface of machine learning, information retrieval and Web search. The central challenge in optimizing various measures of ranking loss is that the objectives tend to be non-convex and discontinuous. To make such functions amenable to gradient based optimization procedures one needs to design clever bounds. In recent years, boosting, neural networks, support vector machines, and many other techniques have been applied. However, there is little work on directly modeling a conditional probability Pr(y|x_q) where y is a permutation of the documents to be ranked and x_q represents their feature vectors with respect to a query q. A major reason is that the space of y is huge: n! if n documents must be ranked. We first propose an intuitive and appealing expected loss minimization objective, and give an efficient shortcut to evaluate it despite the huge space of ys. Unfortunately, the optimization is non-convex, so we propose a convex approximation. We give a new, efficient Monte Carlo sampling method to compute the objective and gradient of this approximation, which can then be used in a quasi-Newton optimizer like LBFGS. Extensive experiments with the widely-used LETOR dataset show large ranking accuracy improvements beyond recent and competitive algorithms.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Learning to Rank, Conditional Models, Monte Carlo Sampling]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1822471</person_id>
				<author_profile_id><![CDATA[81453605939]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Avinava]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dubey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822472</person_id>
				<author_profile_id><![CDATA[81453658848]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jinesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Machchhar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822473</person_id>
				<author_profile_id><![CDATA[81100050056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chiranjib]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bhattacharyya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822474</person_id>
				<author_profile_id><![CDATA[81100424876]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Soumen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chakrabarti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677043</article_id>
		<sort_key>260</sort_key>
		<display_label>Pages</display_label>
		<pages>139-148</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[Unsupervised Class Separation of Multivariate Data through Cumulative Variance-Based Ranking]]></title>
		<page_from>139</page_from>
		<page_to>148</page_to>
		<doi_number>10.1109/ICDM.2009.17</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677043</url>
		<abstract>
			<par><![CDATA[This paper introduces a new extension of outlier detection approaches and a new concept, class separation through variance. We show that accumulating information about the outlierness of points in multiple subspaces leads to a ranking in which classes with differing variance naturally tend to separate. Exploiting this leads to a highly effective and efficient unsupervised class separation approach, especially useful in the difficult case of heavily overlapping distributions. Unlike typical outlier detection algorithms, this method can be applied beyond the `rare classes' case with great success. Two novel algorithms that implement this approach are provided. Additionally, experiments show that the novel methods typically outperform other state-of-the-art outlier detection methods on high dimensional data such as Feature Bagging, SOE1, LOF, ORCA and Robust Mahalanobis Distance and competes even with the leading supervised classification methods.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Outlier Detection, Classification, Subspaces]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1822519</person_id>
				<author_profile_id><![CDATA[81100541836]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Foss]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822520</person_id>
				<author_profile_id><![CDATA[81100104421]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Osmar]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Za&#239;ane]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822521</person_id>
				<author_profile_id><![CDATA[81453628514]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sandra]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zilles]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677044</article_id>
		<sort_key>270</sort_key>
		<display_label>Pages</display_label>
		<pages>149-158</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>23</seq_no>
		<title><![CDATA[Execution Anomaly Detection in Distributed Systems through Unstructured Log Analysis]]></title>
		<page_from>149</page_from>
		<page_to>158</page_to>
		<doi_number>10.1109/ICDM.2009.60</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677044</url>
		<abstract>
			<par><![CDATA[Detection of execution anomalies is very important for the maintenance, development, and performance refinement of large scale distributed systems. Execution anomalies include both work flow errors and low performance problems. People often use system logs produced by distributed systems for troubleshooting and problem diagnosis. However, manually inspecting system logs to detect anomalies is unfeasible due to the increasing scale and complexity of distributed systems. Therefore, there is a great demand for automatic anomalies detection techniques based on log analysis. In this paper, we propose an unstructured log analysis technique for anomalies detection. In the technique, we propose a novel algorithm to convert free form text messages in log files to log keys without heavily relying on application specific knowledge. The log keys correspond to the log-print statements in the source code which can provide cues of system execution behavior. After converting log messages to log keys, we learn a Finite State Automaton (FSA) from training log sequences to present the normal work flow for each system component. At the same time, a performance measurement model is learned to characterize the normal execution performance based on the log mes-sages&#8217; timing information. With these learned models, we can automatically detect anomalies in newly input log files. Experiments on Hadoop and SILK show that the technique can effectively detect running anomalies.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[log analysis, distributed system, problem diagnosis, finite state automaton]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1828370</person_id>
				<author_profile_id><![CDATA[81100244364]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Qiang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828371</person_id>
				<author_profile_id><![CDATA[81452598736]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jian-Guang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828372</person_id>
				<author_profile_id><![CDATA[81384593190]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828373</person_id>
				<author_profile_id><![CDATA[81332511783]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jiang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677045</article_id>
		<sort_key>280</sort_key>
		<display_label>Pages</display_label>
		<pages>159-168</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>24</seq_no>
		<title><![CDATA[Learning the Shared Subspace for Multi-task Clustering and Transductive Transfer Classification]]></title>
		<page_from>159</page_from>
		<page_to>168</page_to>
		<doi_number>10.1109/ICDM.2009.32</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677045</url>
		<abstract>
			<par><![CDATA[There are many clustering tasks which are closely related in the real world, e.g. clustering the web pages of different universities. However, existing clustering approaches neglect the underlying relation and treat these clustering tasks either individually or simply together. In this paper, we will study a novel clustering paradigm, namely multi-task clustering, which performs multiple related clustering tasks together and utilizes the relation of these tasks to enhance the clustering performance. We aim to learn a subspace shared by all the tasks, through which the knowledge of the tasks can be transferred to each other. The objective of our approach consists of two parts: (1) Within-task clustering: clustering the data of each task in its input space individually; and (2) Cross-task clustering: simultaneous learning the shared subspace and clustering the data of all the tasks together. We will show that it can be solved by alternating minimization, and its convergence is theoretically guaranteed. Furthermore, we will show that given the labels of one task, our multi-task clustering method can be extended to transductive transfer classification (a.k.a. cross-domain classification, domain adaption). Experiments on several cross-domain text data sets demonstrate that the proposed multi-task clustering outperforms traditional single-task clustering methods greatly. And the transductive transfer classification method is comparable to or even better than several existing transductive transfer classification approaches.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[multi-task clustering, transductive transfer classification, multi-task learning, transfer learning, cross domain classification, domain adaption]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1823976</person_id>
				<author_profile_id><![CDATA[81436598990]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Quanquan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1823977</person_id>
				<author_profile_id><![CDATA[81331508260]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677046</article_id>
		<sort_key>290</sort_key>
		<display_label>Pages</display_label>
		<pages>169-178</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>25</seq_no>
		<title><![CDATA[Accurate Estimation of the Degree Distribution of Private Networks]]></title>
		<page_from>169</page_from>
		<page_to>178</page_to>
		<doi_number>10.1109/ICDM.2009.11</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677046</url>
		<abstract>
			<par><![CDATA[We describe an efficient algorithm for releasing a provably private estimate of the degree distribution of a network. The algorithm satisfies a rigorous property of differential privacy, and is also extremely efficient, running on networks of 100 million nodes in a few seconds. Theoretical analysis shows that the error scales linearly with the number of unique degrees, whereas the error of conventional techniques scales linearly with the number of nodes. We complement the theoretical analysis with a thorough empirical analysis on real and synthetic graphs, showing that the algorithm's variance and bias is low, that the error diminishes as the size of the input graph increases, and that common analyses like fitting a power-law can be carried out very accurately.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[privacy, social networks, privacy-preserving data mining, differential privacy]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1823978</person_id>
				<author_profile_id><![CDATA[81100443455]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hay]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1823979</person_id>
				<author_profile_id><![CDATA[81464653343]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1823980</person_id>
				<author_profile_id><![CDATA[81100660326]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gerome]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miklau]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1823981</person_id>
				<author_profile_id><![CDATA[81100640360]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677047</article_id>
		<sort_key>300</sort_key>
		<display_label>Pages</display_label>
		<pages>179-188</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>26</seq_no>
		<title><![CDATA[A Linear-Time Graph Kernel]]></title>
		<page_from>179</page_from>
		<page_to>188</page_to>
		<doi_number>10.1109/ICDM.2009.30</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677047</url>
		<abstract>
			<par><![CDATA[The design of a good kernel is fundamental for knowledge discovery from graph-structured data. Existing graph kernels exploit only limited information about the graph structures but are still computationally expensive. We propose a novel graph kernel based on the structural characteristics of graphs. The key is to represent node labels as binary arrays and characterize each node using logical operations on the label set of the connected nodes. Our kernel has a linear time complexity with respect to the number of nodes times the average number of neighboring nodes in the given graphs. The experimental result shows that the proposed kernel performs comparable and much faster than a state-of-the-art graph kernel for benchmark data sets and shows high scalability for new applications with large graphs.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1828374</person_id>
				<author_profile_id><![CDATA[81414611778]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shohei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hido]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828375</person_id>
				<author_profile_id><![CDATA[81100105757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hisashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kashima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677054</article_id>
		<sort_key>310</sort_key>
		<display_label>Pages</display_label>
		<pages>189-198</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>27</seq_no>
		<title><![CDATA[GSML]]></title>
		<subtitle><![CDATA[A Unified Framework for Sparse Metric Learning]]></subtitle>
		<page_from>189</page_from>
		<page_to>198</page_to>
		<doi_number>10.1109/ICDM.2009.22</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677054</url>
		<abstract>
			<par><![CDATA[There has been significant recent interest in sparse metric learning (SML) in which we simultaneously learn both a good distance metric and a low-dimensional representation. Unfortunately, the performance of existing sparse metric learning approaches is usually limited because the authors assumed certain problem relaxations or they target the SML objective indirectly. In this paper, we propose a Generalized Sparse Metric Learning method (GSML). This novel framework offers a unified view for understanding many of the popular sparse metric learning algorithms including the Sparse Metric Learning framework proposed, the Large Margin Nearest Neighbor (LMNN), and the D-ranking Vector Machine (D-ranking VM). Moreover, GSML also establishes a close relationship with the Pairwise Support Vector Machine. Furthermore, the proposed framework is capable of extending many current non-sparse metric learning models such as Relevant Vector Machine (RCA) and a state-of-the-art method proposed into their sparse versions. We present the detailed framework, provide theoretical justifications, build various connections with other models, and propose a practical iterative optimization method, making the framework both theoretically important and practically scalable for medium or large datasets. A series of experiments show that the proposed approach can outperform previous methods in terms of both test accuracy and dimension reduction, on six real-world benchmark datasets.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Sparse, Metric Learning, Unified Framework]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1825448</person_id>
				<author_profile_id><![CDATA[81486642402]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kaizhu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825449</person_id>
				<author_profile_id><![CDATA[81452598288]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yiming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ying]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825450</person_id>
				<author_profile_id><![CDATA[81100210522]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Colin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Campbell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677055</article_id>
		<sort_key>320</sort_key>
		<display_label>Pages</display_label>
		<pages>199-208</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>28</seq_no>
		<title><![CDATA[GRAPE]]></title>
		<subtitle><![CDATA[A Graph-Based Framework for Disambiguating People Appearances in Web Search]]></subtitle>
		<page_from>199</page_from>
		<page_to>208</page_to>
		<doi_number>10.1109/ICDM.2009.25</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677055</url>
		<abstract>
			<par><![CDATA[Finding information about people using search engines is one of the most common activities on the Web. However, search engines usually return a long list of Web pages, which may be relevant to many namesakes, especially given the explosive growth of Web data. To address the challenge caused by name ambiguity in Web people search, this paper proposes a novel graph-based framework, GRAPE (abbr. a Graph-based fRamework for disAmbiguating People appEarances in Web search). In GRAPE, people tag information (e.g., people name, organization, and email address) surrounding the queried people name is extracted from the search results, a graph-based unsupervised algorithm is then developed to cluster the extracted tags, where a new method, Cohesion, is introduced to measure the importance of a tag for clustering, and each final cluster of tags represents a unique people entity. Experimental results show that our proposed framework outperforms the state-of-the-art Web people name disambiguation approaches.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[People Name Disambiguation, Named Entity, Tag Extraction, Clustering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1829893</person_id>
				<author_profile_id><![CDATA[81418597011]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lili]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jiang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1829894</person_id>
				<author_profile_id><![CDATA[81451601319]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jianyong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1829895</person_id>
				<author_profile_id><![CDATA[81100329696]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ning]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[An]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1829896</person_id>
				<author_profile_id><![CDATA[81418598692]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shengyuan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1829897</person_id>
				<author_profile_id><![CDATA[81453639159]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1829898</person_id>
				<author_profile_id><![CDATA[81384612868]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Lian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677056</article_id>
		<sort_key>330</sort_key>
		<display_label>Pages</display_label>
		<pages>209-218</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>29</seq_no>
		<title><![CDATA[A Tree-Based Framework for Difference Summarization]]></title>
		<page_from>209</page_from>
		<page_to>218</page_to>
		<doi_number>10.1109/ICDM.2009.68</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677056</url>
		<abstract>
			<par><![CDATA[Understanding the differences between two datasets is a fundamental data mining question and is also ubiquitously important across many real world scientific applications. In this paper, we propose a tree-based framework to provide a parsimonious explanation of the difference between two distributions based on rigorous two-sample statistical test. We develop two efficient approaches. The first one is a dynamic programming approach that finds a minimal number of data subsets that describe the difference between two data sets. The second one is a greedy approach that approximates the dynamic programming approach. We employ the well-known Friedman's MST (minimal spanning tree) statistics for two-sample statistical tests in our summarization tree construction, and develop novel techniques to speedup its computational procedure. We performed a detailed experimental evaluation on both real and synthetic datasets and demonstrated the effectiveness of our tree-summarization approach.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[difference summarization, minimal spanning tree, two-sample test, Friedman-Rafsky test, Chi-square test, Kolmogorov-Smirnov test]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1831452</person_id>
				<author_profile_id><![CDATA[81100054574]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ruoming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831453</person_id>
				<author_profile_id><![CDATA[81540398856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuri]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Breitbart]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831454</person_id>
				<author_profile_id><![CDATA[81453614177]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Rong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677057</article_id>
		<sort_key>340</sort_key>
		<display_label>Pages</display_label>
		<pages>219-228</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>30</seq_no>
		<title><![CDATA[TrBagg]]></title>
		<subtitle><![CDATA[A Simple Transfer Learning Method and its Application to Personalization in Collaborative Tagging]]></subtitle>
		<page_from>219</page_from>
		<page_to>228</page_to>
		<doi_number>10.1109/ICDM.2009.9</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677057</url>
		<abstract>
			<par><![CDATA[The aim of transfer learning is to improve prediction accuracy on a target task by exploiting the training examples for tasks that are related to the target one. Transfer learning has received more attention in recent years, because this technique is considered to be helpful in reducing the cost of labeling. In this paper, we propose a very simple approach to transfer learning: TrBagg, which is the extension of bagging. TrBagg is composed of two stages: Many weak classifiers are first generated as in standard bagging, and these classifiers are then filtered based on their usefulness for the target task. This simplicity makes it easy to work reasonably well without severe tuning of learning parameters. Further, our algorithm equips an algorithmic scheme to avoid negative transfer. We applied TrBagg to personalized tag prediction tasks for social bookmarks Our approach has several convenient characteristics for this task such as adaptation to multiple tasks with low computational cost.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[transfer learning, bagging, ensemble learning, personalization, recommender system, collaborative tagging]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1820977</person_id>
				<author_profile_id><![CDATA[81100394205]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Toshihiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kamishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1820978</person_id>
				<author_profile_id><![CDATA[81314482894]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Masahiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hamasaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1820979</person_id>
				<author_profile_id><![CDATA[81100206468]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shotaro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Akaho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677058</article_id>
		<sort_key>350</sort_key>
		<display_label>Pages</display_label>
		<pages>229-238</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>31</seq_no>
		<title><![CDATA[PEGASUS]]></title>
		<subtitle><![CDATA[A Peta-Scale Graph Mining System Implementation and Observations]]></subtitle>
		<page_from>229</page_from>
		<page_to>238</page_to>
		<doi_number>10.1109/ICDM.2009.14</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677058</url>
		<abstract>
			<par><![CDATA[In this paper, we describe PEGASUS, an open source Peta Graph Mining library which performs typical graph mining tasks such as computing the diameter of the graph, computing the radius of each node and finding the connected components. As the size of graphs reaches several Giga-, Tera- or Peta-bytes, the necessity for such a library grows too. To the best of our knowledge, PEGASUS is the first such library, implemented on the top of the Hadoop platform, the open source version of MapReduce. Many graph mining operations (PageRank, spectral clustering, diameter estimation, connected components etc.) are essentially a repeated matrix-vector multiplication. In this paper we describe a very important primitive for PEGASUS, called GIM-V (Generalized Iterated Matrix-Vector multiplication). GIM-V is highly optimized, achieving (a) good scale-up on the number of available machines (b) linear running time on the number of edges, and (c) more than 5 times faster performance over the non-optimized version of GIM-V. Our experiments ran on M45, one of the top 50 supercomputers in the world. We report our findings on several real graphs, including one of the largest publicly available Web Graphs, thanks to Yahoo!, with 6,7 billion edges.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[PEGASUS, graph mining, hadoop]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1820980</person_id>
				<author_profile_id><![CDATA[81100492259]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[U.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1820981</person_id>
				<author_profile_id><![CDATA[81384619692]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Charalampos]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Tsourakakis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1820982</person_id>
				<author_profile_id><![CDATA[81100373169]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Christos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Faloutsos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677065</article_id>
		<sort_key>360</sort_key>
		<display_label>Pages</display_label>
		<pages>239-248</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>32</seq_no>
		<title><![CDATA[Efficient Discovery of Frequent Correlated Subgraph Pairs]]></title>
		<page_from>239</page_from>
		<page_to>248</page_to>
		<doi_number>10.1109/ICDM.2009.54</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677065</url>
		<abstract>
			<par><![CDATA[The recent proliferation of graph data in a wide spectrum of applications has led to an increasing demand for advanced data analysis techniques. In view of this, many graph mining techniques, such as frequent subgraph mining and correlated subgraph mining, have been proposed. In many applications, both frequency and correlation play an important role. Thus, this paper studies a new problem of mining the set of frequent correlated subgraph pairs. A simple algorithm that combines existing algorithms for mining frequent subgraphs and correlated subgraphs results in a multiplication of the mining operations, the majority of which are redundant. We discover that most of the graphs correlated to a common graph are also highly correlated. We establish theoretical foundations for this finding and derive a tight lower bound on the correlation of any two graphs that are correlated to a common graph. This theoretical result leads to the design of a very effective skipping mechanism, by which we skip the processing of a majority of graphs in the mining process. Our algorithm, FCP-Miner, is a fast approximate algorithm, but we show that the missing pairs are only a small set of marginally correlated pairs. Extensive experiments verify both the efficiency and effectiveness of FCP-Miner.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[graph mining, Pearson's correlation coefficient, frequent correlated subgraph pairs]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1826947</person_id>
				<author_profile_id><![CDATA[81313481134]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yiping]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1826948</person_id>
				<author_profile_id><![CDATA[81408594266]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cheng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1826949</person_id>
				<author_profile_id><![CDATA[81447600785]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jeffrey]]></first_name>
				<middle_name><![CDATA[Xu]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677066</article_id>
		<sort_key>370</sort_key>
		<display_label>Pages</display_label>
		<pages>249-258</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>33</seq_no>
		<title><![CDATA[Self-Adaptive Anytime Stream Clustering]]></title>
		<page_from>249</page_from>
		<page_to>258</page_to>
		<doi_number>10.1109/ICDM.2009.47</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677066</url>
		<abstract>
			<par><![CDATA[Clustering streaming data requires algorithms which are capable of updating clustering results for the incoming data. As data is constantly arriving, time for processing is limited. Clustering has to be performed in a single pass over the incoming data and within the possibly varying inter-arrival times of the stream. Likewise, memory is limited, making it impossible to store all data. For clustering, we are faced with the challenge of maintaining a current result that can be presented to the user at any given time. In this work, we propose a parameter free algorithm that automatically adapts to the speed of the data stream. It makes best use of the time available under the current constraints to provide a clustering of the objects seen up to that point. Our approach incorporates the age of the objects to reflect the greater importance of more recent data. Moreover, we are capable of detecting concept drift, novelty and outliers in the stream. For efficient and effective handling, we introduce the ClusTree, a compact and self-adaptive index structure for maintaining stream summaries. Our experiments show that our approach is capable of handling a multitude of different stream characteristics for accurate and scalable anytime stream clustering.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[stream clustering, anytime algorithms, self-adaptive algorithms]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1824037</person_id>
				<author_profile_id><![CDATA[81351606473]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Philipp]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kranen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824038</person_id>
				<author_profile_id><![CDATA[81100257752]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ira]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Assent]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824039</person_id>
				<author_profile_id><![CDATA[81453655095]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Corinna]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baldauf]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824040</person_id>
				<author_profile_id><![CDATA[81100145971]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seidl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677067</article_id>
		<sort_key>380</sort_key>
		<display_label>Pages</display_label>
		<pages>259-267</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>34</seq_no>
		<title><![CDATA[Improving SVM Classification on Imbalanced Data Sets in Distance Spaces]]></title>
		<page_from>259</page_from>
		<page_to>267</page_to>
		<doi_number>10.1109/ICDM.2009.59</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677067</url>
		<abstract>
			<par><![CDATA[Imbalanced data sets present a particular challenge to the data mining community. Often, it is the rare event that is of interest and the cost of misclassifying the rare event is higher than misclassifying the usual event. When the data is highly skewed toward the usual, it can be very difficult for a learning system to accurately detect the rare event. There have been many approaches in recent years for handling imbalanced data sets, from under-sampling the majority class to adding synthetic points to the minority class in feature space. Distances between time series are known to be non-Euclidean and nonmetric, since comparing time series requires warping in time. This fact makes it impossible to apply standard methods like SMOTE to insert synthetic data points in feature spaces. We present an innovative approach that augments the minority class by adding synthetic points in distance spaces. We then use Support Vector Machines for classification. Our experimental results on standard time series show that our synthetic points significantly improve the classification rate of the rare events, and in many cases also improves the overall accuracy of SVM.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[imbalanced data sets, support vector machines, time series]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1825479</person_id>
				<author_profile_id><![CDATA[81375619314]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Suzan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[K&#246;knar-Tezel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825480</person_id>
				<author_profile_id><![CDATA[81100594681]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Longin]]></first_name>
				<middle_name><![CDATA[Jan]]></middle_name>
				<last_name><![CDATA[Latecki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677068</article_id>
		<sort_key>390</sort_key>
		<display_label>Pages</display_label>
		<pages>268-277</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>35</seq_no>
		<title><![CDATA[CoCoST]]></title>
		<subtitle><![CDATA[A Computational Cost Efficient Classifier]]></subtitle>
		<page_from>268</page_from>
		<page_to>277</page_to>
		<doi_number>10.1109/ICDM.2009.46</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677068</url>
		<abstract>
			<par><![CDATA[Computational cost of classification is as important as accuracy in on-line classification systems. The computational cost is usually dominated by the cost of computing implicit features of the raw input data. Very few efforts have been made to design classifiers which perform effectively with limited computational power; instead, feature selection is usually employed as a pre-processing step to reduce the cost of running traditional classifiers. We present CoCoST, a novel and effective approach for building classifiers which achieve state-of-the-art classification accuracy, while keeping the expected computational cost of classification low, even without feature selection. CoCost employs a wide range of novel cost-aware decision trees, each of which is tuned to specialize in classifying instances from a subset of the input space, and judiciously consults them depending on the input instance in accordance with a cost-aware meta-classifier. Experimental results on a network flow detection application show that, our approach can achieve better accuracy than classifiers such as SVM and random forests, while achieving 75%-90% reduction in the computational costs.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Cost Efficient Decision Tree, Suppressed Cost, Inverse-Boosting, Meta-Classifier]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1826950</person_id>
				<author_profile_id><![CDATA[81453643997]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Liyun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1826951</person_id>
				<author_profile_id><![CDATA[81100283122]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Umut]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Topkara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1826952</person_id>
				<author_profile_id><![CDATA[81453643196]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Baris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Coskun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1826953</person_id>
				<author_profile_id><![CDATA[81100288010]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Nasir]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Memon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677069</article_id>
		<sort_key>400</sort_key>
		<display_label>Pages</display_label>
		<pages>278-287</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>36</seq_no>
		<title><![CDATA[Semi-naive Exploitation of One-Dependence Estimators]]></title>
		<page_from>278</page_from>
		<page_to>287</page_to>
		<doi_number>10.1109/ICDM.2009.64</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677069</url>
		<abstract>
			<par><![CDATA[It is well known that the key of Bayesian classifier learning is to balance the two important issues, that is, the exploration of attribute dependencies in high orders for ensuring a sufficient flexibility in approximating the ground-truth dependencies, and the exploration of low orders for ensuring a stable probability estimate from limited training samples. By allowing one-order attribute dependencies, one-dependence estimators (ODEs) have been shown to be able to approximate the ground-truth attribute dependencies whilst keeping the effectiveness of probability estimation, and therefore leading to excellent performance. In previous studies, however, ODEs were exploited in simple ways, such as by averaging, for classification. In this paper, we propose a semi-naive exploitation of ODEs that fits a function of ODEs to pursue higher-order attribute dependencies. Extensive experiments show that the proposed SNODE approach can achieve better performance than many state-of-the-art Bayesian classifiers.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Bayesian classifier, one-dependence estimator, semi-naive Bayes]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1826954</person_id>
				<author_profile_id><![CDATA[81453605432]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1826955</person_id>
				<author_profile_id><![CDATA[81453637754]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1826956</person_id>
				<author_profile_id><![CDATA[81451593001]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Zhi-Hua]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677075</article_id>
		<sort_key>410</sort_key>
		<display_label>Pages</display_label>
		<pages>288-297</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>37</seq_no>
		<title><![CDATA[A Framework for Computing the Privacy Scores of Users in Online Social Networks]]></title>
		<page_from>288</page_from>
		<page_to>297</page_to>
		<doi_number>10.1109/ICDM.2009.21</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677075</url>
		<abstract>
			<par><![CDATA[A large body of work has been devoted to address corporate-scale privacy concerns related to social networks. The main focus was on how to share social networks owned by organizations without revealing the identities or sensitive relationships of the users involved. Not much attention has been given to the privacy risk of users posed by their information sharing activities. In this paper, we approach the privacy concerns arising in online social networks from the individual users&#8217; viewpoint: we propose a framework to compute a privacy score of a user, which indicates the potential privacy risk caused by his participation in the network. Our definition of privacy score satisfies the following intuitive properties: the more sensitive the information revealed by a user, the higher his privacy risk. Also, the more visible the disclosed information becomes in the network, the higher the privacy risk. We develop mathematical models to estimate both sensitivity and visibility of the information. We apply our methods to synthetic and real-world data and demonstrate their efficacy and practical utility.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[privacy score, social network, item response theory]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1831521</person_id>
				<author_profile_id><![CDATA[81452593047]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831522</person_id>
				<author_profile_id><![CDATA[81100469792]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Evimaria]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Terzi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677076</article_id>
		<sort_key>420</sort_key>
		<display_label>Pages</display_label>
		<pages>298-306</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>38</seq_no>
		<title><![CDATA[Least Square Incremental Linear Discriminant Analysis]]></title>
		<page_from>298</page_from>
		<page_to>306</page_to>
		<doi_number>10.1109/ICDM.2009.78</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677076</url>
		<abstract>
			<par><![CDATA[Linear discriminant analysis (LDA) is a well-known dimension reduction approach, which projects high-dimensional data into a low-dimensional space with the best separation of different classes. In many tasks, the data accumulates over time, and thus incremental LDA is more desirable than batch LDA. Several incremental LDA algorithms have been developed and achieved success; however, the eigen-problem involved requires a large computation cost, which hampers the efficiency of these algorithms. In this paper, we propose a new incremental LDA algorithm, LS-ILDA, based on the least square solution of LDA. When new samples are received, LS-ILDA incrementally updates the least square solution of LDA. Our analysis discloses that this algorithm produces the exact least square solution of batch LDA, while its computational cost is O(min(n; d)  d) for one update on dataset containing n instances in d-dimensional space. Experimental results show that comparing with state-of-the-art incremental LDA algorithms, our proposed LS-ILDA achieves high accuracy with low time cost.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Dimension reduction, linear discriminant analysis (LDA), incremental learning, least square]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1828498</person_id>
				<author_profile_id><![CDATA[81453628857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Li-Ping]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828499</person_id>
				<author_profile_id><![CDATA[81414616861]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jiang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828500</person_id>
				<author_profile_id><![CDATA[81451593001]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Zhi-Hua]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677077</article_id>
		<sort_key>430</sort_key>
		<display_label>Pages</display_label>
		<pages>307-316</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>39</seq_no>
		<title><![CDATA[Unified Solution to Nonnegative Data Factorization Problems]]></title>
		<page_from>307</page_from>
		<page_to>316</page_to>
		<doi_number>10.1109/ICDM.2009.18</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677077</url>
		<abstract>
			<par><![CDATA[In this paper, we restudy the non-convex data factorization problems (regularized or not, unsupervised or supervised), where the optimization is confined in the \emph{nonnegative} orthant, and provide a \emph{unified} convergency provable solution based on multiplicative nonnegative update rules. This solution is general for optimization problems with block-wisely quadratic objective functions, and thus direct update rules can be derived by skipping over the tedious specific procedure deduction process and algorithmic convergence proof. By taking this unified solution as a general template, we i) re-explain several existing nonnegative data factorization algorithms, ii) develop a variant of nonnegative matrix factorization formulation for handling out-of-sample data, and iii) propose a new nonnegative data factorization algorithm, called Correlated Co-Decomposition (CCD), to simultaneously factorize two feature spaces by exploring the inter-correlated information. Experiments on both face recognition and multi-label image annotation tasks demonstrate the wide applicability of the unified solution as well as the effectiveness of two proposed new algorithms.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1825508</person_id>
				<author_profile_id><![CDATA[81467648530]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xiaobai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825509</person_id>
				<author_profile_id><![CDATA[81100044797]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shuicheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825510</person_id>
				<author_profile_id><![CDATA[81100045080]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825511</person_id>
				<author_profile_id><![CDATA[81100055098]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677078</article_id>
		<sort_key>440</sort_key>
		<display_label>Pages</display_label>
		<pages>317-326</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>40</seq_no>
		<title><![CDATA[Extended Boolean Matrix Decomposition]]></title>
		<page_from>317</page_from>
		<page_to>326</page_to>
		<doi_number>10.1109/ICDM.2009.61</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677078</url>
		<abstract>
			<par><![CDATA[With the vast increase in collection and storage of data, the problem of data summarization is most critical for effective data management. Since much of this data is categorical in nature, it can be viewed in terms of a Boolean matrix. Boolean matrix decomposition (BMD) has been used to provide concise and interpretable representations of Boolean data sets. A Boolean matrix can be expressed as a product of two Boolean matrices, where the first matrix represents a set of meaningful concepts, and the second describes how the observed data can be expressed as combinations of those concepts. Typically, the combination is only in terms of the set union. In other words, a successful Boolean matrix decomposition gives a set of concepts and shows how every column of the input data can be expressed as a union of some subset of those concepts. However, this way of modeling only incompletely represents real data semantics. Essentially, it ignores a critical component -- the set difference operation: a column can be expressed as the combination of union of certain concepts as well as the exclusion of other concepts. This has two significant benefits. First, the total number of concepts required to describe the data may itself be reduced. Second, a more succinct summarization may be found for every column. In this paper, we propose the extended Boolean matrix decomposition (EBMD) problem, which aims to factor Boolean matrices using both the set union and set difference operations. We study several variants of the problem, show that they are NP-hard, and propose efficient heuristics to solve them. Extensive experimental results demonstrate the power of EBMD.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1831523</person_id>
				<author_profile_id><![CDATA[81384618332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Haibing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831524</person_id>
				<author_profile_id><![CDATA[81100496660]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jaideep]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vaidya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831525</person_id>
				<author_profile_id><![CDATA[81100274477]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Vijayalakshmi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Atluri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831526</person_id>
				<author_profile_id><![CDATA[81447593801]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yuan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677079</article_id>
		<sort_key>450</sort_key>
		<display_label>Pages</display_label>
		<pages>327-336</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>41</seq_no>
		<title><![CDATA[Active Learning with Adaptive Heterogeneous Ensembles]]></title>
		<page_from>327</page_from>
		<page_to>336</page_to>
		<doi_number>10.1109/ICDM.2009.63</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677079</url>
		<abstract>
			<par><![CDATA[One common approach to active learning is to iteratively train a single classifier by choosing data points based on its uncertainty, but it is nontrivial to design uncertainty measures unbiased by the choice of classifier. Query by committee suggests that given an ensemble of diverse but accurate classifiers, the most informative data points are those that cause maximal disagreement among the predictions of the ensemble members. However the method for finding ensembles appropriate to a given data set remains an open question. In this paper, the random subspace method is combined with active learning to create multiple instances of different classifier types, and an algorithm is introduced that adapts the ratio of different classifier types in the ensemble towards better overall accuracy. Here we show that the proposed algorithm outperforms C4.5 with uncertainty sampling, Naive Bayes with uncertainty sampling, bagging, boosting and the random subspace method with random sampling. To the best of our knowledge, our work is the first to adapt the ratio of classifiers in a heterogeneous ensemble for active learning.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Active Learning, Ensemble Learning, Adaptive Heterogeneous Ensembles]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1829973</person_id>
				<author_profile_id><![CDATA[81363593677]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Zhenyu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1829974</person_id>
				<author_profile_id><![CDATA[81452596585]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Xindong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1829975</person_id>
				<author_profile_id><![CDATA[81100163056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Josh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bongard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677086</article_id>
		<sort_key>460</sort_key>
		<display_label>Pages</display_label>
		<pages>337-346</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>42</seq_no>
		<title><![CDATA[Non-negative Laplacian Embedding]]></title>
		<page_from>337</page_from>
		<page_to>346</page_to>
		<doi_number>10.1109/ICDM.2009.74</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677086</url>
		<abstract>
			<par><![CDATA[Laplacian embedding provides a low dimensional representation for a matrix of pairwise similarity data using the eigenvectors of the Laplacian matrix. The true power of Laplacian embedding is that it provides an approximation of the Ratio Cut clustering. However, Ratio Cut clustering requires the solution to be {\it nonnegative}. In this paper, we propose a new approach, nonnegative Laplacian embedding, which approximates Ratio Cut clustering in a more direct way than traditional approaches. From the solution of our approach, clustering structures can be read off directly. We also propose an efficient algorithm to optimize the objective function utilized in our approach. Empirical studies on many real world datasets show that our approach leads to more accurate Ratio Cut solution and improves clustering accuracy at the same time.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Laplacian Embedding, Non-negative Matrix Factorization, Clustering, Dimension reduction]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1828547</person_id>
				<author_profile_id><![CDATA[81363601495]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dijun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Luo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828548</person_id>
				<author_profile_id><![CDATA[81100136610]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ding]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828549</person_id>
				<author_profile_id><![CDATA[81336489675]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Heng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828550</person_id>
				<author_profile_id><![CDATA[81453626689]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677087</article_id>
		<sort_key>470</sort_key>
		<display_label>Pages</display_label>
		<pages>347-356</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>43</seq_no>
		<title><![CDATA[Scalable Algorithms for Distribution Search]]></title>
		<page_from>347</page_from>
		<page_to>356</page_to>
		<doi_number>10.1109/ICDM.2009.51</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677087</url>
		<abstract>
			<par><![CDATA[Distribution data naturally arise in countless domains, such as meteorology, biology, geology, industry and economics. However, relatively little attention has been paid to data mining for large distribution sets. Given n distributions of multiple categories and a query distribution Q, we want to find similar clouds (i.e., distributions), to discover patterns, rules and outlier clouds. For example, consider the numerical case of sales of items, where, for each item sold, we record the unit price and quantity; then, each customer is represented as a distribution of 2-d points (one for each item he/she bought). We want to find similar users, e.g., for market segmentation, anomaly/fraud detection. We propose to address this problem and present D-Search, which includes fast and effective algorithms for similarity search in large distribution datasets. Our main contributions are (1) approximate KL divergence, which can speed up cloud-similarity computations, (2) multi-step sequential scan, which efficiently prunes a significant number of search candidates and leads to a direct reduction in the search cost. We also introduce an extended version of D-Search: (3) time-series distribution mining, which finds similar subsequences in time-series distribution datasets. Extensive experiments on real multi-dimensional datasets show that our solution achieves up to 2,300 faster wall-clock time over the naive implementation while it does not sacrifice accuracy.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Distribution sets, KL divergence, Singular value decomposition]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1825548</person_id>
				<author_profile_id><![CDATA[81365594947]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yasuko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matsubara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825549</person_id>
				<author_profile_id><![CDATA[81452606482]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yasushi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sakurai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825550</person_id>
				<author_profile_id><![CDATA[81100409342]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Masatoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677088</article_id>
		<sort_key>480</sort_key>
		<display_label>Pages</display_label>
		<pages>357-366</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>44</seq_no>
		<title><![CDATA[A Deep Non-linear Feature Mapping for Large-Margin kNN Classification]]></title>
		<page_from>357</page_from>
		<page_to>366</page_to>
		<doi_number>10.1109/ICDM.2009.27</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677088</url>
		<abstract>
			<par><![CDATA[KNN is one of the most popular data mining methods for classification, but it often fails to work well with inappropriate choice of distance metric or due to the presence of numerous class-irrelevant features. Linear feature transformation methods have been widely applied to extract class-relevant information to improve kNN classification, which is very limited in many applications. Kernels have also been used to learn powerful non-linear feature transformations, but these methods fail to scale to large datasets. In this paper, we present a scalable non-linear feature mapping method based on a deep neural network pretrained with Restricted Boltzmann Machines for improving kNN classification in a large-margin framework, which we call DNet-kNN. DNet-kNN can be used for both classification and for supervised dimensionality reduction. The experimental results on two benchmark handwritten digit datasets and one newsgroup text dataset show that DNet-kNN has much better performance than large-margin kNN using a linear mapping and kNN based on a deep autoencoder pretrained with Restricted Boltzmann Machines.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[kNN Classification, Large Margin, Non-linear Feature Mapping, Non-linear Dimensionality Reduction, Deep Neural Networks, RBM, Deep Learning]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1825551</person_id>
				<author_profile_id><![CDATA[81361607095]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Renqiang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Min]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825552</person_id>
				<author_profile_id><![CDATA[81453631036]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Stanley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825553</person_id>
				<author_profile_id><![CDATA[81453612157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Zineng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yuan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825554</person_id>
				<author_profile_id><![CDATA[81544657456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Anthony]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bonner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825555</person_id>
				<author_profile_id><![CDATA[81361590909]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Zhaolei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677089</article_id>
		<sort_key>490</sort_key>
		<display_label>Pages</display_label>
		<pages>367-376</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>45</seq_no>
		<title><![CDATA[Finding Time Series Motifs in Disk-Resident Data]]></title>
		<page_from>367</page_from>
		<page_to>376</page_to>
		<doi_number>10.1109/ICDM.2009.15</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677089</url>
		<abstract>
			<par><![CDATA[Time series motifs are sets of very similar subsequences of a long time series. They are of interest in their own right, and are also used as inputs in several higher-level data mining algorithms including classification, clustering, rule-discovery and summarization. In spite of extensive research in recent years, finding exact time series motifs in massive databases is an open problem. Previous efforts either found approximate motifs or considered relatively small datasets residing in main memory. In this work, we describe for the first time a disk-aware algorithm to find exact time series motifs in multi-gigabyte databases which contain on the order of tens of millions of time series. We have evaluated our algorithm on datasets from diverse areas including medicine, anthropology, computer networking and image processing and show that we can find interesting and meaningful motifs in datasets that are many orders of magnitude larger than anything considered before.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[time series motif, exact algorithm, closest pair]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1824140</person_id>
				<author_profile_id><![CDATA[81453650813]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Abdullah]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mueen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824141</person_id>
				<author_profile_id><![CDATA[81100209161]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eamonn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Keogh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824142</person_id>
				<author_profile_id><![CDATA[81442619440]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nima]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bigdely-Shamlo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677090</article_id>
		<sort_key>500</sort_key>
		<display_label>Pages</display_label>
		<pages>377-386</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>46</seq_no>
		<title><![CDATA[Relevant Subspace Clustering]]></title>
		<subtitle><![CDATA[Mining the Most Interesting Non-redundant Concepts in High Dimensional Data]]></subtitle>
		<page_from>377</page_from>
		<page_to>386</page_to>
		<doi_number>10.1109/ICDM.2009.10</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677090</url>
		<abstract>
			<par><![CDATA[Subspace clustering aims at detecting clusters in any subspace projection of a high dimensional space. As the number of possible subspace projections is exponential in the number of dimensions, the result is often tremendously large. Recent approaches fail to reduce results to relevant subspace clusters. Their results are typically highly redundant, i.e. many clusters are detected multiple times in several projections. In this work, we propose a novel model for relevant subspace clustering (RESCU). We present a global optimization which detects the most interesting non-redundant subspace clusters. We prove that computation of this model is NP-hard. For RESCU, we propose an approximative solution that shows high accuracy with respect to our relevance model. Thorough experiments on synthetic and real world data show that RESCU successfully reduces the result to manageable sizes. It reliably achieves top clustering quality while competing approaches show greatly varying performance.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[data mining, high dimensional data, subspace clustering, redundancy removal, global optimization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1828551</person_id>
				<author_profile_id><![CDATA[81350600194]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Emmanuel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[M&#252;ller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828552</person_id>
				<author_profile_id><![CDATA[81100257752]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ira]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Assent]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828553</person_id>
				<author_profile_id><![CDATA[81447604694]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Stephan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[G&#252;nnemann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828554</person_id>
				<author_profile_id><![CDATA[81331496997]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ralph]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Krieger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828555</person_id>
				<author_profile_id><![CDATA[81100145971]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seidl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677097</article_id>
		<sort_key>510</sort_key>
		<display_label>Pages</display_label>
		<pages>387-396</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>47</seq_no>
		<title><![CDATA[Stacked Gaussian Process Learning]]></title>
		<page_from>387</page_from>
		<page_to>396</page_to>
		<doi_number>10.1109/ICDM.2009.56</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677097</url>
		<abstract>
			<par><![CDATA[Triggered by a market relevant application that involves making joint predictions of pedestrian and public transit flows in urban areas, we address the question of how to utilize hidden common cause relations among variables of interest in order to improve performance in the two related regression tasks. Specifically, we propose stacked Gaussian process learning, a meta-learning scheme in which a base Gaussian process is enhanced by adding the posterior covariance functions of other related tasks to its covariance function in a stage-wise optimization. The idea is that the stacked posterior covariances encode the hidden common causes among variables of interest that are shared across the related regression tasks. Stacked Gaussian process learning is efficient, capable of capturing shared common causes, and can be implemented with any kind of standard Gaussian process regression model such as sparse approximations and relational variants. Our experimental results on real-world data from the market relevant application show that stacked Gaussian processes learning can significantly improve prediction performance of a standard Gaussian process.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Statistical Relational Learning, Gaussian Processes, Bayesian Regression, Stacked Learning]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1827082</person_id>
				<author_profile_id><![CDATA[81453624760]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marion]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Neumann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1827083</person_id>
				<author_profile_id><![CDATA[81337490510]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kristian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kersting]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1827084</person_id>
				<author_profile_id><![CDATA[81430657628]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Zhao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1827085</person_id>
				<author_profile_id><![CDATA[81100314488]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schulz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677098</article_id>
		<sort_key>520</sort_key>
		<display_label>Pages</display_label>
		<pages>397-406</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>48</seq_no>
		<title><![CDATA[Evaluating Statistical Tests for Within-Network Classifiers of Relational Data]]></title>
		<page_from>397</page_from>
		<page_to>406</page_to>
		<doi_number>10.1109/ICDM.2009.50</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677098</url>
		<abstract>
			<par><![CDATA[Recently a number of modeling techniques have been developed for data mining and machine learning in relational and network domains where the instances are not independent and identically distributed (i.i.d.). These methods specifically exploit the statistical dependencies among instances in order to improve classification accuracy. However, there has been little focus on how these same dependencies affect our ability to draw accurate conclusions about the performance of the models. More specifically, the complex link structure and attribute dependencies in network data violate the assumptions of many conventional statistical tests and make it difficult to use these tests to assess the models in an unbiased manner. In this work, we examine the task of within-network classification and the question of whether two algorithms will learn models which will result in significantly different levels of performance. We show that the commonly-used form of evaluation (paired t-test on overlapping network samples) can result in an unacceptable level of Type I error. Furthermore we show that Type I error increases as (1) the correlation among instances increases and (2) the size of the evaluation set increases (i.e., the proportion of labeled nodes in the network decreases). We propose a method for network cross-validation that combined with paired t-tests produces more acceptable levels of Type I error while still providing reasonable levels of statistical power (i.e., Type II error).]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1831585</person_id>
				<author_profile_id><![CDATA[81100563777]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jennifer]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Neville]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831586</person_id>
				<author_profile_id><![CDATA[81335490719]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gallagher]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831587</person_id>
				<author_profile_id><![CDATA[81100597717]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eliassi-Rad]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677099</article_id>
		<sort_key>530</sort_key>
		<display_label>Pages</display_label>
		<pages>407-416</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>49</seq_no>
		<title><![CDATA[Discovering Excitatory Networks from Discrete Event Streams with Applications to Neuronal Spike Train Analysis]]></title>
		<page_from>407</page_from>
		<page_to>416</page_to>
		<doi_number>10.1109/ICDM.2009.73</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677099</url>
		<abstract>
			<par><![CDATA[Mining temporal network models from discrete event streams is an important problem with applications in computational neuroscience, physical plant diagnostics, and human-computer interaction modeling. We focus in this paper on temporal models representable as excitatory networks where all connections are stimulative, rather than inhibitory. Through this emphasis on excitatory networks, we show how they can be learned by creating bridges to frequent episode mining. Specifically, we show that frequent episodes help identify nodes with high mutual information relationships and which can be summarized into a dynamic Bayesian network (DBN). To demonstrate the practical feasibility of our approach, we show how excitatory networks can be inferred from both mathematical models of spiking neurons as well as real neuroscience datasets.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Frequent Episodes, Dynamic Bayesian Network, Computational Neuroscience, Spike train analysis, Temporal Data Mining]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1827086</person_id>
				<author_profile_id><![CDATA[81436598063]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Debprakash]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Patnaik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1827087</person_id>
				<author_profile_id><![CDATA[81100551630]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Srivatsan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Laxman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1827088</person_id>
				<author_profile_id><![CDATA[81350601212]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Naren]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ramakrishnan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677100</article_id>
		<sort_key>540</sort_key>
		<display_label>Pages</display_label>
		<pages>417-427</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>50</seq_no>
		<title><![CDATA[Clustering Trajectories of Moving Objects in an Uncertain World]]></title>
		<page_from>417</page_from>
		<page_to>427</page_to>
		<doi_number>10.1109/ICDM.2009.57</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677100</url>
		<abstract>
			<par><![CDATA[Mining Trajectory Databases (TD) has recently gained great interest due to the popularity of tracking devices. On the other hand, the inherent presence of uncertainty in TD (e.g., due to GPS errors) has not been taken yet into account during the mining process. In this paper, we study the effect of uncertainty in TD clustering and introduce a three-step approach to deal with it. First, we propose an intuitionistic point vector representation of trajectories that encompasses the underlying uncertainty and introduce an effective distance metric to cope with uncertainty. Second, we devise CenTra, a novel algorithm which tackles the problem of discovering the Centroid Trajectory of a group of movements. Third, we propose a variant of the Fuzzy C-Means (FCM) clustering algorithm, which embodies CenTra at its update procedure. The experimental evaluation over real world TD demonstrates the efficiency and effectiveness of our approach.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1822713</person_id>
				<author_profile_id><![CDATA[81100046429]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nikos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pelekis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822714</person_id>
				<author_profile_id><![CDATA[81100574680]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ioannis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kopanakis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822715</person_id>
				<author_profile_id><![CDATA[81367594659]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Evangelos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kotsifakos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822716</person_id>
				<author_profile_id><![CDATA[81351605915]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Elias]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Frentzos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822717</person_id>
				<author_profile_id><![CDATA[81100535035]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Yannis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Theodoridis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677101</article_id>
		<sort_key>550</sort_key>
		<display_label>Pages</display_label>
		<pages>428-437</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>51</seq_no>
		<title><![CDATA[Semi-Supervised Sequence Labeling with Self-Learned Features]]></title>
		<page_from>428</page_from>
		<page_to>437</page_to>
		<doi_number>10.1109/ICDM.2009.40</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677101</url>
		<abstract>
			<par><![CDATA[Typical information extraction (IE) systems can be seen as tasks assigning labels to words in a natural language sequence. The performance is restricted by the availability of labeled words. To tackle this issue, we propose a semi-supervised approach to improve the sequence labeling procedure in IE through a class of algorithms with {\em self-learned features} (SLF). A supervised classifier can be trained with annotated text sequences and used to classify each word in a large set of unannotated sentences. By averaging predicted labels over all cases in the unlabeled corpus, SLF training builds class label distribution patterns for each word (or word attribute) in the dictionary and re-trains the current model iteratively adding these distributions as extra word {\em features}. Basic SLF models how likely a word could be assigned to target class types. Several extensions are proposed, such as learning words' class boundary distributions. SLF exhibits robust and scalable behaviour and is easy to tune. We applied this approach on four classical IE tasks: named entity recognition (German and English), part-of-speech tagging (English) and one gene name recognition corpus. Experimental results show effective improvements over the supervised baselines on all tasks. In addition, when compared with the closely related self-training idea, this approach shows favorable advantages.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[semi-supervised learning, semi-supervised feature learning, structural output learning, sequence labeling, self-learned features, information extraction]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1831588</person_id>
				<author_profile_id><![CDATA[81447597003]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yanjun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Qi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831589</person_id>
				<author_profile_id><![CDATA[81387607525]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pavel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kuksa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831590</person_id>
				<author_profile_id><![CDATA[81100001072]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ronan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Collobert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831591</person_id>
				<author_profile_id><![CDATA[81447603696]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kunihiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sadamasa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831592</person_id>
				<author_profile_id><![CDATA[81416596384]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Koray]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kavukcuoglu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831593</person_id>
				<author_profile_id><![CDATA[81100015405]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Jason]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Weston]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677107</article_id>
		<sort_key>560</sort_key>
		<display_label>Pages</display_label>
		<pages>438-446</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>52</seq_no>
		<title><![CDATA[Semi-Markov kMeans Clustering and Activity Recognition from Body-Worn Sensors]]></title>
		<page_from>438</page_from>
		<page_to>446</page_to>
		<doi_number>10.1109/ICDM.2009.13</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677107</url>
		<abstract>
			<par><![CDATA[Subsequence clustering aims to find patterns that appear repeatedly in time series data. We introduce a novel subsequence clustering technique that we call semi-Markov kmeans clustering. The clustering results in ideal examples of the repeating patterns and in labeled segmentations that can be used as training data for sophisticated discriminative methods like max-margin semi-Markov models. We are applying the new clustering technique to activity recognition from body-worn sensors by showing how it can enable a system to learn from data that is only annotated by an ordered list of activity types that have been undertaken. This kind of annotation, unlike a detailed segmentation of the sensor data, is easily provided by a non-expert user. We show that we can achieve equally good results using only an ordered list of activity types for training as when using a full detailed labeled segmentation.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[time-series, subsequence, clustering, activity recognition]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1821137</person_id>
				<author_profile_id><![CDATA[81453609364]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Robards]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1821138</person_id>
				<author_profile_id><![CDATA[81453616870]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sunehag]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677108</article_id>
		<sort_key>570</sort_key>
		<display_label>Pages</display_label>
		<pages>447-456</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>53</seq_no>
		<title><![CDATA[A Sparsification Approach for Temporal Graphical Model Decomposition]]></title>
		<page_from>447</page_from>
		<page_to>456</page_to>
		<doi_number>10.1109/ICDM.2009.67</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677108</url>
		<abstract>
			<par><![CDATA[Temporal causal modeling can be used to recover the causal structure among a group of relevant time series variables. Several methods have been developed to explicitly construct temporal causal graphical models. However, how to best understand and conceptualize these complicated causal relationships is still an open problem. In this paper, we propose a decomposition approach to simplify the temporal graphical model. Our method clusters time series variables into groups such that strong interactions appear among the variables within each group and weak (or no) interactions exist for cross-group variable pairs. Specifically, we formulate the clustering problem for temporal graphical models as a regression-coefficient sparsification problem and define an interesting objective function which balances the model prediction power and its cluster structure. We introduce an iterative optimization approach utilizing the Quasi-Newton method and generalized ridge regression to minimize the objective function and to produce a clustered temporal graphical model. We also present a novel optimization procedure utilizing a graph theoretical tool based on the maximum weight independent set problem to speed up the Quasi-Newton method for a large number of variables. Finally, our detailed experimental study on both synthetic and real datasets demonstrates the effectiveness of our methods.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[temporal graphical model decomposition, Quasi-Newton method, generalized ridge regression, maximum weight independent set]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1830112</person_id>
				<author_profile_id><![CDATA[81351609287]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ning]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ruan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1830113</person_id>
				<author_profile_id><![CDATA[81453629287]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ruoming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1830114</person_id>
				<author_profile_id><![CDATA[81436592719]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Victor]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1830115</person_id>
				<author_profile_id><![CDATA[81453635379]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677109</article_id>
		<sort_key>580</sort_key>
		<display_label>Pages</display_label>
		<pages>457-465</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>54</seq_no>
		<title><![CDATA[Resolving Identity Uncertainty with Learned Random Walks]]></title>
		<page_from>457</page_from>
		<page_to>465</page_to>
		<doi_number>10.1109/ICDM.2009.69</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677109</url>
		<abstract>
			<par><![CDATA[A pervasive problem in large relational databases is identity uncertainty which occurs when multiple entries in a database refer to the same underlying entity in the world. Relational databases exhibit rich graphical structure and are naturally modeled as graphs whose nodes represent entities and whose typed-edges represent relations between them. We propose using random walk models for resolving identity uncertainty since they have proven effective for finding points which are proximately located in a network. Because not all types of relations are equally helpful in alleviating identity uncertainty, we develop a supervised approach to learning the usefulness of different database relations from a training set of database entries whose true identities are known. When tested on the task of resolving uncertainty of ambiguously named authors in bibliographical data, the learned random walk models yield performance superior to support vector machines, and to a related spectral clustering method.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[semi-supervised learning, random walks, identity uncertainty]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1824201</person_id>
				<author_profile_id><![CDATA[81387604070]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ted]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sandler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824202</person_id>
				<author_profile_id><![CDATA[81100365076]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Lyle]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Ungar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824203</person_id>
				<author_profile_id><![CDATA[81100138543]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Koby]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Crammer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677110</article_id>
		<sort_key>590</sort_key>
		<display_label>Pages</display_label>
		<pages>466-475</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>55</seq_no>
		<title><![CDATA[Discriminative Mixed-Membership Models]]></title>
		<page_from>466</page_from>
		<page_to>475</page_to>
		<doi_number>10.1109/ICDM.2009.58</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677110</url>
		<abstract>
			<par><![CDATA[Although mixed-membership models have achieved great success in unsupervised learning, they have not been widely applied to classification problems. In this paper, we propose a family of discriminative mixed-membership models for classification by combining unsupervised mixed-membership models with multi-class logistic regression. In particular, we propose two variants respectively applicable to text classification based on latent Dirichlet allocation and usual feature vector classification based on mixed-membership naive Bayes models. The proposed models allow the number of components in the mixed membership to be different from the number of classes. We propose two variational inference based algorithms for learning the models, including a fast variational inference which is substantially more efficient than mean-field variational approximation. Through extensive experiments on UCI and text classification benchmark datasets, we show that the models are competitive with the state of the art, and can discover components not explicitly captured by the class labels.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1825642</person_id>
				<author_profile_id><![CDATA[81375593685]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hanhuai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825643</person_id>
				<author_profile_id><![CDATA[81100144629]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Arindam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Banerjee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825644</person_id>
				<author_profile_id><![CDATA[81453633109]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nikunj]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Oza]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1676999</article_id>
		<sort_key>600</sort_key>
		<display_label>Pages</display_label>
		<pages>476-482</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>56</seq_no>
		<title><![CDATA[Argumentation Based Constraint Acquisition]]></title>
		<page_from>476</page_from>
		<page_to>482</page_to>
		<doi_number>10.1109/ICDM.2009.62</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1676999</url>
		<abstract>
			<par><![CDATA[Efficient acquisition of constraint networks is a key factor for the applicability of constraint problem solving methods. Current techniques learn constraint networks from sets of training examples, where each example is classified as either a solution or non-solution of a target network. However, in addition to this classification, an expert can usually provide arguments as to why examples should be rejected or accepted. Generally speaking domain specialists have partial knowledge about the theory to be acquired which can be exploited for knowledge acquisition. Based on this observation, we discuss the various types of arguments an expert can formulate and develop a knowledge acquisition algorithm for processing these types of arguments which gives the expert the possibility to input arguments in addition to the learning examples. The result of this approach is a significant reduction in the number of examples which must be provided to the learner in order to learn the target constraint network.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[argumentation, constrains, knowledge acquisition]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1831102</person_id>
				<author_profile_id><![CDATA[81331504196]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kostyantyn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shchekotykhin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831103</person_id>
				<author_profile_id><![CDATA[81100594391]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gerhard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Friedrich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677111</article_id>
		<sort_key>610</sort_key>
		<display_label>Pages</display_label>
		<pages>483-492</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>57</seq_no>
		<title><![CDATA[Extending Semi-supervised Learning Methods for Inductive Transfer Learning]]></title>
		<page_from>483</page_from>
		<page_to>492</page_to>
		<doi_number>10.1109/ICDM.2009.75</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677111</url>
		<abstract>
			<par><![CDATA[Inductive transfer learning and semi-supervised learning are two different branches of machine learning. The former tries to reuse knowledge in labeled out-of-domain instances while the later attempts to exploit the usefulness of unlabeled in-domain instances. In this paper, we bridge the two branches by pointing out that many semi-supervised learning methods can be extended for inductive transfer learning, if the step of labeling an unlabeled instance is replaced by re-weighting a diff-distribution instance. Based on this recognition, we develop a new transfer learning method, namely COITL, by extending the co-training method in semi-supervised learning. Experimental results reveal that COITL can achieve significantly higher generalization and robustness, compared with two state-of-the-art methods in inductive transfer learning.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Inductive transfer learning, semi-supervised learning, co-training]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1830116</person_id>
				<author_profile_id><![CDATA[81392594990]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1830117</person_id>
				<author_profile_id><![CDATA[81453648763]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Zhenzhong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1830118</person_id>
				<author_profile_id><![CDATA[81453637087]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Wei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1830119</person_id>
				<author_profile_id><![CDATA[81453620000]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Wei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677115</article_id>
		<sort_key>620</sort_key>
		<display_label>Pages</display_label>
		<pages>493-502</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>58</seq_no>
		<title><![CDATA[iTopicModel]]></title>
		<subtitle><![CDATA[Information Network-Integrated Topic Modeling]]></subtitle>
		<page_from>493</page_from>
		<page_to>502</page_to>
		<doi_number>10.1109/ICDM.2009.43</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677115</url>
		<abstract>
			<par><![CDATA[Document networks, i.e., networks associated with text information, are becoming increasingly popular due to the ubiquity of Web documents, blogs, and various kinds of online data. In this paper, we propose a novel topic modeling framework for document networks, which builds a unified generative topic model that is able to consider both text and structure information for documents. A graphical model is proposed to describe the generative model. On the top layer of this graphical model, we define a novel multivariate Markov Random Field for topic distribution random variables for each document, to model the dependency relationships among documents over the network structure. On the bottom layer, we follow the traditional topic model to model the generation of text for each document. A joint distribution function for both the text and structure of the documents is thus provided. A solution to estimate this topic model is given, by maximizing the log-likelihood of the joint probability. Some important practical issues in real applications are also discussed, including how to decide the topic number and how to choose a good network structure. We apply the model on two real datasets, DBLP and Cora, and the experiments show that this model is more effective in comparison with the state-of-the-art topic modeling algorithms.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[document networks, topic model, Markov Random Field]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1831691</person_id>
				<author_profile_id><![CDATA[81351602311]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yizhou]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831692</person_id>
				<author_profile_id><![CDATA[81351593425]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jiawei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Han]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831693</person_id>
				<author_profile_id><![CDATA[81314494134]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831694</person_id>
				<author_profile_id><![CDATA[81436595539]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yintao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677116</article_id>
		<sort_key>630</sort_key>
		<display_label>Pages</display_label>
		<pages>503-512</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>59</seq_no>
		<title><![CDATA[Uncoverning Groups via Heterogeneous Interaction Analysis]]></title>
		<page_from>503</page_from>
		<page_to>512</page_to>
		<doi_number>10.1109/ICDM.2009.20</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677116</url>
		<abstract>
			<par><![CDATA[With the pervasive availability of Web 2.0 and social networking sites, people can interact with each other easily through various social media. For instance, popular sites like Del.icio.us, Flickr, and YouTube allow users to comment shared content (bookmark, photos, videos), and users can tag their own favorite content. Users can also connect to each other, and subscribe to or become a fan or a follower of others. These diverse individual activities result in a multi-dimensional network among actors, forming cross-dimension group structures with group members sharing certain similarities. It is challenging to effectively integrate the network information of multiple dimensions in order to discover cross-dimension group structures. In this work, we propose a two-phase strategy to identify the hidden structures shared across dimensions in multi-dimensional networks. We extract structural features from each dimension of the network via modularity analysis, and then integrate them all to find out a robust community structure among actors. Experiments on synthetic and real-world data validate the superiority of our strategy, enabling the analysis of collective behavior underneath diverse individual activities in a large scale.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Heterogeneous Interaction, Multi-Dimensional Networks, Community Detection, Heterogeneous Network, Cross-Dimension Network Validation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1828686</person_id>
				<author_profile_id><![CDATA[81453641306]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828687</person_id>
				<author_profile_id><![CDATA[81484643431]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Xufei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828688</person_id>
				<author_profile_id><![CDATA[81367594306]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Huan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677117</article_id>
		<sort_key>640</sort_key>
		<display_label>Pages</display_label>
		<pages>513-522</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>60</seq_no>
		<title><![CDATA[Significance of Episodes Based on Minimal Windows]]></title>
		<page_from>513</page_from>
		<page_to>522</page_to>
		<doi_number>10.1109/ICDM.2009.23</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677117</url>
		<abstract>
			<par><![CDATA[Discovering episodes, frequent sets of events from a sequence has been an active field in pattern mining. Traditionally, a level-wise approach is used to discover all frequent episodes. While this technique is computationally feasible it may result in a vast number of patterns, especially when low thresholds are used. In this paper we propose a new quality measure for episodes. We say that an episode is significant if the average length of its minimal windows deviates greatly when compared to the expected length according to the independence model. We can apply this measure as a post-pruning step to test whether the discovered frequent episodes are truly interesting and consequently to reduce the number of output. As a main contribution we introduce a technique that allows us to compute the distribution of lengths of minimal windows using the independence model. Such a computation task is surpisingly complex and in order to solve it we compute the distribution iteratively starting from simple episodes and progressively moving towards the more complex ones. In our experiments we discover candidate episodes that have a sufficient amount of minimal windows and test each candidate for significance. The experimental results demonstrate that our approach finds significant episodes while ignoring uninteresting ones.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[episode mining, statistical test, independence model, minimal window]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1827162</person_id>
				<author_profile_id><![CDATA[81363600602]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nikolaj]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tatti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677118</article_id>
		<sort_key>650</sort_key>
		<display_label>Pages</display_label>
		<pages>523-532</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>61</seq_no>
		<title><![CDATA[Convex Non-negative Matrix Factorization in the Wild]]></title>
		<page_from>523</page_from>
		<page_to>532</page_to>
		<doi_number>10.1109/ICDM.2009.55</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677118</url>
		<abstract>
			<par><![CDATA[Non-negative matrix factorization (NMF) has recently received a lot of attention in data mining, information retrieval, and computer vision. It factorizes a non-negative input matrix V into two non-negative matrix factors V = WH such that W describes "clusters" of the datasets. Analyzing genotypes, social networks, or images, it can be beneficial to ensure V to contain meaningful ``cluster centroids'', i.e., to restrict W to be convex combinations of data points. But how can we run this convex NMF in the wild, i.e., given millions of data points? Triggered by the simple observation that each data point is a convex combination of vertices of the data convex hull, we propose to restrict W further to be vertices of the convex hull. The benefits of this convex-hull NMF approach are twofold. First, the expected size of the convex hull of the candidate set typically grows much slower than the data set. Second, distance preserving low-dimensional embeddings allow one to compute candidate vertices efficiently. Our extensive experimental evaluation shows that convex-hull NMF compares favorably to convex NMF for large data sets both in terms of speed and reconstruction quality. Moreover, we show that our method can easily be applied to large-scale, real-world data sets, in our case consisting of 1.6 million images respectively 160 million votes on World of Warcraft guilds.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[data mining, matrix decomposition, data handling, non negative matrix factorization, archetypal analysis, social network analysis]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1824247</person_id>
				<author_profile_id><![CDATA[81319502388]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thurau]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824248</person_id>
				<author_profile_id><![CDATA[81337490510]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kristian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kersting]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824249</person_id>
				<author_profile_id><![CDATA[81100148661]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Christian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bauckhage]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677119</article_id>
		<sort_key>660</sort_key>
		<display_label>Pages</display_label>
		<pages>533-542</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>62</seq_no>
		<title><![CDATA[On K-Means Cluster Preservation Using Quantization Schemes]]></title>
		<page_from>533</page_from>
		<page_to>542</page_to>
		<doi_number>10.1109/ICDM.2009.12</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677119</url>
		<abstract>
			<par><![CDATA[This work examines under what conditions compression methodologies can retain the outcome of clustering operations. We focus on the popular k-Means clustering algorithm and we demonstrate how a properly constructed compression scheme based on post-clustering quantization is capable of maintaining the global cluster structure. Our analytical derivations indicate that a 1-bit moment preserving quantizer per cluster is sufficient to retain the original data clusters. Merits of the proposed compression technique include: a) reduced storage requirements with clustering guarantees, b) data privacy on the original values, and c) shape preservation for data visualization purposes. We evaluate quantization scheme on various high-dimensional datasets, including 1-dimensional and 2-dimensional time-series (shape datasets) and demonstrate the cluster preservation property. We also compare with previously proposed simplification techniques in the time-series area and show significant improvements both on the clustering and shape preservation of the compressed datasets.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[moment preserving quantization, privacy preservation, clustering preservation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1828689</person_id>
				<author_profile_id><![CDATA[81100638226]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Deepak]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Turaga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828690</person_id>
				<author_profile_id><![CDATA[81100026796]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michail]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vlachos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828691</person_id>
				<author_profile_id><![CDATA[81100414472]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Olivier]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Verscheure]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677120</article_id>
		<sort_key>670</sort_key>
		<display_label>Pages</display_label>
		<pages>543-551</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>63</seq_no>
		<title><![CDATA[Scalable Classification in Large Scale Spatiotemporal Domains Applied to Voltage-Sensitive Dye Imaging]]></title>
		<page_from>543</page_from>
		<page_to>551</page_to>
		<doi_number>10.1109/ICDM.2009.24</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677120</url>
		<abstract>
			<par><![CDATA[We present an approach for learning models that obtain accurate classification of large scale data objects, collected in spatiotemporal domains. The model generation is structured in three phases: pixel selection (spatial dimension reduction), spatiotemporal features extraction and feature selection. Novel techniques for the first two phases are presented, with two alternatives for the middle phase. Model generation based on the combinations of techniques from each phase is explored. The introduced methodology is applied on datasets from the Voltage-Sensitive Dye Imaging (VSDI) domain, where the generated classification models successfully decode neuronal population responses in the visual cortex of behaving animals. VSDI currently is the best technique enabling simultaneous high spatial (10,000 points) and temporal (10 ms or less) resolution imaging from neuronal population in the cortex. We demonstrate that not only our approach is scalable enough to handle computationally challenging data, but it also contributes to the neuroimaging field of study with its decoding abilities.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[classification, spatiotemporal, application, brain imaging, neural decoding, visual cortex]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1828718</person_id>
				<author_profile_id><![CDATA[81453654939]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Igor]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vainer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828719</person_id>
				<author_profile_id><![CDATA[81453642515]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sarit]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kraus]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828720</person_id>
				<author_profile_id><![CDATA[81100639280]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kaminka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828721</person_id>
				<author_profile_id><![CDATA[81453654962]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hamutal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Slovin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677121</article_id>
		<sort_key>680</sort_key>
		<display_label>Pages</display_label>
		<pages>552-561</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>64</seq_no>
		<title><![CDATA[Extracting Output Metadata from Scientific Deep Web Data Sources]]></title>
		<page_from>552</page_from>
		<page_to>561</page_to>
		<doi_number>10.1109/ICDM.2009.41</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677121</url>
		<abstract>
			<par><![CDATA[Increasingly, many data sources appear as online databases, hidden behind query forms, thus forming the deep web. The popularity of this new medium for data dissemination is leading to new problems in data integration. Particularly, to enable data integration from multiple deep web data sources, one needs to obtain the metadata for each of the data sources. Obtaining the metadata, particularly, the output schema, can be very challenging. This is because, given an input query, many deep web data sources only return a subset of the output schema attributes, i.e, the ones that have a non-NULL value for the corresponding input. In this paper, we propose two approaches, which are the sampling model approach and the mixture model approach, respectively, to efficiently obtain an approximately complete set of output schema attributes from a deep web data source. Our experiments show while each of the above two approaches has limitations, a hybrid strategy, where we combine the two approaches, achieves high recall with good precision for most data sources.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[deep web, schema extraction]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1828722</person_id>
				<author_profile_id><![CDATA[81384591112]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828723</person_id>
				<author_profile_id><![CDATA[81100288827]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gagan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Agrawal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677122</article_id>
		<sort_key>690</sort_key>
		<display_label>Pages</display_label>
		<pages>562-568</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>65</seq_no>
		<title><![CDATA[Semi-supervised Multi-task Learning with Task Regularizations]]></title>
		<page_from>562</page_from>
		<page_to>568</page_to>
		<doi_number>10.1109/ICDM.2009.66</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677122</url>
		<abstract>
			<par><![CDATA[Multi-task learning refers to the learning problem of performing inference by jointly considering multiple related tasks. There have already been many research efforts on supervised multi-task learning. However, collecting sufficient labeled data for each task is usually time consuming and expensive. In this paper, we consider the semi-supervised multitask learning (SSMTL) problem, where we are given a small portion of labeled points together with a large pool of unlabeled data within each task. We assume that the different tasks can form some task clusters and the task in the same cluster share similar classifier parameters. The final learning problem is relaxed to a convex one and an efficient gradient descent strategy is proposed. Finally the experimental results on both synthetic and real world data sets are presented to show the effectiveness of our method.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1821208</person_id>
				<author_profile_id><![CDATA[81408592258]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1821209</person_id>
				<author_profile_id><![CDATA[81436592535]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Xin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1821210</person_id>
				<author_profile_id><![CDATA[81100475528]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677123</article_id>
		<sort_key>700</sort_key>
		<display_label>Pages</display_label>
		<pages>569-577</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>66</seq_no>
		<title><![CDATA[Fast Online Training of Ramp Loss Support Vector Machines]]></title>
		<page_from>569</page_from>
		<page_to>577</page_to>
		<doi_number>10.1109/ICDM.2009.53</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677123</url>
		<abstract>
			<par><![CDATA[A fast online algorithm OnlineSVMR for training Ramp-Loss Support Vector Machines (SVMRs) is proposed. It finds the optimal SVMR for t+1 training examples using SVMR built on t previous examples. The algorithm retains the Karush&#8211;Kuhn&#8211;Tucker conditions on all previously observed examples. This is achieved by an SMO-style incremental learning and decremental unlearning under the Concave-Convex Procedure framework. Further speedup of training time could be achieved by dropping the requirement of optimality. A variant, called OnlineASVMR, is a greedy approach that approximately optimizes the SVMR objective function and is suitable for online active learning. The proposed algorithms were comprehensively evaluated on 9 large benchmark data sets. The results demonstrate that OnlineSVMR (1) has the similar computational cost as its offline counterpart; (2) outperforms IDSVM, its competing online algorithm that uses hinge-loss, in terms of accuracy, model sparsity and training time. The experiments on online active learning show that for a fixed number of label queries OnlineASVMR (1) achieves consistently better accuracy than QueryAll and competitive accuracy to Greedy approach; (2) outperforms the active learning version of IDSVM.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[SVM, CCCP, SMO, ramp loss, online learning, active learning]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1831741</person_id>
				<author_profile_id><![CDATA[81453633856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Zhuang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831742</person_id>
				<author_profile_id><![CDATA[81100461872]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Slobodan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vucetic]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677124</article_id>
		<sort_key>710</sort_key>
		<display_label>Pages</display_label>
		<pages>578-587</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>67</seq_no>
		<title><![CDATA[Mining Peculiarity Groups in Day-by-Day Behavioral Datasets]]></title>
		<page_from>578</page_from>
		<page_to>587</page_to>
		<doi_number>10.1109/ICDM.2009.48</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677124</url>
		<abstract>
			<par><![CDATA[Behavior mining is one of the most important issues in data mining. The growing interest in the study of behavior mining has been credited to the availability of a large amount of individual behavioral data. Some objects containing common behavioral patterns in the dataset are dramatically different from other individual objects and show their peculiarities. It is very important for behavior analysis to mine these peculiar objects' groups as this has great potential in practice. However, to the best of our knowledge, it has not been explored before. In this paper, we identify this interesting and practical problem of behavior mining: mining peculiarity groups and defining a measurement of the degree of peculiarity. As the first attempt to tackle the problem, we present a set-value-oriented day-by-day behavioral data expression mode considering that daily behaviors with respect to an object should be recorded as a set of behaviors, and devise a peculiarity group mining algorithm in view of the set-value-oriented data expression which cannot be very well handled by existing methods. Furthermore, we show that our method is practical and efficient using real datasets.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[peculiarity groups mining, behavior mining, set-value-oriented data, data mining]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1822821</person_id>
				<author_profile_id><![CDATA[81100313927]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xiong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822822</person_id>
				<author_profile_id><![CDATA[81452599411]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yangyong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677125</article_id>
		<sort_key>720</sort_key>
		<display_label>Pages</display_label>
		<pages>588-597</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>68</seq_no>
		<title><![CDATA[Online System Problem Detection by Mining Patterns of Console Logs]]></title>
		<page_from>588</page_from>
		<page_to>597</page_to>
		<doi_number>10.1109/ICDM.2009.19</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677125</url>
		<abstract>
			<par><![CDATA[We describe a novel application of using data mining and statistical learning methods to automatically monitor and detect abnormal execution traces from console logs in an online setting. Different from existing solutions, we use a two stage detection system. The first stage uses frequent pattern mining and distribution estimation techniques to capture the dominant patterns (both frequent sequences and time duration). The second stage use principal component analysis based anomaly detection technique to identify actual problems. Using real system data from a 203-node Hadoop [1] cluster, we show that we can not only achieve highly accurate and fast problem detection, but also help operators better understand execution patterns in their system.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[console logs, system management, monitoring, problem detection, logs, pattern mining]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1824310</person_id>
				<author_profile_id><![CDATA[81453620498]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824311</person_id>
				<author_profile_id><![CDATA[81331494486]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ling]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824312</person_id>
				<author_profile_id><![CDATA[81100498790]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Armando]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fox]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824313</person_id>
				<author_profile_id><![CDATA[81100565162]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Patterson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824314</person_id>
				<author_profile_id><![CDATA[81339507945]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jordan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677126</article_id>
		<sort_key>730</sort_key>
		<display_label>Pages</display_label>
		<pages>598-606</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>69</seq_no>
		<title><![CDATA[Synthesizing Novel Dimension Reduction Algorithms in Matrix Trace Oriented Optimization Framework]]></title>
		<page_from>598</page_from>
		<page_to>606</page_to>
		<doi_number>10.1109/ICDM.2009.34</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677126</url>
		<abstract>
			<par><![CDATA[Dimension Reduction (DR) algorithms are generally categorized into feature extraction and feature selection algorithms. In the past, few works have been done to contrast and unify the two algorithm categories. In this work, we introduce a matrix trace oriented optimization framework to provide a unifying view for both feature extraction and selection algorithms. We show that the unified view of DR algorithms allows us to discover some essential relationships among many state-of- the-art DR algorithms. Inspired by these essential insights, we propose to synthesize unlimited number of novel DR algorithms by combining, mapping and integrating the state- of-the-art algorithms. We present examples of newly synthesized DR algorithms with experimental results to show the effectiveness of our automatically synthesized algorithms.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[dimension reduction, feature selection, feature extraction]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1822853</person_id>
				<author_profile_id><![CDATA[81375615225]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822854</person_id>
				<author_profile_id><![CDATA[81392606688]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ning]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822855</person_id>
				<author_profile_id><![CDATA[81453621312]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shuicheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822856</person_id>
				<author_profile_id><![CDATA[81372591186]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Qiang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822857</person_id>
				<author_profile_id><![CDATA[81416601059]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Zheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677127</article_id>
		<sort_key>740</sort_key>
		<display_label>Pages</display_label>
		<pages>607-616</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>70</seq_no>
		<title><![CDATA[Peculiarity Analysis for Classifications]]></title>
		<page_from>607</page_from>
		<page_to>616</page_to>
		<doi_number>10.1109/ICDM.2009.31</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677127</url>
		<abstract>
			<par><![CDATA[Peculiarity-oriented mining (POM) is a new data mining method consisting of peculiar data identification and peculiar data analysis. Peculiarity factor (PF) and local peculiarity factor (LPF) are important concepts employed to describe the peculiarity of points in the identification step. One can study the notions at both attribute and record levels. In this paper, a new record LPF called distance based record LPF (D-record LPF) is proposed, which is defined as the sum of distances between a point and its nearest neighbors. It is proved mathematically that D-record LPF can characterize accurately the probability density function of a continuous m-dimensional distribution. This provides a theoretical basis for some existing distance based anomaly detection techniques. More important, it also provides an effective method for describing the class conditional probabilities in the Bayesian classifier. The result enables us to apply peculiarity analysis for classification problems. A novel algorithm called LPF-Bayes classifier and its kernelized implementation are presented, which have some connection to the Bayesian classifier. Experimental results on several benchmark data sets demonstrate that the proposed classifiers are effective.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Peculiarity factor, local peculiarity factor, probability density function, Bayesian classifier, LPF-Bayes classifier]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1825735</person_id>
				<author_profile_id><![CDATA[81453606279]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825736</person_id>
				<author_profile_id><![CDATA[81327492811]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ning]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825737</person_id>
				<author_profile_id><![CDATA[81100543359]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yiyu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825738</person_id>
				<author_profile_id><![CDATA[81100233470]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jue]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677128</article_id>
		<sort_key>750</sort_key>
		<display_label>Pages</display_label>
		<pages>617-626</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>71</seq_no>
		<title><![CDATA[Filtering and Refinement]]></title>
		<subtitle><![CDATA[A Two-Stage Approach for Efficient and Effective Anomaly Detection]]></subtitle>
		<page_from>617</page_from>
		<page_to>626</page_to>
		<doi_number>10.1109/ICDM.2009.44</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677128</url>
		<abstract>
			<par><![CDATA[Anomaly detection is an important data mining task. Most existing methods treat anomalies as inconsistencies and spend the majority amount of time on modeling normal instances. A recently proposed, sampling-based approach may substantially boost the efficiency in anomaly detection but may also lead to weaker accuracy and robustness. In this study, we propose a two-stage approach to find anomalies in complex datasets with high accuracy as well as low time complexity and space cost. Instead of analyzing normal instances, our algorithm first employs an efficient deterministic space partition algorithm to eliminate obvious normal instances and generates a small set of anomaly candidates with a single scan of the dataset. It then checks each candidate with density-based multiple criteria to determine the final results. This two-stage framework also detects anomalies of different notions. Our experiments show that this new approach finds anomalies successfully in different conditions and ensures a good balance of efficiency, accuracy, and robustness.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1825739</person_id>
				<author_profile_id><![CDATA[81486653339]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xiao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825740</person_id>
				<author_profile_id><![CDATA[81453641307]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Lu]]></first_name>
				<middle_name><![CDATA[An]]></middle_name>
				<last_name><![CDATA[Tang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825741</person_id>
				<author_profile_id><![CDATA[81453609649]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jiawei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Han]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677129</article_id>
		<sort_key>760</sort_key>
		<display_label>Pages</display_label>
		<pages>627-636</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>72</seq_no>
		<title><![CDATA[Mining Data Streams with Labeled and Unlabeled Training Examples]]></title>
		<page_from>627</page_from>
		<page_to>636</page_to>
		<doi_number>10.1109/ICDM.2009.76</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677129</url>
		<abstract>
			<par><![CDATA[In this paper, we propose a framework to build prediction models from data streams which contain both labeled and unlabeled examples. We argue that due to the increasing data collection ability but limited resources for labeling, stream data collected at hand may only have a small number of labeled examples, whereas a large portion of data remain unlabeled but can be beneficial for learning. Unleashing the full potential of the unlabeled instances for stream data mining is, however, a significant challenge, consider that even fully labeled data streams may suffer from the concept drifting, and inappropriate uses of the unlabeled samples may only make the problem even worse. To build prediction models, we first categorize the stream data into four different categories, each of which corresponds to the situation where concept drifting may or may not exist in the labeled and unlabeled data. After that, we propose a relational k-means based transfer semi-supervised SVM learning framework (RK-TS3VM), which intends to leverage labeled and unlabeled samples to build prediction models. Experimental results and comparisons on both synthetic and real-world data streams demonstrate that the proposed framework is able to help build prediction models more accurate than other simple approaches can offer.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[data stream, unlabeled samples, support vector machines]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1821254</person_id>
				<author_profile_id><![CDATA[81384603947]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Peng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1821255</person_id>
				<author_profile_id><![CDATA[81452600756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Xingquan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1821256</person_id>
				<author_profile_id><![CDATA[81325488381]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Li]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677130</article_id>
		<sort_key>770</sort_key>
		<display_label>Pages</display_label>
		<pages>637-646</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>73</seq_no>
		<title><![CDATA[Maximum Margin Clustering with Multivariate Loss Function]]></title>
		<page_from>637</page_from>
		<page_to>646</page_to>
		<doi_number>10.1109/ICDM.2009.37</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677130</url>
		<abstract>
			<par><![CDATA[This paper presents a simple but powerful extension of the maximum margin clustering (MMC) algorithm that optimizes multivariate performance measure specifically defined for clustering, including Normalized Mutual In- formation, Rand Index and F-measure. Different from previous MMC algorithms that always employ the error rate as the loss function, our formulation involves a multivariate loss function that is a non-linear combination of the individual clustering results. Computationally, we propose a cutting plane algorithm to approximately solve the resulting optimization problem with a guaranteed accuracy. Experimental evaluations show clear improvements in clustering performance of our method over previous maximum margin clustering algorithms.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[maximum margin clustering, multivariate performance measure]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1821257</person_id>
				<author_profile_id><![CDATA[81100535703]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1821258</person_id>
				<author_profile_id><![CDATA[81100525095]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kwok]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1821259</person_id>
				<author_profile_id><![CDATA[81372592002]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Changshui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677131</article_id>
		<sort_key>780</sort_key>
		<display_label>Pages</display_label>
		<pages>647-656</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>74</seq_no>
		<title><![CDATA[Efficient Discovery of Confounders in Large Data Sets]]></title>
		<page_from>647</page_from>
		<page_to>656</page_to>
		<doi_number>10.1109/ICDM.2009.77</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677131</url>
		<abstract>
			<par><![CDATA[Given a large transaction database, association analysis is concerned with efficiently finding strongly related objects. Unlike traditional associate analysis, where relationships among variables are searched at a global level, we examine confounding factors at a local level. Indeed, many real-world phenomena are localized to specific regions and times. These relationships may not be visible when the entire data set is analyzed. Specially, confounding effects that change the direction of correlation is the most significant. Along this line, we propose to efficiently find confounding effects attributable to local associations. Specifically, we derive an upper bound by a necessary condition of confounders, which can help us prune the search space and efficiently identify confounders. Experimental results show that the proposed CONFOUND algorithm can effectively identify confounders and the computational performance is an order of magnitude faster than benchmark methods.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Phi Correlation coefficient, Correlation, Partial Correlation, Local Association, Confounder]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1821296</person_id>
				<author_profile_id><![CDATA[81323498342]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wenjun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1821297</person_id>
				<author_profile_id><![CDATA[81451596433]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xiong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677132</article_id>
		<sort_key>790</sort_key>
		<display_label>Pages</display_label>
		<pages>657-666</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>75</seq_no>
		<title><![CDATA[Vague One-Class Learning for Data Streams]]></title>
		<page_from>657</page_from>
		<page_to>666</page_to>
		<doi_number>10.1109/ICDM.2009.70</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677132</url>
		<abstract>
			<par><![CDATA[In this paper, we formulate a new research problem of learning from vaguely labeled one-class data streams, where the main objective is to allow users to label instance groups, instead of single instances, as positive samples for learning. The batch-labeling, however, raises serious issues because labeled groups may contain non-positive samples, and users may change their labeling interests at any time. To solve this problem, we propose a Vague One-Class Learning (VOCL) framework which employs a double weighting approach, at both instance and classifier levels, to build an ensembling framework for learning. At instance level, both local and global filterings are considered for instance weight adjustment. Two solutions are proposed to take instance weight values into the classifier training process. At classifier level, a weight value is assigned to each classifier of the ensemble to ensure that learning can quickly adapt to users&#8217; interests. Experimental results on synthetic and real-world data streams demonstrate that the proposed VOCL framework significantly outperforms other methods for vaguely labeled one-class data streams.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[stream data, one-class learning, vague labeling]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1828785</person_id>
				<author_profile_id><![CDATA[81452600756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xingquan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828786</person_id>
				<author_profile_id><![CDATA[81452596585]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Xindong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828787</person_id>
				<author_profile_id><![CDATA[81453613239]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chengqi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677133</article_id>
		<sort_key>800</sort_key>
		<display_label>Pages</display_label>
		<pages>667-676</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>76</seq_no>
		<title><![CDATA[Inverse Time Dependency in Convex Regularized Learning]]></title>
		<page_from>667</page_from>
		<page_to>676</page_to>
		<doi_number>10.1109/ICDM.2009.28</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677133</url>
		<abstract>
			<par><![CDATA[In the conventional regularized learning, training time increases as the training set expands. Recent work on L2 linear SVM challenges this common sense by proposing the inverse time dependency on the training set size. In this paper, we first put forward a Primal Gradient Solver (PGS) to effectively solve the convex regularized learning problem. This solver is based on the stochastic gradient descent method and the Fenchel conjugate adjustment, employing the well-known online strongly convex optimization algorithm with logarithmic regret. We then theoretically prove the inverse dependency property of our PGS, embracing the previous work of the L2 linear SVM as a special case and enable the l_p-norm optimization to run within a bounded sphere, which qualifies more convex loss functions in PGS. We further illustrate this solver in three examples: SVM, logistic regression and regularized least square. Experimental results substantiate the property of the inverse dependency on training data size.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Fenchel conjugate]]></kw>
			<kw><![CDATA[Primal Gradient Solver]]></kw>
			<kw><![CDATA[inverse time dependency]]></kw>
			<kw><![CDATA[online convex optimization]]></kw>
			<kw><![CDATA[regularized learning]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1828788</person_id>
				<author_profile_id><![CDATA[81447600723]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Zeyuan]]></first_name>
				<middle_name><![CDATA[Allen]]></middle_name>
				<last_name><![CDATA[Zhu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828789</person_id>
				<author_profile_id><![CDATA[81363596263]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Weizhu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828790</person_id>
				<author_profile_id><![CDATA[81447592454]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chenguang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828791</person_id>
				<author_profile_id><![CDATA[81418594285]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Gang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828792</person_id>
				<author_profile_id><![CDATA[81453632944]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Haixun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828793</person_id>
				<author_profile_id><![CDATA[81416601059]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Zheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677134</article_id>
		<sort_key>810</sort_key>
		<display_label>Pages</display_label>
		<pages>677-686</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>77</seq_no>
		<title><![CDATA[P-packSVM]]></title>
		<subtitle><![CDATA[Parallel Primal grAdient desCent Kernel SVM]]></subtitle>
		<page_from>677</page_from>
		<page_to>686</page_to>
		<doi_number>10.1109/ICDM.2009.29</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677134</url>
		<abstract>
			<par><![CDATA[It is an extreme challenge to produce a nonlinear SVM classifier on very large scale data. In this paper we describe a novel P-packSVM algorithm that can solve the Support Vector Machine (SVM) optimization problem with an arbitrary kernel. This algorithm embraces the best known stochastic gradient descent method to optimize the primal objective, and has 1/&#1013; dependency in complexity to obtain a solution of optimization error &#1013;. The algorithm can be highly parallelized with a special packing strategy, and experiences sub-linear speed-up with hundreds of processors. We demonstrate that P-packSVM achieves accuracy sufficiently close to that of SVM-light, and overwhelms the state-of-the-art parallel SVM trainer PSVM in both accuracy and efficiency. As an illustration, our algorithm trains CCAT dataset with 800k samples in 13 minutes and 95% accuracy, while PSVM needs 5 hours but only has 92% accuracy. We at last demonstrate the capability of P-packSVM on 8 million training samples.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[kernel]]></kw>
			<kw><![CDATA[packing strategy]]></kw>
			<kw><![CDATA[parallel]]></kw>
			<kw><![CDATA[stochastic gradient descent]]></kw>
			<kw><![CDATA[support vector machine]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1821298</person_id>
				<author_profile_id><![CDATA[81447600723]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Zeyuan]]></first_name>
				<middle_name><![CDATA[Allen]]></middle_name>
				<last_name><![CDATA[Zhu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1821299</person_id>
				<author_profile_id><![CDATA[81363596263]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Weizhu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1821300</person_id>
				<author_profile_id><![CDATA[81418594285]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1821301</person_id>
				<author_profile_id><![CDATA[81447592454]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Chenguang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1821302</person_id>
				<author_profile_id><![CDATA[81416601059]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Zheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677135</article_id>
		<sort_key>820</sort_key>
		<display_label>Pages</display_label>
		<pages>687-692</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>78</seq_no>
		<title><![CDATA[An L-infinity Norm Visual Classifier]]></title>
		<page_from>687</page_from>
		<page_to>692</page_to>
		<doi_number>10.1109/ICDM.2009.119</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677135</url>
		<abstract>
			<par><![CDATA[We introduce a mathematical framework, based on the L-infinity norm distance metric, to describe human interactions in a visual data mining environment. We use the framework to build a classifier that involves an algebra on hyper-rectangles. Our classifier, called VisClassifier, generates set-wise rules from simple gestures in an exploratory visual GUI. Logging these rules allows us to apply our analysis to a new sample or batch of data so that we can assess the predictive power of our visual-processing motivated classifier. The accuracy of this classifier on widely-used benchmark datasets rivals the accuracy of competitive classifiers.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Visual data mining, supervised classification]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1821303</person_id>
				<author_profile_id><![CDATA[81319487848]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Anushka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Anand]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1821304</person_id>
				<author_profile_id><![CDATA[81100304143]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Leland]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wilkinson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1821305</person_id>
				<author_profile_id><![CDATA[81453628826]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Dang]]></first_name>
				<middle_name><![CDATA[Nhon]]></middle_name>
				<last_name><![CDATA[Tuan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677136</article_id>
		<sort_key>830</sort_key>
		<display_label>Pages</display_label>
		<pages>693-698</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>79</seq_no>
		<title><![CDATA[Outlier Detection Using Inductive Logic Programming]]></title>
		<page_from>693</page_from>
		<page_to>698</page_to>
		<doi_number>10.1109/ICDM.2009.127</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677136</url>
		<abstract>
			<par><![CDATA[We present a novel definition of outlier in the context of inductive logic programming. Given a set of positive and negative examples, the definition aims at singling out the examples showing anomalous behavior. We note that the task here pursued is different from noise removal, and, in fact, the anomalous observations we discover are different in nature from noisy ones. We discuss pecularities of the novel approach, present an algorithm for detecting outliers, discuss some examples of knowledge mined, and compare it with alternative approaches.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Outlier detection, Inductive Logic Programming]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1828794</person_id>
				<author_profile_id><![CDATA[81100400419]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fabrizio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Angiulli]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828795</person_id>
				<author_profile_id><![CDATA[81314493715]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Fabio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fassetti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677137</article_id>
		<sort_key>840</sort_key>
		<display_label>Pages</display_label>
		<pages>699-704</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>80</seq_no>
		<title><![CDATA[Joint Emotion-Topic Modeling for Social Affective Text Mining]]></title>
		<page_from>699</page_from>
		<page_to>704</page_to>
		<doi_number>10.1109/ICDM.2009.94</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677137</url>
		<abstract>
			<par><![CDATA[This paper is concerned with the problem of social affective text mining, which aims to discover the connections between social emotions and affective terms based on user-generated emotion labels. We propose a joint emotion-topic model by augmenting latent Dirichlet allocation with an additional layer for emotion modeling. It first generates a set of latent topics from emotions, followed by generating affective terms from each topic. Experimental results on an online news collection show that the proposed model can effectively identify meaningful latent topics for each emotion. Evaluation on emotion prediction further verifies the effectiveness of the proposed model.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Emotion-Topic Model, Social Affective Text Mining, Emotion Prediction]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1822910</person_id>
				<author_profile_id><![CDATA[81453610810]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shenghua]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822911</person_id>
				<author_profile_id><![CDATA[81340494008]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shengliang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822912</person_id>
				<author_profile_id><![CDATA[81453606624]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Li]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822913</person_id>
				<author_profile_id><![CDATA[81508699942]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Rong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822914</person_id>
				<author_profile_id><![CDATA[81453629575]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Zhong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Su]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822915</person_id>
				<author_profile_id><![CDATA[81309495513]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Dingyi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Han]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822916</person_id>
				<author_profile_id><![CDATA[81453637756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Yong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677138</article_id>
		<sort_key>850</sort_key>
		<display_label>Pages</display_label>
		<pages>705-710</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>81</seq_no>
		<title><![CDATA[Algorithms for Large, Sparse Network Alignment Problems]]></title>
		<page_from>705</page_from>
		<page_to>710</page_to>
		<doi_number>10.1109/ICDM.2009.135</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677138</url>
		<abstract>
			<par><![CDATA[We propose a new distributed algorithm for sparse variants of the network alignment problem, which occurs in a variety of data mining areas including systems biology, database matching, and computer vision. Our algorithm uses a belief propagation heuristic and provides near optimal solutions for this NP-hard combinatorial optimization problem. We show that our algorithm is faster and outperforms or ties existing algorithms on synthetic problems, a problem in bioinformatics, and a problem in ontology matching. We also provide a unified framework for studying and comparing all network alignment solvers.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[network alignment, belief propagation, graph matching, message-passing]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1824361</person_id>
				<author_profile_id><![CDATA[81322488823]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mohsen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bayati]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824362</person_id>
				<author_profile_id><![CDATA[81453645620]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Margot]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gerritsen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824363</person_id>
				<author_profile_id><![CDATA[81367596421]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Gleich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824364</person_id>
				<author_profile_id><![CDATA[81100527689]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Amin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Saberi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824365</person_id>
				<author_profile_id><![CDATA[81453615419]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ying]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677139</article_id>
		<sort_key>860</sort_key>
		<display_label>Pages</display_label>
		<pages>711-715</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>82</seq_no>
		<title><![CDATA[Dirichlet Mixture Allocation for Multiclass Document Collections Modeling]]></title>
		<page_from>711</page_from>
		<page_to>715</page_to>
		<doi_number>10.1109/ICDM.2009.102</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677139</url>
		<abstract>
			<par><![CDATA[Topic model, Latent Dirichlet Allocation (LDA), is an effective tool for statistical analysis of large collections of documents. In LDA, each document is modeled as a mixture of topics and the topic proportions are generated from the unimodal Dirichlet distribution prior. When a collection of documents are drawn from multiple classes, this unimodal prior is insufficient for data fitting. To solve this problem, we exploit the multimodal Dirichlet mixture prior, and propose the Dirichlet mixture allocation (DMA). We report experiments on the popular TDT2 Corpus demonstrating that DMA models a collection of documents more precisely than LDA when the documents are obtained from multiple classes.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[topic model, Dirichlet mixture, multiclass, latent Dirichlet allocation, text modeling]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1828816</person_id>
				<author_profile_id><![CDATA[81542952556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bian]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828817</person_id>
				<author_profile_id><![CDATA[81100159571]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dacheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677140</article_id>
		<sort_key>870</sort_key>
		<display_label>Pages</display_label>
		<pages>716-721</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>83</seq_no>
		<title><![CDATA[SLIDER]]></title>
		<subtitle><![CDATA[Mining Correlated Motifs in Protein-Protein Interaction Networks]]></subtitle>
		<page_from>716</page_from>
		<page_to>721</page_to>
		<doi_number>10.1109/ICDM.2009.92</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677140</url>
		<abstract>
			<par><![CDATA[Correlated motif mining (CMM) is the problem to find overrepresented pairs of patterns, called motif pairs, in interacting protein sequences. Algorithmic solutions for CMM thereby provide a computational method for predicting binding sites for protein interaction. In this paper, we adopt a motif-driven approach where the support of candidate motif pairs is evaluated in the network. We experimentally establish the superiority of the Chi-square-based support measure over other support measures. Furthermore, we obtain that CMM is an NP-hard problem for a large class of support measures (including Chi-square) and reformulate the search for correlated motifs as a combinatorial optimization problem. We then present the method SLIDER which uses local search with a neighborhood function based on sliding motifs and employs the Chi-square-based support measure. We show that SLIDER outperforms existing motif-driven CMM methods and scales to large protein-protein interaction networks.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Correlated motifs, PPI networks, local search]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1824366</person_id>
				<author_profile_id><![CDATA[81453661183]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Boyen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824367</person_id>
				<author_profile_id><![CDATA[81100091691]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Frank]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Neven]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824368</person_id>
				<author_profile_id><![CDATA[81453647270]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Dries]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Van Dyck]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824369</person_id>
				<author_profile_id><![CDATA[81388593068]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Aalt-Jan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[van Dijk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824370</person_id>
				<author_profile_id><![CDATA[81453634739]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Roeland]]></first_name>
				<middle_name><![CDATA[C. H. J.]]></middle_name>
				<last_name><![CDATA[van Ham]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677141</article_id>
		<sort_key>880</sort_key>
		<display_label>Pages</display_label>
		<pages>722-727</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>84</seq_no>
		<title><![CDATA[Effective Anomaly Detection in Sensor Networks Data Streams]]></title>
		<page_from>722</page_from>
		<page_to>727</page_to>
		<doi_number>10.1109/ICDM.2009.110</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677141</url>
		<abstract>
			<par><![CDATA[&#151;This paper addresses a major challenge in data mining applications where the full information about the underlying processes, such as sensor networks or large online database, cannot be practically obtained due to physical limitations such as low bandwidth or memory, storage, or computing power. Motivated by the recent theory on direct information sampling called compressed sensing (CS), we propose a framework for detecting anomalies from these large-scale data mining applications where the full information is not practically possible to obtain. Exploiting the fact that the intrinsic dimension of the data in these applications are typically small relative to the raw dimension and the fact that compressed sensing is capable of capturing most information with few measurements, our work show that spectral methods that used for volume anomaly detection can be directly applied to the CS data with guarantee on performance. Our theoretical contributions are supported by extensive experimental results on large datasets which show satisfactory performance.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[stream data processing, anomaly detection, spectral methods, residual analysis]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1824371</person_id>
				<author_profile_id><![CDATA[81453656225]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Saha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Budhaditya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824372</person_id>
				<author_profile_id><![CDATA[81453632479]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Duc-Son]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pham]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824373</person_id>
				<author_profile_id><![CDATA[81452605550]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mihai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lazarescu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824374</person_id>
				<author_profile_id><![CDATA[81361600568]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Svetha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Venkatesh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677142</article_id>
		<sort_key>890</sort_key>
		<display_label>Pages</display_label>
		<pages>728-733</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>85</seq_no>
		<title><![CDATA[Hierarchical Bayesian Models for Collaborative Tagging Systems]]></title>
		<page_from>728</page_from>
		<page_to>733</page_to>
		<doi_number>10.1109/ICDM.2009.121</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677142</url>
		<abstract>
			<par><![CDATA[Collaborative tagging systems with user generated content have become a fundamental element of websites such as Delicious, Flickr or CiteULike. By sharing common knowledge, massively linked semantic data sets are generated that provide new challenges for data mining. In this paper, we reduce the data complexity in these systems by finding meaningful topics that serve to group similar users and serve to recommend tags or resources to users. We propose a well-founded probabilistic approach that can model every aspect of a collaborative tagging system. By integrating both user information and tag information into the well-known Latent Dirichlet Allocation framework, the developed models can be used to solve a number of important information extraction and retrieval tasks.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[collaborative tagging, LDA, user modeling]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1828818</person_id>
				<author_profile_id><![CDATA[81367598941]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Markus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bundschus]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828819</person_id>
				<author_profile_id><![CDATA[81100473562]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shipeng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828820</person_id>
				<author_profile_id><![CDATA[81100337356]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Volker]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tresp]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828821</person_id>
				<author_profile_id><![CDATA[81365591416]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Achim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rettinger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828822</person_id>
				<author_profile_id><![CDATA[81100067315]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Mathaeus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dejori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828823</person_id>
				<author_profile_id><![CDATA[81100512920]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Hans-Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kriegel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677143</article_id>
		<sort_key>900</sort_key>
		<display_label>Pages</display_label>
		<pages>734-739</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>86</seq_no>
		<title><![CDATA[Efficient Algorithm for Computing Link-Based Similarity in Real World Networks]]></title>
		<page_from>734</page_from>
		<page_to>739</page_to>
		<doi_number>10.1109/ICDM.2009.136</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677143</url>
		<abstract>
			<par><![CDATA[Similarity calculation has many applications, such as information retrieval, and collaborative filtering, among many others. It has been shown that link-based similarity measure, such as SimRank, is very effective in characterizing the object similarities in networks, such as the Web, by exploiting the object-to-object relationship. Unfortunately, it is prohibitively expensive to compute the link-based similarity in a relatively large graph. In this paper, based on the observation that link-based similarity scores of real world graphs follow the power-law distribution, we propose a new approximate algorithm, namely Power-SimRank, with guaranteed error bound to efficiently compute link-based similarity measure. We also prove the convergence of the proposed algorithm. Extensive experiments conducted on real world datasets and synthetic datasets show that the proposed algorithm outperforms SimRank by four-five times in terms of efficiency while the error generated by the approximation is small.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Similarity Calculation, SimRank, Graph Mining]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1827299</person_id>
				<author_profile_id><![CDATA[81418596020]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuanzhe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1827300</person_id>
				<author_profile_id><![CDATA[81100468764]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1827301</person_id>
				<author_profile_id><![CDATA[81418596101]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Xu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1827302</person_id>
				<author_profile_id><![CDATA[81456625293]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hongyan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1827303</person_id>
				<author_profile_id><![CDATA[81100331873]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[He]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1827304</person_id>
				<author_profile_id><![CDATA[81453625376]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Jiaheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1827305</person_id>
				<author_profile_id><![CDATA[81100306603]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Xiaoyong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Du]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677000</article_id>
		<sort_key>910</sort_key>
		<display_label>Pages</display_label>
		<pages>740-745</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>87</seq_no>
		<title><![CDATA[Fine-Grain Perturbation for Privacy Preserving Data Publishing]]></title>
		<page_from>740</page_from>
		<page_to>745</page_to>
		<doi_number>10.1109/ICDM.2009.98</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677000</url>
		<abstract>
			<par><![CDATA[Recent work [12] shows that conventional privacy preserving publishing techniques based on anonymity-groups are susceptible to corruption attacks. In a corruption attack, if the sensitive information of any anonymity-group member is uncovered, then the remaining group members are at risk. In this study, we abandon anonymity-groups and hide sensitive information through perturbation on the sensitive attribute. With each record being perturbed independently, corruption attacks cannot be effectively carried out. Previous anti-corruption work did not minimize information loss. This paper proposes to address this issue by allowing fine-grain privacy specification. We demonstrate the power of our approach through experiments on real medical and synthetic datasets.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1820820</person_id>
				<author_profile_id><![CDATA[81461646020]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Rhonda]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chaytor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1820821</person_id>
				<author_profile_id><![CDATA[81350574174]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1820822</person_id>
				<author_profile_id><![CDATA[81490667409]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Patricia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Brantingham]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677001</article_id>
		<sort_key>920</sort_key>
		<display_label>Pages</display_label>
		<pages>746-751</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>88</seq_no>
		<title><![CDATA[Accelerated Gradient Method for Multi-task Sparse Learning Problem]]></title>
		<page_from>746</page_from>
		<page_to>751</page_to>
		<doi_number>10.1109/ICDM.2009.128</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677001</url>
		<abstract>
			<par><![CDATA[Many real world learning problems can be recast as multi-task learning problems which utilize correlations among different tasks to obtain better generalization performance than learning each task individually. The feature selection problem in multi-task setting has many applications in fields of computer vision, text classification and bio-informatics. Generally, it can be realized by solving a L-1-infinity regularized optimization problem. And the solution automatically yields the joint sparsity among different tasks. However, due to the nonsmooth nature of the L-1-infinity norm, there lacks an efficient training algorithm for solving such problem with general convex loss functions. In this paper, we propose an accelerated gradient method based on an ``optimal'' first order black-box method named after Nesterov and provide the convergence rate for smooth convex loss functions. For nonsmooth convex loss functions, such as hinge loss, our method still has fast convergence rate empirically. Moreover, by exploiting the structure of the L-1-infinity ball, we solve the black-box oracle in Nesterov's method by a simple sorting scheme. Our method is suitable for large-scale multi-task learning problem since it only utilizes the first order information and is very easy to implement. Experimental results show that our method significantly outperforms the most state-of-the-art methods in both convergence speed and learning accuracy.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[multi-task learning, L-1-infinity regularization, optimal method, gradient descend]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1825243</person_id>
				<author_profile_id><![CDATA[81453638592]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825244</person_id>
				<author_profile_id><![CDATA[81453634054]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Weike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825245</person_id>
				<author_profile_id><![CDATA[81100525095]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[Kwok]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825246</person_id>
				<author_profile_id><![CDATA[81452611253]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jaime]]></first_name>
				<middle_name><![CDATA[G.]]></middle_name>
				<last_name><![CDATA[Carbonell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677002</article_id>
		<sort_key>930</sort_key>
		<display_label>Pages</display_label>
		<pages>752-757</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>89</seq_no>
		<title><![CDATA[CoFKM]]></title>
		<subtitle><![CDATA[A Centralized Method for Multiple-View Clustering]]></subtitle>
		<page_from>752</page_from>
		<page_to>757</page_to>
		<doi_number>10.1109/ICDM.2009.138</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677002</url>
		<abstract>
			<par><![CDATA[This paper deals with clustering for multi-view data, i.e. objects described by several sets of variables or proximity matrices. Many important domains or applications such as Information Retrieval, biology, chemistry and marketing are concerned by this problematic. The aim of this data mining research field is to search for clustering patterns that perform a consensus between the patterns from different views. This requires to merge information from each view by performing a fusion process that identifies the agreement between the views and solves the conflicts. Various fusion strategies can be applied, occurring either before, after or during the clustering process. We draw our inspiration from the existing algorithms based on a centralized strategy. We propose a fuzzy clustering approach that generalizes the three fusion strategies and outperforms the main existing multi-view clustering algorithm both on synthetic and real datasets.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[multi-view data, fuzzy clustering, collaborative clustering, centralized clustering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1823817</person_id>
				<author_profile_id><![CDATA[81384609277]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Guillaume]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cleuziou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1823818</person_id>
				<author_profile_id><![CDATA[81479646341]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Matthieu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Exbrayat]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1823819</person_id>
				<author_profile_id><![CDATA[81100425805]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Lionel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Martin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1823820</person_id>
				<author_profile_id><![CDATA[81453657779]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jacques-Henri]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sublemontier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677003</article_id>
		<sort_key>940</sort_key>
		<display_label>Pages</display_label>
		<pages>758-763</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>90</seq_no>
		<title><![CDATA[Active Selection of Sensor Sites in Remote Sensing Applications]]></title>
		<page_from>758</page_from>
		<page_to>763</page_to>
		<doi_number>10.1109/ICDM.2009.139</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677003</url>
		<abstract>
			<par><![CDATA[In a data-mining approach, a model for estimation of Aerosol Optical Depth (AOD) from satellite observations is learned using collocated satellite and ground-based observations. For accurate learning of such a spatio-temporal model, it is important to collect ground-based data from a large number of sites. The objective of this project is to determine appropriate locations for the next set of ground-based data collection sites to maximize accuracy of AOD estimation. Ideally, a new site should capture the most significant unseen aerosol patterns and should be the least correlated with the previously observed patterns. We propose achieving this aim by selecting the locations on which the existing prediction model is the most uncertain. Several criteria were considered for site selection, including uncertainty, spatial diversity, similarity in temporal pattern, and their combination. Extensive experiments on globally distributed data over 90 AERONET sites from the years 2005 and 2006 provide strong evidence that sites selected using the proposed algorithms improve the overall AOD prediction accuracy at a faster rate than those selected randomly or based on spatial diversity among sites.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[aerosol estimation, sensor site selection, uncertainty sampling, active learning]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1822348</person_id>
				<author_profile_id><![CDATA[81100143288]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Debasish]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Das]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822349</person_id>
				<author_profile_id><![CDATA[81100041040]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Zoran]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Obradovic]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822350</person_id>
				<author_profile_id><![CDATA[81100461872]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Slobodan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vucetic]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677004</article_id>
		<sort_key>950</sort_key>
		<display_label>Pages</display_label>
		<pages>764-769</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>91</seq_no>
		<title><![CDATA[Large Scale Relation Acquisition Using Class Dependent Patterns]]></title>
		<page_from>764</page_from>
		<page_to>769</page_to>
		<doi_number>10.1109/ICDM.2009.140</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677004</url>
		<abstract>
			<par><![CDATA[This paper proposes a minimally supervised method for acquiring high-level semantic relations such as causality and prevention from the Web. Our method learns linguistic patterns that express causality such as &#8220;x gave rise to y&#8221;, and uses them to extract causal noun pairs like (global warming, malaria epidemic) from sentences like &#8220;global warming gave rise to a new malaria epidemic&#8221;. The novelty of our method lies in the use of semantic word classes acquired by large scale clustering for learning class dependent patterns. We demonstrate the effectiveness of this class based approach on three large-scale relation mining tasks from 50 million Japanese Web pages. In two of these tasks we obtained more than 30,000 relation instances with over 80% precision, outperforming a state-of-the-art system by a large margin.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1820823</person_id>
				<author_profile_id><![CDATA[81442615295]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stijn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[De Saeger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1820824</person_id>
				<author_profile_id><![CDATA[81100583329]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kentaro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Torisawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1820825</person_id>
				<author_profile_id><![CDATA[81323491475]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jun'ichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kazama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1820826</person_id>
				<author_profile_id><![CDATA[81448599839]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kow]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kuroda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1820827</person_id>
				<author_profile_id><![CDATA[81453642794]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Masaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Murata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677016</article_id>
		<sort_key>960</sort_key>
		<display_label>Pages</display_label>
		<pages>770-775</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>92</seq_no>
		<title><![CDATA[Finding Maximal Fully-Correlated Itemsets in Large Databases]]></title>
		<page_from>770</page_from>
		<page_to>775</page_to>
		<doi_number>10.1109/ICDM.2009.89</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677016</url>
		<abstract>
			<par><![CDATA[Finding the most interesting correlations among items is essential for problems in many commercial, medical, and scientific domains. Much previous research focuses on finding correlated pairs instead of correlated itemsets in which all items are correlated with each other. When designing gift sets, store shelf arrangements, or website product categories, we are more interested in correlated itemsets than correlated pairs. We solve this problem by finding maximal fully-correlated itemsets (MFCIs), in which all subsets are closely related to all other subsets. Putting the items in an MFCI together can promote sales within this itemset. Though some exsiting methods find high-correlation itemsets, they suffer from both efficiency and effectiveness problems in large datasets. In this paper, we explore high-dimensional correlation in two ways. First, we expand the set of desirable properties for correlation measures and study the advantages and disadvantages of various measures. Second, we propose an MFCI framework to decouple the correlation measure from the need for efficient search. By wrapping the best measure in our MFCI framework, we take advantage of likelihood ratio&#8217;s superiority in evaluating itemsets, make use of the properties of MFCI to eliminate itemsets with irrelevant items, and still achieve good computational performance.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1831330</person_id>
				<author_profile_id><![CDATA[81453608240]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Duan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831331</person_id>
				<author_profile_id><![CDATA[81453607905]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[Nick]]></middle_name>
				<last_name><![CDATA[Street]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1676996</article_id>
		<sort_key>970</sort_key>
		<display_label>Pages</display_label>
		<pages>776-781</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>93</seq_no>
		<title><![CDATA[Bayesian Overlapping Subspace Clustering]]></title>
		<page_from>776</page_from>
		<page_to>781</page_to>
		<doi_number>10.1109/ICDM.2009.132</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1676996</url>
		<abstract>
			<par><![CDATA[Given a data matrix, the problem of finding dense/uniform sub-blocks in the matrix is becoming important in several applications. The problem is inherently combinatorial since the uniform sub-blocks may involve arbitrary subsets of rows and columns and may even be overlapping. While there are a few existing methods based on co-clustering or subspace clustering, they typically rely on local search heuristics and in general do not have a systematic model for such data. We present a Bayesian Overlapping Subspace Clustering (BOSC) model which is a hierarchical generative model for matrices with potentially overlapping uniform sub-block structures. The BOSC model can also handle matrices with missing entries. We propose an EM-style algorithm based on approximate inference using Gibbs sampling and parameter estimation using coordinate descent for the BOSC model. Through experiments on both simulated and real datasets, we demonstrate that the proposed algorithm outperforms the state-of-the-art.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1820663</person_id>
				<author_profile_id><![CDATA[81414617732]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Qiang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1820664</person_id>
				<author_profile_id><![CDATA[81100144629]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Arindam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Banerjee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677017</article_id>
		<sort_key>980</sort_key>
		<display_label>Pages</display_label>
		<pages>782-787</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>94</seq_no>
		<title><![CDATA[Unsupervised Relation Extraction by Massive Clustering]]></title>
		<page_from>782</page_from>
		<page_to>787</page_to>
		<doi_number>10.1109/ICDM.2009.81</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677017</url>
		<abstract>
			<par><![CDATA[The goal of Information Extraction is to automatically generate structured pieces of information from the relevant information contained in text documents. Machine Learning techniques have been applied to reduce the cost of Information Extraction system adaptation. However, elements of human supervision strongly bias the learning process. Unsupervised learning approaches can avoid these biases. In this paper, we propose an unsupervised approach to learning for Relation Detection, based on the use of massive clustering ensembles. The results obtained on the ACE Relation Mention Detection task outperform in terms of F1 score by 5 points the state of the art of unsupervised techniques for this evaluation framework, in addition to being simpler and more flexible.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Relation Detection, Unsupervised Methods, Ensemble Clustering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1825303</person_id>
				<author_profile_id><![CDATA[81536825556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Edgar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gonz&#224;lez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825304</person_id>
				<author_profile_id><![CDATA[81100332545]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jordi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Turmo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1676997</article_id>
		<sort_key>990</sort_key>
		<display_label>Pages</display_label>
		<pages>788-793</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>95</seq_no>
		<title><![CDATA[Regression Learning Vector Quantization]]></title>
		<page_from>788</page_from>
		<page_to>793</page_to>
		<doi_number>10.1109/ICDM.2009.145</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1676997</url>
		<abstract>
			<par><![CDATA[Learning Vector Quantization (LVQ) is a popular class of nearest prototype classifiers for multiclass classification. Learning algorithms from this family are widely used because of their intuitively clear learning process and ease of implementation. In this paper we propose an extension of the LVQ algorithm to regression. Just like the LVQ algorithm, the proposed modification uses a supervised learning procedure to learn the best prototype positions, but unlike LVQ algorithm for classification, it also learns the best prototype target values. This results in the effective partition of the feature space, similar to the one the K-means algorithm would make. Experimental results on benchmark datasets showed that the proposed Regression LVQ algorithm performs better than the nearest prototype competitors that choose prototypes randomly or through K-means clustering, classification LVQ on quantized target values, and similarly to the memory-based Parzen Window and Nearest Neighbor algorithms.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[regression, learning vector quantization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1825109</person_id>
				<author_profile_id><![CDATA[81435610735]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mihajlo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grbovic]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825110</person_id>
				<author_profile_id><![CDATA[81100461872]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Slobodan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vucetic]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677018</article_id>
		<sort_key>1000</sort_key>
		<display_label>Pages</display_label>
		<pages>794-799</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>96</seq_no>
		<title><![CDATA[Projective Clustering Ensembles]]></title>
		<page_from>794</page_from>
		<page_to>799</page_to>
		<doi_number>10.1109/ICDM.2009.131</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677018</url>
		<abstract>
			<par><![CDATA[Recent advances in data clustering concern clustering ensembles and projective clustering methods, each addressing different issues in clustering problems. In this paper, we consider for the first time the projective clustering ensemble (PCE) problem, whose main goal is to derive a proper projective consensus partition from an ensemble of projective clustering solutions. We formalize PCE as an optimization problem which does not rely on any particular clustering ensemble algorithm, and which has the ability to handle hard as well as soft data clustering, and different feature weightings. We provide two formulations for PCE, namely a two-objective and a single-objective problem, in which the object-based and feature-based representations of the ensemble solutions are taken into account differently. Experiments have demonstrated that the proposed methods for PCE show clear improvements in terms of accuracy of the output consensus partition.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[data mining, clustering, clustering ensembles, projective clustering, subspace clustering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1825305</person_id>
				<author_profile_id><![CDATA[81335491254]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Francesco]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gullo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825306</person_id>
				<author_profile_id><![CDATA[81100062921]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Carlotta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Domeniconi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825307</person_id>
				<author_profile_id><![CDATA[81100286971]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Andrea]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tagarelli]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677019</article_id>
		<sort_key>1010</sort_key>
		<display_label>Pages</display_label>
		<pages>800-805</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>97</seq_no>
		<title><![CDATA[Knowledge Discovery from Citation Networks]]></title>
		<page_from>800</page_from>
		<page_to>805</page_to>
		<doi_number>10.1109/ICDM.2009.137</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677019</url>
		<abstract>
			<par><![CDATA[Knowledge discovery from scientific articles has received increasing attentions recently since huge repositories are made available by the development of the Internet and digital databases. In a corpus of scientific articles such as a digital library, documents are connected by citations and one document plays two different roles in the corpus: \emph{document itself} and \emph{a citation of other documents}. In the existing topic models, little effort is made to differentiate these two roles. We believe that the topic distributions of these two roles are different and related in a certain way. In this paper we propose a \emph{Bernoulli Process Topic}~(BPT) model which models the corpus at two levels: \emph{document level} and \emph{citation level}. In the BPT model, each document has two different representations in the latent topic space associated with its roles. Moreover, the multi-level hierarchical structure of the citation network is captured by a generative process involving a Bernoulli process. The distribution parameters of the BPT model are estimated by a variational approximation approach. In addition to conducting the experimental evaluations on the document modeling task, we also apply the BPT model to a well known scientific corpus to discover the latent topics. The comparisons against state-of-the-art methods demonstrate a very promising performance.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Unsupervised learning, latent models, text mining]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1829712</person_id>
				<author_profile_id><![CDATA[81100085033]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Zhen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1829713</person_id>
				<author_profile_id><![CDATA[81453627335]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Zhongfei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1829714</person_id>
				<author_profile_id><![CDATA[81100413890]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shenghuo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1829715</person_id>
				<author_profile_id><![CDATA[81350577988]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1829716</person_id>
				<author_profile_id><![CDATA[81453648814]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Yihong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677026</article_id>
		<sort_key>1020</sort_key>
		<display_label>Pages</display_label>
		<pages>806-811</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>98</seq_no>
		<title><![CDATA[An Effective Approach to Inverse Frequent Set Mining]]></title>
		<page_from>806</page_from>
		<page_to>811</page_to>
		<doi_number>10.1109/ICDM.2009.123</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677026</url>
		<abstract>
			<par><![CDATA[The inverse frequent set mining problem is the problem of computing a database on which a given collection of itemsets must emerge to be frequent. Earlier studies focused on investigating computational and approximability properties of this problem. In this paper, we face it under the pragmatic perspective of defining heuristic solution approaches that are effective and scalable in real scenarios. In particular, a general formulation of the problem is considered where minimum and maximum support constraints can be defined on each itemset, and where no bound is given beforehand on the size of the resulting output database. Within this setting, an algorithm is proposed that always satisfies the maximum support constraints, but which treats minimum support constraints as soft ones that are enforced as long as possible. A thorough experimentation evidences that minimum support constraints are hardly violated in practice, and that such negligible degradation in accuracy (which is unavoidable due to the theoretical intractability of the problem) is compensated by very good scaling performances.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Inverse Frequent Mining, Data Reconstruction, Complexity]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1828275</person_id>
				<author_profile_id><![CDATA[81100560785]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Antonella]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guzzo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828276</person_id>
				<author_profile_id><![CDATA[81100111213]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Domenico]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sacc&#224;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828277</person_id>
				<author_profile_id><![CDATA[81453617297]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Edoardo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Serra]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677027</article_id>
		<sort_key>1030</sort_key>
		<display_label>Pages</display_label>
		<pages>812-817</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>99</seq_no>
		<title><![CDATA[Parallel PathFinder Algorithms for Mining Structures from Graphs]]></title>
		<page_from>812</page_from>
		<page_to>817</page_to>
		<doi_number>10.1109/ICDM.2009.142</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677027</url>
		<abstract>
			<par><![CDATA[PathFinder networks are increasingly used in Data Mining for different purposes, like network visualization or knowledge extraction. This novel way of representing graphical data has been proven to give better results than other link reduction algorithms, like minimum spanning networks However, this increase in quality comes with a high computation cost, typically of the order of n^3 or higher, where n is the number of nodes in the graph. While this problem has previously been tackled by using mathematical properties to speed up the algorithm, in this paper, we propose two new algorithms to speed up PathFinder computation based on parallelization techniques to take advantage of the increasingly available multi-core hardware platform. Experiments show that both new algorithms are more efficient than the state of the art algorithms; one of them can achieve speed-ups of up to x127 with an average of x23 on recent hardware (2007).]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Graph Mining, Parallel Computing]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1825339</person_id>
				<author_profile_id><![CDATA[81453660640]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Samson]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hauguel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825340</person_id>
				<author_profile_id><![CDATA[81453628552]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[ChengXiang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825341</person_id>
				<author_profile_id><![CDATA[81351593425]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jiawei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Han]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677020</article_id>
		<sort_key>1040</sort_key>
		<display_label>Pages</display_label>
		<pages>818-823</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>100</seq_no>
		<title><![CDATA[A Bootstrap Approach to Eigenvalue Correction]]></title>
		<page_from>818</page_from>
		<page_to>823</page_to>
		<doi_number>10.1109/ICDM.2009.111</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677020</url>
		<abstract>
			<par><![CDATA[Eigenvalue analysis is an important aspect in many data modeling methods. Unfortunately, the eigenvalues of the sample covariance matrix (sample eigenvalues) are biased estimates of the eigenvalues of the covariance matrix of the data generating process (population eigenvalues). We present a new method based on bootstrapping to reduce the bias in the sample eigenvalues: the eigenvalue estimates are updated in several iterations, where in each iteration synthetic data is generated to determine how to update the population eigenvalue estimates. Comparison of the bootstrap eigenvalue correction with a state of the art correction method by Karoui shows that depending on the type of population eigenvalue distribution, sometimes the Karoui method performs better and sometimes our bootstrap method.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[bootstrapping, eigenvalue correction, general statistical analysis, isotonic tree method]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1820860</person_id>
				<author_profile_id><![CDATA[81436599542]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Anne]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hendrikse]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1820861</person_id>
				<author_profile_id><![CDATA[81436597961]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Luuk]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Spreeuwers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1820862</person_id>
				<author_profile_id><![CDATA[81100638108]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Raymond]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Veldhuis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677028</article_id>
		<sort_key>1050</sort_key>
		<display_label>Pages</display_label>
		<pages>824-829</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>101</seq_no>
		<title><![CDATA[Modeling Syntactic Structures of Topics with a Nested HMM-LDA]]></title>
		<page_from>824</page_from>
		<page_to>829</page_to>
		<doi_number>10.1109/ICDM.2009.144</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677028</url>
		<abstract>
			<par><![CDATA[Latent Dirichlet Allocation (LDA) is a commonly used topic modeling method for text analysis and mining. Standard LDA treats documents as bags of words, ignoring the syntactic structures of sentences. In this paper, we propose a hybrid model that embeds hidden Markov models (HMMs) within LDA topics to jointly model both the topics and the syntactic structures within each topic. Our model is general and subsumes standard LDA and HMM as special cases. Compared with standard LDA and HMM, our model can simultaneously discover both topic-specific content words and background functional words shared among topics. Our model can also automatically separate content words that play different roles within a topic. Using perplexity as evaluation metric, our model returns lower perplexity for unseen test documents compared with standard LDA, which shows its better generalization power than LDA.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1825342</person_id>
				<author_profile_id><![CDATA[81453608512]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jiang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677029</article_id>
		<sort_key>1060</sort_key>
		<display_label>Pages</display_label>
		<pages>830-835</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>102</seq_no>
		<title><![CDATA[Redistricting Using Heuristic-Based Polygonal Clustering]]></title>
		<page_from>830</page_from>
		<page_to>835</page_to>
		<doi_number>10.1109/ICDM.2009.126</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677029</url>
		<abstract>
			<par><![CDATA[Redistricting is the process of dividing a geographic area into districts or zones. This process has been considered in the past as a problem that is computationally too complex for an automated system to be developed that can produce unbiased plans. In this paper we present a novel method for redistricting a geographic area using a heuristic-based approach for polygonal spatial clustering. While clustering geospatial polygons several complex issues need to be addressed &#8211; such as: removing order dependency, clustering all polygons assuming no outliers, and strategically utilizing domain knowledge to guide the clustering process. In order to address these special needs, we have developed the Constrained Polygonal Spatial Clustering (CPSC) algorithm that holistically integrates do-main knowledge in the form of cluster-level and instance-level constraints and uses heuristic functions to grow clusters. In order to illustrate the usefulness of our algorithm we have applied it to the problem of formation of unbiased congressional districts. Furthermore, we compare and contrast our algorithm with two other approaches proposed in the literature for redistricting, namely &#8211; graph partitioning and simulated annealing.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[redistricitng, spatial data mining, polygon, polygonal clustering, district formation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1826821</person_id>
				<author_profile_id><![CDATA[81100641774]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Deepti]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Joshi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1826822</person_id>
				<author_profile_id><![CDATA[81100202110]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Leen-Kiat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Soh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1826823</person_id>
				<author_profile_id><![CDATA[81100048997]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ashok]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Samal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677030</article_id>
		<sort_key>1070</sort_key>
		<display_label>Pages</display_label>
		<pages>836-841</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>103</seq_no>
		<title><![CDATA[A Walk from 2-Norm SVM to 1-Norm SVM]]></title>
		<page_from>836</page_from>
		<page_to>841</page_to>
		<doi_number>10.1109/ICDM.2009.100</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677030</url>
		<abstract>
			<par><![CDATA[This paper studies how useful the standard 2-norm regularized SVM is in approximating the 1-norm SVM problem. To this end, we examine a general method that is based on iteratively re-weighting the features and solving a 2-norm optimization problem. The convergence rate of this method is unknown. Previous work indicates that it might require an excessive number of iterations. We study how well we can do with just a small number of iterations. In theory the convergence rate is fast, except for coordinates of the current solution that are close to zero. Our empirical experiments confirm this. In many problems with irrelevant features, already one iteration is often enough to produce accuracy as good as or better than that of the 1-norm SVM. Hence, it seems that in these problems we do not need to converge to the 1-norm SVM solution near zero values. The benefit of this approach is that we can build something similar to the 1-norm regularized solver based on any 2-norm regularized solver. This is quick to implement and the solution inherits the good qualities of the solver such as scalability and stability.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[SVM, 1-norm minimization, reductions, large-scale data]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1828278</person_id>
				<author_profile_id><![CDATA[81384598440]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jussi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kujala]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828279</person_id>
				<author_profile_id><![CDATA[81387604133]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Timo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828280</person_id>
				<author_profile_id><![CDATA[81100604655]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tapio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Elomaa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677037</article_id>
		<sort_key>1080</sort_key>
		<display_label>Pages</display_label>
		<pages>842-847</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>104</seq_no>
		<title><![CDATA[Semi-supervised Density-Based Clustering]]></title>
		<page_from>842</page_from>
		<page_to>847</page_to>
		<doi_number>10.1109/ICDM.2009.143</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677037</url>
		<abstract>
			<par><![CDATA[Most of the effort in the semi-supervised clustering literature was devoted to variations of the K-means algorithm. In this paper we show how background knowledge can be used to bias a partitional density-based clustering algorithm. Our work describes how labeled objects can be used to help the algorithm detecting suitable density parameters for the algorithm to extract density-based clusters in specific parts of the feature space. Considering the set of constraints estabilished by the labeled dataset we show that our algorithm, called SSDBSCAN, automatically finds density parameters for each natural cluster in a dataset. Four of the most interesting characteristics of SSDBSCAN are that (1) it only requires a single, robust input parameter, (2) it does not need any user intervention, (3) it automaticaly finds the noise objects according to the density of the natural clusters and (4) it is able to find the natural cluster structure even when the density among clusters vary widely. The algorithm presented in this paper is evaluated with artificial and real-world datasets, demonstrating better results when compared to other unsupervised and semi-supervised density-based approaches.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Semi-supervised, density-based clustering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1829791</person_id>
				<author_profile_id><![CDATA[81453656893]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Levi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lelis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1829792</person_id>
				<author_profile_id><![CDATA[81351602082]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[J&#246;rg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sander]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677031</article_id>
		<sort_key>1090</sort_key>
		<display_label>Pages</display_label>
		<pages>848-853</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>105</seq_no>
		<title><![CDATA[VIF Regression]]></title>
		<subtitle><![CDATA[A Fast Regression Algorithm for Large Data]]></subtitle>
		<page_from>848</page_from>
		<page_to>853</page_to>
		<doi_number>10.1109/ICDM.2009.146</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677031</url>
		<abstract>
			<par><![CDATA[We propose a fast regression algorithm that can substantially reduce the computational complexity of searching, yet retain good accuracy. It also guarantees to discover correlated features that are collectively predictive, and avoid model over-fitting. Its capability of controlling mFDR (marginal False Discovery Rate) statistically enables the one-pass search of the fast algorithm and guarantees the accuracy of the sparse model chosen by the algorithm without cross validation. Numerical results show that our algorithm is much faster than any other algorithm and is competitively as accurate as the best but slower algorithms.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[variable selection, stepwise regression, variance inflation factor, false discovery rate]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1828281</person_id>
				<author_profile_id><![CDATA[81453648806]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dongyu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828282</person_id>
				<author_profile_id><![CDATA[81100120952]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dean]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Foster]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677038</article_id>
		<sort_key>1100</sort_key>
		<display_label>Pages</display_label>
		<pages>854-859</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>106</seq_no>
		<title><![CDATA[Sparse Norm-Regularized Reconstructive Coefficients Learning]]></title>
		<page_from>854</page_from>
		<page_to>859</page_to>
		<doi_number>10.1109/ICDM.2009.106</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677038</url>
		<abstract>
			<par><![CDATA[Inspired by the fact that the final decision rule is mainly affected by a small subset of the training samples, i.e., Support Vector Machine(SVM) shows that the decision function relies on the few samples that are on or over the margin. We propose a new framework that explicitly strengthen this intuitive fact by adding an $l_1$-norm regularizer. We give different formulations for our framework in different scenarios, and the experiments show that our framework can not only lead to high sparse solutions but also better performance than traditional methods.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[support vector machine, $l_1$ norm, sparse]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1828331</person_id>
				<author_profile_id><![CDATA[81387601905]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828332</person_id>
				<author_profile_id><![CDATA[81447604802]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shuo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828333</person_id>
				<author_profile_id><![CDATA[81447597753]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mingjie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Qian]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828334</person_id>
				<author_profile_id><![CDATA[81372592002]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Changshui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677039</article_id>
		<sort_key>1110</sort_key>
		<display_label>Pages</display_label>
		<pages>860-865</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>107</seq_no>
		<title><![CDATA[A Contrast Pattern Based Clustering Quality Index for Categorical Data]]></title>
		<page_from>860</page_from>
		<page_to>865</page_to>
		<doi_number>10.1109/ICDM.2009.105</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677039</url>
		<abstract>
			<par><![CDATA[Since clustering is unsupervised and highly explorative, clustering validation (i.e. assessing the quality of clustering solutions) has been an important and long standing research problem. Existing validity measures have significant shortcomings. This paper proposes a novel Contrast Pattern based Clustering Quality index (CPCQ) for categorical data, by utilizing the quality and diversity of the contrast patterns (CPs) which contrast the clusters in clusterings. High quality CPs can characterize clusters and discriminate them against each other. Experiments show that the CPCQ index (1) can recognize that expert-determined classes are the best clusters for many datasets from the UCI repository; (2) does not give inappropriate preference to larger number of clusters; (3) does not require a user to provide a distance function.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Clustering validation, contrast pattern, clustering quality index]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1822475</person_id>
				<author_profile_id><![CDATA[81453643737]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Qingbao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822476</person_id>
				<author_profile_id><![CDATA[81453613118]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Guozhu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677040</article_id>
		<sort_key>1120</sort_key>
		<display_label>Pages</display_label>
		<pages>866-871</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>108</seq_no>
		<title><![CDATA[Multi-document Summarization by Information Distance]]></title>
		<page_from>866</page_from>
		<page_to>871</page_to>
		<doi_number>10.1109/ICDM.2009.107</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677040</url>
		<abstract>
			<par><![CDATA[Fast changing knowledge on the Internet can be acquired more efficiently with the help of automatic document summarization and updating techniques. This paper described a novel approach for multi-document update summarization. The best summary is defined to be the one which has the minimum information distance to the entire document set. The best update summary has the minimum conditional information distance to a document cluster given that a prior document cluster has already been read. Experiments on the DUC 2007 dataset and the TAC 2008 dataset have proved that our method closely correlates with the human summaries and outperforms other programs such as LexRank in many categories under the ROUGE evaluation criterion.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Data Mining, Text Mining, Kolmogorov Complexity, Information Distance]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1822477</person_id>
				<author_profile_id><![CDATA[81384601302]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Long]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822478</person_id>
				<author_profile_id><![CDATA[81430592684]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Minlie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822479</person_id>
				<author_profile_id><![CDATA[81438595316]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Xiaoyan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822480</person_id>
				<author_profile_id><![CDATA[81381602704]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677041</article_id>
		<sort_key>1130</sort_key>
		<display_label>Pages</display_label>
		<pages>872-877</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>109</seq_no>
		<title><![CDATA[On the (In)Security and (Im)Practicality of Outsourcing Precise Association Rule Mining]]></title>
		<page_from>872</page_from>
		<page_to>877</page_to>
		<doi_number>10.1109/ICDM.2009.122</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677041</url>
		<abstract>
			<par><![CDATA[The recent interest in outsourcing IT services onto the cloud raises two main concerns: security and cost. One task that could be outsourced is data mining. In VLDB 2007, Wong et al. propose an approach for outsourcing association rule mining. Their approach maps a set of real items into a set of pseudo items, then maps each transaction non-deterministically. This paper, analyzes both the security and costs associated with outsourcing association rule mining. We show how to break the encoding scheme from Wong et al. without using context specific information and reduce the security to a one-to-one mapping. We present a stricter notion of security than used by Wong et al., and then consider the practicality of outsourcing association rule mining. Our results indicate that outsourcing association rule mining may not be practical, if the data owner is concerned with data confidentiality.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[association rule mining, outsourcing, security]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1825379</person_id>
				<author_profile_id><![CDATA[81351591462]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Molloy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825380</person_id>
				<author_profile_id><![CDATA[81453605431]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ninghui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825381</person_id>
				<author_profile_id><![CDATA[81331498191]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tiancheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677042</article_id>
		<sort_key>1140</sort_key>
		<display_label>Pages</display_label>
		<pages>878-883</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>110</seq_no>
		<title><![CDATA[Promoting Total Efficiency in Text Clustering via Iterative and Interactive Metric Learning]]></title>
		<page_from>878</page_from>
		<page_to>883</page_to>
		<doi_number>10.1109/ICDM.2009.124</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677042</url>
		<abstract>
			<par><![CDATA[In this paper, we propose a framework to make the text clustering process, as a whole, efficient. In a real text clustering task, an analyst usually has some expectation on the results in mind. However, a single run of a clustering algorithm on the preprocessed data would not satisfy the expectation. Then the analyst faces labor-intensive trials for improving the results that involve repetitive feature refinement and parameter tuning. We develop the Iterative and Interactive Metric Learning System (IIMLS) for addressing the challenge. Specifically, IIMLS allows analysts to input feedback on a current clustering result. Given the feedback, IIMLS optimizes metric in the feature space so that the clustering algorithm applied with the refined metric would reflect the feedback. As a byproduct, learned metric may be used for a similar dataset. Illustrative examples on a real-world dataset show IIMLS can dramatically improve efficiency of a text clustering task. The learned &#8220;knowledge&#8221;, or the metric, is visualized for gaining insights of the optimized feature metric.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[data preprocessing, metric learning, interactive system]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1828335</person_id>
				<author_profile_id><![CDATA[81453616048]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michinari]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Momma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828336</person_id>
				<author_profile_id><![CDATA[81537765256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Satoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morinaga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828337</person_id>
				<author_profile_id><![CDATA[81453613777]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Daisuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Komura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677048</article_id>
		<sort_key>1150</sort_key>
		<display_label>Pages</display_label>
		<pages>884-889</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>111</seq_no>
		<title><![CDATA[A New Clustering Algorithm Based on Regions of Influence with Self-Detection of the Best Number of Clusters]]></title>
		<page_from>884</page_from>
		<page_to>889</page_to>
		<doi_number>10.1109/ICDM.2009.133</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677048</url>
		<abstract>
			<par><![CDATA[Clustering methods usually require to know the best number of clusters, or another parameter, e.g. a threshold, which is not ever easy to provide. This paper proposes a new graph-based clustering method called GBC which detects automatically the best number of clusters, without requiring any other parameter. In this method based on regions of influence, a graph is constructed and the edges of the graph having the higher values are cut according to a hierarchical divisive procedure. An index is calculated from the size average of the cut edges which self-detects the more appropriate number of clusters. The results of GBC for 3 quality indices (Dunn, Silhouette and Davies-Bouldin) are compared with those of K-Means, Ward's hierarchical clustering method and DBSCAN on 8 benchmarks. The experiments show the good performance of GBC in the case of well separated clusters, even if the data are unbalanced, non-convex or with presence of outliers, whatever the shape of the clusters.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[clustering, neighborhood graph]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1831424</person_id>
				<author_profile_id><![CDATA[81100547384]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fabrice]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Muhlenbach]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831425</person_id>
				<author_profile_id><![CDATA[81100589472]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[St&#233;phane]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lallich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677049</article_id>
		<sort_key>1160</sort_key>
		<display_label>Pages</display_label>
		<pages>890-895</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>112</seq_no>
		<title><![CDATA[Automatically Extracting Dialog Models from Conversation Transcripts]]></title>
		<page_from>890</page_from>
		<page_to>895</page_to>
		<doi_number>10.1109/ICDM.2009.113</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677049</url>
		<abstract>
			<par><![CDATA[There is a growing need for task-oriented natural language dialog systems that can interact with a user to accomplish a given objective. Recent work on building task-oriented dialog systems have emphasized the need for acquiring task-specific knowledge from un-annotated conversational data. In our work we acquire task-specific knowledge by defining \textit{sub-task} as the key unit of a task-oriented conversation. We propose an unsupervised, apriori like algorithm that extracts the sub-tasks and their valid orderings from un-annotated human-human conversations. Modeling dialogues as a combination of sub-tasks and their valid orderings easily captures the variability in conversations. It also provides us the ability to map our dialogue model to AIML constructs and therefore use off-the-shelf AIML interpreters to build task-oriented chat-bots. We conduct experiments on real world data sets to establish the effectiveness of the sub-task extraction process. We codify the extracted sub-tasks in an AIML knowledge base and build a chatbot using this knowledge base. We also show the usefulness of the chatbot in automatically handling customer requests by performing a user evaluation study.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Dialog Models, AIML, Chat-bot]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1823982</person_id>
				<author_profile_id><![CDATA[81100551216]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sumit]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Negi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1823983</person_id>
				<author_profile_id><![CDATA[81453614672]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sachindra]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Joshi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1823984</person_id>
				<author_profile_id><![CDATA[81384621068]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Anup]]></first_name>
				<middle_name><![CDATA[K.]]></middle_name>
				<last_name><![CDATA[Chalamalla]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1823985</person_id>
				<author_profile_id><![CDATA[81100473494]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[L.]]></first_name>
				<middle_name><![CDATA[Venkata]]></middle_name>
				<last_name><![CDATA[Subramaniam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677050</article_id>
		<sort_key>1170</sort_key>
		<display_label>Pages</display_label>
		<pages>896-901</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>113</seq_no>
		<title><![CDATA[To Trust or Not to Trust? Predicting Online Trusts Using Trust Antecedent Framework]]></title>
		<page_from>896</page_from>
		<page_to>901</page_to>
		<doi_number>10.1109/ICDM.2009.115</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677050</url>
		<abstract>
			<par><![CDATA[This paper analyzes the trustor and trustee factors that lead to inter-personal trust using a well studied Trust Antecedent framework in management science \cite{mayer}. To apply these factors to trust ranking problem in online rating systems, we derive features that correspond to each factor and develop different trust ranking models. The advantage of this approach is that features relevant to trust can be systematically derived so as to achieve good prediction accuracy. Through a series of experiments on real data from Epinions, we show that even a simple model using the derived features yields good accuracy and outperforms MoleTrust, a trust propagation based model. SVM classifiers using these features also show improvements.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Trust prediction, trust ranking, trust antecedent framework]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1828376</person_id>
				<author_profile_id><![CDATA[81447599091]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Viet-An]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nguyen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828377</person_id>
				<author_profile_id><![CDATA[81100399142]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ee-Peng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828378</person_id>
				<author_profile_id><![CDATA[81448594241]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jiang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828379</person_id>
				<author_profile_id><![CDATA[81100446994]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Aixin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677051</article_id>
		<sort_key>1180</sort_key>
		<display_label>Pages</display_label>
		<pages>902-907</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>114</seq_no>
		<title><![CDATA[Analysis of Subsequence Time-Series Clustering Based on Moving Average]]></title>
		<page_from>902</page_from>
		<page_to>907</page_to>
		<doi_number>10.1109/ICDM.2009.147</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677051</url>
		<abstract>
			<par><![CDATA[Subsequence time-series clustering (STSC), which consists of subsequence cutout with a sliding window and k-means clustering, had been commonly used in time-series data mining. However, a problem was pointed out that STSC always generates moderate sinusoidal patterns independently of the input. To address this problem, we theoretically explain and empirically confirm the similarity between STSC and moving average. The present analysis is consistent with, and simpler than, one of the most important analyses of STSC. We also question the pattern extraction in the time domain and discuss another solution.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Time-series, Subsequence, Clustering, Moving Average, Power Spectrum]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1825411</person_id>
				<author_profile_id><![CDATA[81453650006]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Miho]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ohsaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825412</person_id>
				<author_profile_id><![CDATA[81453654676]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Masakazu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakase]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825413</person_id>
				<author_profile_id><![CDATA[81453648326]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shigeru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Katagiri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677052</article_id>
		<sort_key>1190</sort_key>
		<display_label>Pages</display_label>
		<pages>908-913</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>115</seq_no>
		<title><![CDATA[Permutation Tests for Studying Classifier Performance]]></title>
		<page_from>908</page_from>
		<page_to>913</page_to>
		<doi_number>10.1109/ICDM.2009.108</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677052</url>
		<abstract>
			<par><![CDATA[We explore the framework of permutation-based p-values for assessing the behavior of the classification error. In this paper we study two simple permutation tests. The first test estimates the null distribution by permuting the labels in the data; this has been used extensively in classification problems in computational biology. The second test produces permutations of the features within classes, inspired by restricted randomization techniques traditionally used in statistics. We study the properties of these tests and present an extensive empirical evaluation on real and synthetic data. Our analysis shows that studying the classification error via permutation tests is effective; in particular, the restricted permutation test clearly reveals whether the classifier exploits the interdependency between the features in the data.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[classification, labeled data, permutation tests, restricted randomization, significance testing]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1831426</person_id>
				<author_profile_id><![CDATA[81436594498]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Markus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ojala]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831427</person_id>
				<author_profile_id><![CDATA[81363590792]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gemma]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Garriga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677053</article_id>
		<sort_key>1200</sort_key>
		<display_label>Pages</display_label>
		<pages>914-919</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>116</seq_no>
		<title><![CDATA[Interaction-Based Clustering of Multivariate Time Series]]></title>
		<page_from>914</page_from>
		<page_to>919</page_to>
		<doi_number>10.1109/ICDM.2009.109</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677053</url>
		<abstract>
			<par><![CDATA[In this paper, we present a novel approach to clustering multivariate time series. In contrast to previous approaches, we base our cluster notion on the interactions between the univariate time series within a data object. Our objective is to assign objects with a similar intrinsic interaction pattern to a common cluster. To formalize this idea, we define a cluster by a set of mathematical models describing the cluster-specific interaction pattern. In addition, we propose interaction K-means (IKM), an efficient algorithm for partitioning clustering of multivariate time series. The cluster-specific interaction patterns detected by IKM provide valuable information for interpretation of the cluster content. An extensive experimental evaluation on synthetic and real world data demonstrates the effectiveness and efficiency of our approach.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Algorithms, Clustering methods, Time series]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1829841</person_id>
				<author_profile_id><![CDATA[81321496970]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Claudia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Plant]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1829842</person_id>
				<author_profile_id><![CDATA[81453651114]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Afra]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Wohlschl&#228;ger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1829843</person_id>
				<author_profile_id><![CDATA[81442617206]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zherdin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677059</article_id>
		<sort_key>1210</sort_key>
		<display_label>Pages</display_label>
		<pages>920-925</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>117</seq_no>
		<title><![CDATA[PUB]]></title>
		<subtitle><![CDATA[A Class Description Technique Based on Partial Coverage of Subspace]]></subtitle>
		<page_from>920</page_from>
		<page_to>925</page_to>
		<doi_number>10.1109/ICDM.2009.97</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677059</url>
		<abstract>
			<par><![CDATA[A good description of a class should be accurate and interpretable. Previous works describe classes either by analyzing the correlation of each attribute with the class, or by producing rules as in building a classifier. These solutions suffer from issues in accuracy and interpretability. A description naturally consists of sentences, where each sentence consists of a set of terms. Normally, a sentence is defined as a disjunction or conjunction of several terms, each of which specifies a constraint (range/set of values) on an attribute. From the data analysis point of view, a sentence specifies a subspace in the database. In this paper, we create a richer yet interpretable form of a sentence, i.e., a sentence describes an object if any $k$ attributes of that object satisfy the specified constraints. To that end, we design \textsc{Pub}, an algorithm that produces descriptions with our form of sentences. While constructing a sentence (within the description), \textsc{Pub} finds the optimal range/set of values for each attribute in linear time. We also empirically show that \textsc{Pub} is efficient, and able to produce more accurate, concise and interpretable descriptions than current approaches on various real datasets.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[class description, frequent pattern mining, fault-tolerant pattern, classification]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1831455</person_id>
				<author_profile_id><![CDATA[81436593426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ardian]]></first_name>
				<middle_name><![CDATA[Kristanto]]></middle_name>
				<last_name><![CDATA[Poernomo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831456</person_id>
				<author_profile_id><![CDATA[81319492485]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Vivekanand]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gopalkrishnan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677060</article_id>
		<sort_key>1220</sort_key>
		<display_label>Pages</display_label>
		<pages>926-931</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>118</seq_no>
		<title><![CDATA[Online and Batch Learning of Generalized Cosine Similarities]]></title>
		<page_from>926</page_from>
		<page_to>931</page_to>
		<doi_number>10.1109/ICDM.2009.114</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677060</url>
		<abstract>
			<par><![CDATA[In this paper, we define an online algorithm to learn the generalized cosine similarity measures for kNN classification and hence a similarity matrix A corresponding to a bilinear form. In contrary to the standard cosine measure, the normalization is itself dependent on the similarity matrix which makes it impossible to use directly the algorithms developed for learning Mahanalobis distances, based on positive, semi-definite (PSD) matrices. We follow the approach where we first find an appropriate matrix and then project it onto the cone of PSD matrices, which we have adapted to the particular form of generalized cosine similarities, and more particularly to the fact that such measures are normalized. The resulting online algorithm as well as its batch version is fast and has got better accuracy as compared with state-of-the-art methods on standard data sets.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Generalized cosine, Similarity learning, k-NN classification]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1829899</person_id>
				<author_profile_id><![CDATA[81414617002]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ali]]></first_name>
				<middle_name><![CDATA[Mustafa]]></middle_name>
				<last_name><![CDATA[Qamar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1829900</person_id>
				<author_profile_id><![CDATA[81100467616]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gaussier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677061</article_id>
		<sort_key>1230</sort_key>
		<display_label>Pages</display_label>
		<pages>932-937</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>119</seq_no>
		<title><![CDATA[Discovering Organizational Structure in Dynamic Social Network]]></title>
		<page_from>932</page_from>
		<page_to>937</page_to>
		<doi_number>10.1109/ICDM.2009.86</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677061</url>
		<abstract>
			<par><![CDATA[Applying the concept of organizational structure to social network analysis may well represent the power of members and the scope of their power in a social network. In this paper, we propose a data structure, called Community Tree, to represent the organizational structure in the social network. We combine the PageRank algorithm and random walks on graph to derive the community tree from the social network. In the real world, a social network is constantly changing. Hence, the organizational structure in the social network is also constantly changing. In order to present the organizational structure in a dynamic social network, we propose a tree learning algorithm to derive an evolving community tree. The evolving community tree enables a smooth transition between the two community trees and well represents the evolution of organizational structure in the dynamic social network. Experiments conducted on real data show our methods are effective at discovering the organizational structure and representing the evolution of organizational structure in a dynamic social network.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Organizational structure, Dynamical social network]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1820983</person_id>
				<author_profile_id><![CDATA[81384605362]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jiangtao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Qiu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1820984</person_id>
				<author_profile_id><![CDATA[81100220042]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Zhangxi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1820985</person_id>
				<author_profile_id><![CDATA[81339531336]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Changjie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1820986</person_id>
				<author_profile_id><![CDATA[81361592464]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shaojie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Qiao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677062</article_id>
		<sort_key>1240</sort_key>
		<display_label>Pages</display_label>
		<pages>938-943</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>120</seq_no>
		<title><![CDATA[Kernel Conditional Quantile Estimation via Reduction Revisited]]></title>
		<page_from>938</page_from>
		<page_to>943</page_to>
		<doi_number>10.1109/ICDM.2009.82</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677062</url>
		<abstract>
			<par><![CDATA[Quantile regression refers to the process of estimating the quantiles of a conditional distribution and has many important applications within econometrics and data mining, among other domains. In this paper, we show how to estimate these conditional quantile functions within a Bayes risk minimization framework using a Gaussian process prior. The resulting non-parametric probabilistic model is easy to implement and allows non-crossing quantile functions to be enforced. Moreover, it can directly be used in combination with tools and extensions of standard Gaussian Processes such as principled hyperparameter estimation, sparsification, and quantile regression with input-dependent noise rates. No existing approach enjoys all of these desirable properties. Experiments on benchmark datasets show that our method is competitive with state-of-the-art approaches."]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Regression, Quantile Regression, Gaussian Processes]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1829901</person_id>
				<author_profile_id><![CDATA[81421597256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Novi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Quadrianto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1829902</person_id>
				<author_profile_id><![CDATA[81337490510]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kristian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kersting]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1829903</person_id>
				<author_profile_id><![CDATA[81453631543]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Reid]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1829904</person_id>
				<author_profile_id><![CDATA[81100060357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tib&#233;rio]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Caetano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1829905</person_id>
				<author_profile_id><![CDATA[81100245904]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Wray]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Buntine]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677063</article_id>
		<sort_key>1250</sort_key>
		<display_label>Pages</display_label>
		<pages>944-949</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>121</seq_no>
		<title><![CDATA[Naive Bayes Classification of Uncertain Data]]></title>
		<page_from>944</page_from>
		<page_to>949</page_to>
		<doi_number>10.1109/ICDM.2009.90</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677063</url>
		<abstract>
			<par><![CDATA[Traditional machine learning algorithms assume that data are exact or precise. However, this assumption may not hold in some situations because of data uncertainty arising from measurement errors, data staleness, and repeated measurements, etc. With uncertainty, the value of each data item is represented by a probability distribution function (pdf). In this paper, we propose a novel naive Bayes classification algorithm for uncertain data with a pdf. Our key solution is to extend the class conditional probability estimation in the Bayes model to handle pdf&#8217;s. Extensive experiments on UCI datasets show that the accuracy of naive Bayes model can be improved by taking into account the uncertainty information.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Uncertain data mining, naive Bayes model]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1825451</person_id>
				<author_profile_id><![CDATA[81453622435]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jiangtao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ren]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825452</person_id>
				<author_profile_id><![CDATA[81100411220]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sau]]></first_name>
				<middle_name><![CDATA[Dan]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825453</person_id>
				<author_profile_id><![CDATA[81453638593]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Xianlu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825454</person_id>
				<author_profile_id><![CDATA[81100519737]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ben]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825455</person_id>
				<author_profile_id><![CDATA[81100098135]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Reynold]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cheng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825456</person_id>
				<author_profile_id><![CDATA[81100460281]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cheung]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677064</article_id>
		<sort_key>1260</sort_key>
		<display_label>Pages</display_label>
		<pages>950-955</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>122</seq_no>
		<title><![CDATA[Constraint-Based Pattern Mining in Dynamic Graphs]]></title>
		<page_from>950</page_from>
		<page_to>955</page_to>
		<doi_number>10.1109/ICDM.2009.99</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677064</url>
		<abstract>
			<par><![CDATA[Dynamic graphs are used to represent relationships between entities that evolve over time. Meaningful patterns in such structured data must capture strong interactions and their evolution over time. In social networks, such patterns can be seen as dynamic community structures, i.e., sets of individuals who strongly and repeatedly interact. In this paper, we propose a constraint-based mining approach to uncover evolving patterns. We propose to mine dense and isolated subgraphs defined by two user-parameterized constraints. The temporal evolution of such patterns is captured by associating a temporal event type to each identified subgraph. We consider five basic temporal events: The formation, dissolution, growth, diminution and stability of subgraphs from one time stamp to the next. We propose an algorithm that finds such subgraphs in a time series of graphs processed incrementally. The extraction is feasible due to efficient patterns and data pruning strategies. We demonstrate the applicability of our method on several real-world dynamic graphs and extract meaningful evolving communities.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[dynamic graph, local pattern, evolving pattern]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1829906</person_id>
				<author_profile_id><![CDATA[81100271634]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[C&#233;line]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Robardet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677070</article_id>
		<sort_key>1270</sort_key>
		<display_label>Pages</display_label>
		<pages>956-961</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>123</seq_no>
		<title><![CDATA[Global Slope Change Synopses for Measurement Maps]]></title>
		<page_from>956</page_from>
		<page_to>961</page_to>
		<doi_number>10.1109/ICDM.2009.117</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677070</url>
		<abstract>
			<par><![CDATA[Quality control using scalar quality measures is standard practice in manufacturing. However, there are also quality measures that are determined at a large number of positions on a product, since the spatial distribution is important. We denote such a mapping of local coordinates on the product to values of a measure as a measurement map. In this paper, we examine how measurement maps can be clustered according to a novel notion of similarity&#8212;mapscape similarity&#8212;that considers the overall course of the measure on the map. We present a class of synopses called global slope change that uses the profile of the measure along several lines from a reference point to different points on the borders to represent a measurement map. We conduct an evaluation of global slope change using a real-world data set from manufacturing and demonstrate its superiority over other synopses.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[measurement map, mapscape similarity, synopsis]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1822585</person_id>
				<author_profile_id><![CDATA[81332524559]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Frank]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rosenthal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822586</person_id>
				<author_profile_id><![CDATA[81340489046]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ulrike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fischer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822587</person_id>
				<author_profile_id><![CDATA[81361595186]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[B.]]></middle_name>
				<last_name><![CDATA[Volk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822588</person_id>
				<author_profile_id><![CDATA[81100335067]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Wolfgang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lehner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677071</article_id>
		<sort_key>1280</sort_key>
		<display_label>Pages</display_label>
		<pages>962-967</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>124</seq_no>
		<title><![CDATA[Aspect Guided Text Categorization with Unobserved Labels]]></title>
		<page_from>962</page_from>
		<page_to>967</page_to>
		<doi_number>10.1109/ICDM.2009.129</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677071</url>
		<abstract>
			<par><![CDATA[This paper proposes a novel multiclass classification method and exhibits its advantage in the domain of text categorization with a large label space and, most importantly, when some of the labels were not observed in the training data. The key insight is the introduction of intermediate aspect variables that encode properties of the labels. Aspect variables serve as a joint representation for observed and unobserved labels. This way the classification problem can be viewed as a structure learning problem with natural constraints on assignments to the aspect variables. We solve the problem as a constrained optimization problem over multiple learners and show significant improvement in classifying short sentences into a large label space of categories, including previously unobserved categories.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[multiclass classsification, text categorization, structure learning, constrained optimization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1824041</person_id>
				<author_profile_id><![CDATA[81453608083]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Roth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824042</person_id>
				<author_profile_id><![CDATA[81538818956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuancheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677072</article_id>
		<sort_key>1290</sort_key>
		<display_label>Pages</display_label>
		<pages>968-973</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>125</seq_no>
		<title><![CDATA[A Fully Automated Method for Discovering Community Structures in High Dimensional Data]]></title>
		<page_from>968</page_from>
		<page_to>973</page_to>
		<doi_number>10.1109/ICDM.2009.141</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677072</url>
		<abstract>
			<par><![CDATA[Identifying modules, or natural communities, in large complex networks is fundamental in many fields, including social sciences, biological sciences and engineering. Recently several methods have been developed to automatically identify communities from complex networks by optimizing the modularity function. The advantage of this type of approaches is that the algorithm does not require any parameter to be tuned. However, the modularity-based methods for community discovery assume that the network structure is given explicitly and is correct. In addition, these methods work best if the network is unweighted and/or sparse. In reality, networks are often not directly defined, or may be given as an affinity matrix. In the first case, each node of the network is defined as a point in a high dimensional space and different networks can be obtained with different network construction methods, resulting in different community structures. In the second case, an affinity matrix may define a dense weighted graph, for which modularity-based methods do not perform well. In this work, we propose a very simple algorithm to automatically identify community structures from these two types of data. Our approach utilizes a k-nearest-neighbor network construction method to capture the topology embedded in high dimensional data, and applies a modularity-based algorithm to identify the optimal community structure. A key to our approach is that the network construction is incorporated with the community identification process and is totally parameter-free. Furthermore, our method can suggest appropriate pre-processing / normalization of the data to improve the results of community identification. We tested our methods on several synthetic and real data sets, and evaluated its performance by internal or external accuracy indices. Compared with several existing approaches, our method is not only fully automatic, but also has the best accuracy overall.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[community structure, modularity, image clustering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1831493</person_id>
				<author_profile_id><![CDATA[81100626099]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jianhua]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ruan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677073</article_id>
		<sort_key>1300</sort_key>
		<display_label>Pages</display_label>
		<pages>974-979</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>126</seq_no>
		<title><![CDATA[Hierarchical Probabilistic Segmentation of Discrete Events]]></title>
		<page_from>974</page_from>
		<page_to>979</page_to>
		<doi_number>10.1109/ICDM.2009.87</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677073</url>
		<abstract>
			<par><![CDATA[Segmentation, the task of splitting a long sequence of discrete symbols into chunks, can provide important information about the nature of the sequence that is understandable to humans. Algorithms for segmenting mostly belong to the supervised learning family, where a labeled corpus is available to the algorithm in the learning phase. We are interested, however, in the unsupervised scenario, where the algorithm never sees examples of successful segmentation, but still needs to discover meaningful segments. In this paper we present an unsupervised learning algorithm for segmenting sequences of symbols or categorical events. Our algorithm, Hierarchical Multigram, hierarchically builds a lexicon of segments and computes a maximum likelihood segmentation given the current lexicon. Thus, our algorithm is most appropriate to hierarchical sequences, where smaller segments are grouped into larger segments. Our probabilistic approach also allows us to suggest conditional entropy as a measurement of the quality of a segmentation in the absence of labeled data. We compare our algorithm to two previous approaches from the unsupervised segmentation literature, showing it to provide superior segmentation over a number of benchmarks. We also compare our algorithm to previous approaches over a segmentation of the unlabeled interactions of a web service and its client.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Segmentation, Multigram, Software analysis]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1824043</person_id>
				<author_profile_id><![CDATA[81321498048]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Guy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824044</person_id>
				<author_profile_id><![CDATA[81100312568]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Meek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824045</person_id>
				<author_profile_id><![CDATA[81100284670]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Asela]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gunawardana]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677074</article_id>
		<sort_key>1310</sort_key>
		<display_label>Pages</display_label>
		<pages>980-985</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>127</seq_no>
		<title><![CDATA[Topic Modeling for Sequences of Temporal Activities]]></title>
		<page_from>980</page_from>
		<page_to>985</page_to>
		<doi_number>10.1109/ICDM.2009.83</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677074</url>
		<abstract>
			<par><![CDATA[Temporally-ordered activity sequences are popular in many real-world domains. This paper presents an LDA-style topic model for sequences of temporal activities that captures three features of such sequences: 1) the counts of unique activities, 2) the Markov transition dependence and 3) the absolute or relative timestamp on each activity. In modeling the first two features we propose the concept of global transition probability and distinguish it with local transition probability used in previous work. In modeling the third feature, we employ a continuous time distribution to depict the time range of latent topics. The combination of the global transition probability and the temporal information helps to refine the mixture distribution over topics for temporal sequence analysis. We present results on the data of system call traces, showing better next activity prediction and sequence clustering.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[topic modeling, LDA, sequence, temporal activities]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1822589</person_id>
				<author_profile_id><![CDATA[81392598013]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Zhiyong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822590</person_id>
				<author_profile_id><![CDATA[81100416055]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ping]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Luo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822591</person_id>
				<author_profile_id><![CDATA[81548030167]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yuhong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xiong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822592</person_id>
				<author_profile_id><![CDATA[81392599651]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822593</person_id>
				<author_profile_id><![CDATA[81450594087]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Yidong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677080</article_id>
		<sort_key>1320</sort_key>
		<display_label>Pages</display_label>
		<pages>986-991</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>128</seq_no>
		<title><![CDATA[Combining Super-Structuring and Abstraction on Sequence Classification]]></title>
		<page_from>986</page_from>
		<page_to>991</page_to>
		<doi_number>10.1109/ICDM.2009.130</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677080</url>
		<abstract>
			<par><![CDATA[We present an approach to adapting the data representation used by a learner on sequence classification tasks. Our approach that exploits the complementary strengths of super-structuring (constructing complex features by combining existing features) and abstraction (grouping of similar features to generate more abstract features), yields smaller and, at the same time, accurate models. Super-structuring provides a way to increase the predictive accuracy of the learned models by enriching the data representation (and hence, increases the complexity of the learned models) whereas abstraction helps reduce the number of model parameters by simplifying the data representation. The results of our experiments on two data sets drawn from macromolecular sequence classification applications show that adapting data representation by combining super-structuring and abstraction, makes it possible to construct predictive models that use significantly smaller number of features (by one to three orders of magnitude) than those that are obtained using super-structuring alone, without sacrificing predictive accuracy. Our experiments also show that simplifying data representation using abstraction yields better performing models than those obtained using feature selection.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[super-structuring, abstraction, feature selection]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1824090</person_id>
				<author_profile_id><![CDATA[81100327674]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Adrian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Silvescu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824091</person_id>
				<author_profile_id><![CDATA[81392612505]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Cornelia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Caragea]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824092</person_id>
				<author_profile_id><![CDATA[81100409481]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Vasant]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Honavar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677081</article_id>
		<sort_key>1330</sort_key>
		<display_label>Pages</display_label>
		<pages>992-997</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>129</seq_no>
		<title><![CDATA[A Global-Model Naive Bayes Approach to the Hierarchical Prediction of Protein Functions]]></title>
		<page_from>992</page_from>
		<page_to>997</page_to>
		<doi_number>10.1109/ICDM.2009.85</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677081</url>
		<abstract>
			<par><![CDATA[In this paper we propose a new global--model approach for hierarchical classification, where a single global classification model is built by considering all the classes in the hierarchy -- rather than building a number of local classification models as it is more usual in hierarchical classification. The method is an extension of the flat classification algorithm naive Bayes. We present the extension made to the original algorithm as well as its evaluation on eight protein function hierarchical classification datasets. The achieved results are positive and show that the proposed global model is better than using a local model approach.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[hierarchical classification, bayesian classification, protein function prediction]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1822639</person_id>
				<author_profile_id><![CDATA[81453657782]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Carlos]]></first_name>
				<middle_name><![CDATA[N.]]></middle_name>
				<last_name><![CDATA[Silla Jr.]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822640</person_id>
				<author_profile_id><![CDATA[81100455231]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alex]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Freitas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677082</article_id>
		<sort_key>1340</sort_key>
		<display_label>Pages</display_label>
		<pages>998-1003</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>130</seq_no>
		<title><![CDATA[Spatio-temporal Energy Based Gait Recognition]]></title>
		<page_from>998</page_from>
		<page_to>1003</page_to>
		<doi_number>10.1109/ICDM.2009.93</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677082</url>
		<abstract>
			<par><![CDATA[Recently there has been lot of interest in using the gait energy image (GEI) of human walk sequence for individual recognition. Researchers have reported very good recognition rates using both unsupervised and supervised methods for normal walk sequences. However, the performance degrades when there is a variant like change in clothing or carrying a bag. This paper shows that the performance for the variant situations can be improved by constructing the GEI with sway alignment instead of upper body alignment, and dynamically selecting just the required number of rows from the bottom of the silhouette as inputs for an unsupervised feature selection approach. The improvement in recognition rates are established with performance testing on a large gait dataset.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Gait Recognition, Gait Energy Image(GEI), Human Identification]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1825512</person_id>
				<author_profile_id><![CDATA[81453615846]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shamsher]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Singh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825513</person_id>
				<author_profile_id><![CDATA[81100075423]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[K.]]></first_name>
				<middle_name><![CDATA[K.]]></middle_name>
				<last_name><![CDATA[Biswas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677083</article_id>
		<sort_key>1350</sort_key>
		<display_label>Pages</display_label>
		<pages>1004-1009</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>131</seq_no>
		<title><![CDATA[Feature Selection in the Tensor Product Feature Space]]></title>
		<page_from>1004</page_from>
		<page_to>1009</page_to>
		<doi_number>10.1109/ICDM.2009.101</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677083</url>
		<abstract>
			<par><![CDATA[Classifying objects that are sampled jointly from two or more domains has many applications. The tensor product feature space is useful for modeling interactions between feature sets in different domains but feature selection in the tensor product feature space is challenging. Conventional feature selection methods ignore the structure of the feature space and may not provide the optimal results. In this paper we propose methods for selecting features in the original feature spaces of different domains. We obtained sparsity through two approaches, one using integer quadratic programming and another using L1-norm regularization. Experimental studies on biological data sets validate our approach.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1826987</person_id>
				<author_profile_id><![CDATA[81414594413]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Aaron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Smalter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1826988</person_id>
				<author_profile_id><![CDATA[81342497864]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1826989</person_id>
				<author_profile_id><![CDATA[81100216721]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gerald]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lushington]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677084</article_id>
		<sort_key>1360</sort_key>
		<display_label>Pages</display_label>
		<pages>1010-1015</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>132</seq_no>
		<title><![CDATA[Topic Distributions over Links on Web]]></title>
		<page_from>1010</page_from>
		<page_to>1015</page_to>
		<doi_number>10.1109/ICDM.2009.116</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677084</url>
		<abstract>
			<par><![CDATA[It is well known that Web users create links with different intentions. However, a key question, which is not well studied, is how to categorize the links and how to quantify the strength of the influence of a web page on another if there is a link between the two linked web pages. In this paper, we focus on the problem of link semantics analysis, and propose a novel supervised learning approach to build a model, based on a training link-labeled and link-weighted graph where a link-label represents the category of a link and a link-weight represents the influence of one web page on the other in a link. Based on the model built, we categorize links and quantify the influence of web pages on the others in a large graph in the same application domain. We discuss our proposed approach, namely Pairwise Restricted Boltzmann Machines (PRBMs), and conduct extensive experimental studies to demonstrate the effectiveness of our approach using large real datasets.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Link semantic analysis, Link analysis, Pairwise restricted boltzmann machines]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1821043</person_id>
				<author_profile_id><![CDATA[81350588685]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1821044</person_id>
				<author_profile_id><![CDATA[81408596200]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1821045</person_id>
				<author_profile_id><![CDATA[81453625130]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jeffrey]]></first_name>
				<middle_name><![CDATA[Xu]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1821046</person_id>
				<author_profile_id><![CDATA[81453619217]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Zi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1821047</person_id>
				<author_profile_id><![CDATA[81329487942]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Keke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1821048</person_id>
				<author_profile_id><![CDATA[81323493105]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Rui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1821049</person_id>
				<author_profile_id><![CDATA[81453606622]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Li]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1821050</person_id>
				<author_profile_id><![CDATA[81453629574]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Zhong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Su]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677085</article_id>
		<sort_key>1370</sort_key>
		<display_label>Pages</display_label>
		<pages>1016-1021</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>133</seq_no>
		<title><![CDATA[Clustering with Multiple Graphs]]></title>
		<page_from>1016</page_from>
		<page_to>1021</page_to>
		<doi_number>10.1109/ICDM.2009.125</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677085</url>
		<abstract>
			<par><![CDATA[In graph-based learning models, entities are often represented as vertices in an undirected graph with weighted edges describing the relationships between entities. In many real-world applications, however, entities are often associated with relations of different types and/or from different sources, which can be well captured by multiple undirected graphs over the same set of vertices. How to exploit such multiple sources of information to make better inferences on entities remains an interesting open problem. In this paper, we focus on the problem of clustering the vertices based on multiple graphs in both unsupervised and semi-supervised settings. As one of our contributions, we propose Linked Matrix Factorization (LMF) as a novel way of fusing information from multiple graph sources. In LMF, each graph is approximated by matrix factorization with a graph-specific factor and a factor common to all graphs, where the common factor provides features for all vertices. Experiments on SIAM journal data show that (1) we can improve the clustering accuracy through fusing multiple sources of information with several models, and (2) LMF yields superior or competitive results compared to other graph-based clustering methods.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[clustering, multiple sources, graph, semi-supervised learning]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1828501</person_id>
				<author_profile_id><![CDATA[81453633414]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828502</person_id>
				<author_profile_id><![CDATA[81435594802]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Zhengdong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828503</person_id>
				<author_profile_id><![CDATA[81100098715]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Inderjit]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Dhillon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677091</article_id>
		<sort_key>1380</sort_key>
		<display_label>Pages</display_label>
		<pages>1022-1027</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>134</seq_no>
		<title><![CDATA[Two Heads Better Than One]]></title>
		<subtitle><![CDATA[Metric+Active Learning and its Applications for IT Service Classification]]></subtitle>
		<page_from>1022</page_from>
		<page_to>1027</page_to>
		<doi_number>10.1109/ICDM.2009.103</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677091</url>
		<abstract>
			<par><![CDATA[Large IT service providers track service requests and their execution through problem/change tickets. It is important to classify the tickets based on the problem/change description in order to understand service quality and to optimize service processes. However, two challenges exist in solving this classification problem: 1) ticket descriptions from different classes are of highly diverse characteristics, which invalidates most standard distance metrics; 2) it is very expensive to obtain high-quality labeled data. To address these challenges, we develop two seemingly independent methods 1) Discriminative Neighborhood Metric Learning (DNML) and 2) Active Learning with Median Selection (ALMS), both of which are, however, based on the same core technique: iterated representative selection. A case study on real IT service classification application is presented to demonstrate the effectiveness and efficiency of our proposed methods.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1824143</person_id>
				<author_profile_id><![CDATA[81408592258]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824144</person_id>
				<author_profile_id><![CDATA[81455605573]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jimeng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824145</person_id>
				<author_profile_id><![CDATA[81100475528]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1824146</person_id>
				<author_profile_id><![CDATA[81100432735]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Nikos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Anerousis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677092</article_id>
		<sort_key>1390</sort_key>
		<display_label>Pages</display_label>
		<pages>1028-1033</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>135</seq_no>
		<title><![CDATA[Maximum Margin Clustering on Data Manifolds]]></title>
		<page_from>1028</page_from>
		<page_to>1033</page_to>
		<doi_number>10.1109/ICDM.2009.104</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677092</url>
		<abstract>
			<par><![CDATA[Clustering is one of the most fundamental and important problems in computer vision and pattern recognition communities. Maximum Margin Clustering(MMC) is a recently proposed clustering technique which has shown promising experimental results. The main theme behind MMC is to extend the standard maximum margin principle in Support Vector Machine (SVM) to the unsupervised scenario. This paper will consider the problem of maximum margin clustering on data manifolds. Specifically, we propose an approach called Manifold Regularized Maximum Margin clustering (MRMMC) which combines both the maximum margin data discrimination and data manifold information in a unified clustering objective and propose an efficient algorithm to solve it. Finally the experimental results on several real world data sets are presented to show the effectiveness of our method.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1822671</person_id>
				<author_profile_id><![CDATA[81408592258]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822672</person_id>
				<author_profile_id><![CDATA[81453628680]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Xin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822673</person_id>
				<author_profile_id><![CDATA[81453626688]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677093</article_id>
		<sort_key>1400</sort_key>
		<display_label>Pages</display_label>
		<pages>1034-1039</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>136</seq_no>
		<title><![CDATA[Discovering Contexts and Contextual Outliers Using Random Walks in Graphs]]></title>
		<page_from>1034</page_from>
		<page_to>1039</page_to>
		<doi_number>10.1109/ICDM.2009.95</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677093</url>
		<abstract>
			<par><![CDATA[The identifying of contextual outliers allows the discovery of anomalous behavior that other forms of outlier detection cannot find. What may appear to be normal behavior with respect to the entire data set can be shown to be anomalous by subsetting the data according to specific spatial or temporal context. However, in many real-world applications, we may not have sufficient a priori contextual information to discover these contextual outliers. This paper addresses the problem by proposing a probabilistic approach based on random walks, which can simultaneously explore meaningful contexts and score contextual outliers therein. Our approach has several advantages including producing outlier scores which can be interpreted as stationary expectations and their calculation in closed form in polynomial time. In addition, we show that point outlier detection using the stationary distribution is a special case of our approach. It allows us to find both global and contextual outliers simultaneously and to create a meaningful ranked list consisting of both types of outliers. This is a major departure from existing work where an algorithm typically identifies one type of outlier. The effectiveness of our method is justified by empirical results on real data sets, with comparison to related work.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1825556</person_id>
				<author_profile_id><![CDATA[81453628679]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xiang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825557</person_id>
				<author_profile_id><![CDATA[81100099431]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Davidson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677094</article_id>
		<sort_key>1410</sort_key>
		<display_label>Pages</display_label>
		<pages>1040-1045</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>137</seq_no>
		<title><![CDATA[Effective Criterion Functions for Efficient Agglomerative Clustering on Very Large Networks]]></title>
		<page_from>1040</page_from>
		<page_to>1045</page_to>
		<doi_number>10.1109/ICDM.2009.91</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677094</url>
		<abstract>
			<par><![CDATA[As the agglomerative clustering algorithm is widely used in data mining, image processing, bioinformatics and pattern recognition. it has attracted great interests from both academical and industrial communities. However, existing studies neglect the decisive factor of the efficiency of the agglomerative clustering algorithm for large complex networks and usually use criterion functions which lead to inefficiency. In this paper, we propose three effective criterion functions for improving performance of agglomerative clustering algorithm. We note that clustering efficiency is determined by two factors: a) the number of neighbors of two merged clusters in each merge step; b) the number of neighbors shared by the two clusters. Based on these observations, we propose a framework for designing criterion functions in order to efficiently find clusters in very large networks. We devise three criterion functions that can effectively control the number of neighbors of clusters, and they can efficiently produce high-quality clusters. We have implemented our method and compared with existing studies on real networks, and our method outperforms state-of-the-art approaches significantly on large networks.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[agglomerative clustering, criterion function, graph]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1821074</person_id>
				<author_profile_id><![CDATA[81453615418]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1821075</person_id>
				<author_profile_id><![CDATA[81384615912]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mingyuan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[An]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677095</article_id>
		<sort_key>1420</sort_key>
		<display_label>Pages</display_label>
		<pages>1046-1051</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>138</seq_no>
		<title><![CDATA[Binomial Matrix Factorization for Discrete Collaborative Filtering]]></title>
		<page_from>1046</page_from>
		<page_to>1051</page_to>
		<doi_number>10.1109/ICDM.2009.79</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677095</url>
		<abstract>
			<par><![CDATA[Matrix factorization (MF) models have proved efficient and well scalable for collaborative filtering (CF) problems. Many researchers also present the probabilistic interpretation of MF. They usually assume that the factor vectors of users and items are from normal distributions, and so are the ratings when the user and item factors are given. Then they can derive the exact MF algorithm by finding a MAP estimate of the model parameters. In this paper we suggest a new probabilistic perspective on MF for discrete CF problems. We assume that all ratings are from binomial distributions with different preference parameters instead of the original normal distributions. The new interpretation is more reasonable for discrete CF problems since they only allow several legal discrete rating values. We also present two effective algorithms to learn the new model and make predictions. They are applied to the Netflix Prize data set and acquire considerably better accuracy than those of MF.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[(probabilistic) matrix factorization, binomial, variational Bayes, collaborative filtering, Netflix Prize]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1827041</person_id>
				<author_profile_id><![CDATA[81453625658]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jinlong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677102</article_id>
		<sort_key>1430</sort_key>
		<display_label>Pages</display_label>
		<pages>1052-1057</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>139</seq_no>
		<title><![CDATA[Bi-relational Network Analysis Using a Fast Random Walk with Restart]]></title>
		<page_from>1052</page_from>
		<page_to>1057</page_to>
		<doi_number>10.1109/ICDM.2009.134</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677102</url>
		<abstract>
			<par><![CDATA[Identification of nodes relevant to a given node in a relational network is a basic problem in network analysis with great practical importance. Most existing network analysis algorithms utilize one single relation to define relevancy among nodes. However, in real world applications multiple relationships exist between nodes in a network. Therefore, network analysis algorithms that can make use of more than one relation to identify the relevance set for a node are needed. In this paper, we show how the Random Walk with Restart (RWR) approach can be used to study relevancy in a bi-relational network from the bibliographic domain, and show that making use of two relations results in better results as compared to approaches that use a single relation. As relational networks can be very large, we also propose a fast implementation for RWR by adapting an existing Iterative Aggregation and Disaggregation (IAD) approach. The IAD-based RWR exploits the block-wise structure of real world networks. Experimental results show significant increase in running time for the IAD-based RWR compared to the traditional power method based RWR.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Relational data mining, node relevancy, random walk, iterative aggregation and disaggregation approach]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1822718</person_id>
				<author_profile_id><![CDATA[81537868656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822719</person_id>
				<author_profile_id><![CDATA[81100256663]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Doina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Caragea]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822720</person_id>
				<author_profile_id><![CDATA[81453635847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Hsu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677103</article_id>
		<sort_key>1440</sort_key>
		<display_label>Pages</display_label>
		<pages>1058-1063</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>140</seq_no>
		<title><![CDATA[A New MCA-Based Divisive Hierarchical Algorithm for Clustering Categorical Data]]></title>
		<page_from>1058</page_from>
		<page_to>1063</page_to>
		<doi_number>10.1109/ICDM.2009.118</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677103</url>
		<abstract>
			<par><![CDATA[Clustering categorical data faces two challenges, one is lacking of inherent similarity measure, and the other is that the clusters are prone to being embedded in different subspace. In this paper, we propose the first divisive hierarchical clustering algorithm for categorical data. The algorithm, which is based on Multiple Correspondence Analysis (MCA), is systematic, efficient and effective. In our algorithm, MCA plays an important role in analyzing the data globally. The proposed algorithm has five merits. First, our algorithm yields a dendrogram representing nested groupings of patterns and similarity levels at different granularities. Second, it is parameter-free, fully automatic and, most importantly, requires no assumption regarding the number of clusters. Third, it is independent of the order in which the data are processed. Forth, it is scalable to large data sets; and finally, using the novel data representation and Chi-square distance measures makes our algorithm capable of seamlessly discovering the clusters embedded in the subspaces. Experiments on both synthetic and real data demonstrate the superior performance of our algorithm.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Clustering, Categorical Data, MCA, Divisive Hierarchical]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1828603</person_id>
				<author_profile_id><![CDATA[81453622686]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tengke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xiong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828604</person_id>
				<author_profile_id><![CDATA[81100216088]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shengrui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828605</person_id>
				<author_profile_id><![CDATA[81100203370]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Andr&#233;]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mayers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1828606</person_id>
				<author_profile_id><![CDATA[81453631130]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ernest]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Monga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677096</article_id>
		<sort_key>1450</sort_key>
		<display_label>Pages</display_label>
		<pages>1064-1069</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>141</seq_no>
		<title><![CDATA[Non-sparse Multiple Kernel Learning for Fisher Discriminant Analysis]]></title>
		<page_from>1064</page_from>
		<page_to>1069</page_to>
		<doi_number>10.1109/ICDM.2009.84</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677096</url>
		<abstract>
			<par><![CDATA[We consider the problem of learning a linear combination of pre-specified kernel matrices in the Fisher discriminant analysis setting. Existing methods for such a task impose an $\ell_1$ norm regularisation on the kernel weights, which produces sparse solution but may lead to loss of information. In this paper, we propose to use $\ell_2$ norm regularisation instead. The resulting learning problem is formulated as a semi-infinite program and can be solved efficiently. Through experiments on both synthetic data and a very challenging object recognition benchmark, the relative advantages of the proposed method and its $\ell_1$ counterpart are demonstrated, and insights are gained as to how the choice of regularisation norm should be made.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Fisher Discriminant Analysis, Multiple Kernel Learning, Semi-Infinite Programming, Object Recognition]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1822674</person_id>
				<author_profile_id><![CDATA[81339538120]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822675</person_id>
				<author_profile_id><![CDATA[81100004495]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Josef]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kittler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822676</person_id>
				<author_profile_id><![CDATA[81300478201]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Krystian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mikolajczyk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822677</person_id>
				<author_profile_id><![CDATA[81453649857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Atif]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tahir]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677104</article_id>
		<sort_key>1460</sort_key>
		<display_label>Pages</display_label>
		<pages>1070-1075</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>142</seq_no>
		<title><![CDATA[Multirelational Topic Models]]></title>
		<page_from>1070</page_from>
		<page_to>1075</page_to>
		<doi_number>10.1109/ICDM.2009.88</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677104</url>
		<abstract>
			<par><![CDATA[In this paper we propose the multirelational topic model (MRTM) for multiple types of link modeling such as citation and coauthor links in document networks. In the citation network, the MRTM models the citation link between each pair of documents as a binary variable conditioned on their topic distributions. In the coauthor network, the MRTM models the coauthor link between each pair of authors as a binary variable conditioned on their expertise distributions. The topic discovery is collectively regularized by multiple relations in both citation and coauthor networks. This model can summarize topics from the document network, predict citation links between documents and coauthor links between authors. Efficient inference and learning algorithms are derived based on Gibbs sampling. Experiments demonstrate that the MRTM significantly outperforms other state-of-the-art single-relational link modeling methods for large scientific document networks.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Topic models, Markov random fields, multirelational link modeling, document networks]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1822721</person_id>
				<author_profile_id><![CDATA[81453641278]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zeng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822722</person_id>
				<author_profile_id><![CDATA[81474672774]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[K.]]></middle_name>
				<last_name><![CDATA[Cheung]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822723</person_id>
				<author_profile_id><![CDATA[81453623432]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chun-hung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1822724</person_id>
				<author_profile_id><![CDATA[81453623483]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jiming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677105</article_id>
		<sort_key>1470</sort_key>
		<display_label>Pages</display_label>
		<pages>1076-1081</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>143</seq_no>
		<title><![CDATA[Learning Local Components to Understand Large Bayesian Networks]]></title>
		<page_from>1076</page_from>
		<page_to>1081</page_to>
		<doi_number>10.1109/ICDM.2009.120</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677105</url>
		<abstract>
			<par><![CDATA[Bayesian networks are known for providing an intuitive and compact representation of probabilistic information and allowing the creation of models over a large and complex domain. Bayesian learning and reasoning are nontrivial for a large Bayesian network. In parallel, it is a tough job for users (domain experts) to extract accurate information from a large Bayesian network due to dimensional difficulty. We define a formulation of local components and propose a clustering algorithm to learn such local components given complete data. The algorithm groups together most inter-relevant attributes in a domain. We evaluate its performance on three benchmark Bayesian networks and provide results in support. We further show that the learned components may represent local knowledge more precisely in comparison to the full Bayesian networks when working with a small amount of data.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1827089</person_id>
				<author_profile_id><![CDATA[81100630080]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yifeng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zeng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1827090</person_id>
				<author_profile_id><![CDATA[81453612284]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yanping]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xiang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1827091</person_id>
				<author_profile_id><![CDATA[81342499337]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jorge]]></first_name>
				<middle_name><![CDATA[Cordero]]></middle_name>
				<last_name><![CDATA[H.]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1827092</person_id>
				<author_profile_id><![CDATA[81453640788]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yujian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1676998</article_id>
		<sort_key>1480</sort_key>
		<display_label>Pages</display_label>
		<pages>1082-1087</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>144</seq_no>
		<title><![CDATA[RING]]></title>
		<subtitle><![CDATA[An Integrated Method for Frequent Representative Subgraph Mining]]></subtitle>
		<page_from>1082</page_from>
		<page_to>1087</page_to>
		<doi_number>10.1109/ICDM.2009.96</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1676998</url>
		<abstract>
			<par><![CDATA[We propose a novel representative based subgraph mining model. A series of standards and methods are proposed to select invariants. Patterns are mapped into invariant vectors in a multidimensional space. To find qualified patterns, only a subset of frequent patterns is generated as representatives, such that every frequent pattern is close to one of the representative patterns while representative patterns are distant from each other. We devise the RING algorithm, integrating the representative selection into the pattern mining process. Meanwhile, we use R-trees to assist this mining process. Last but not least, a large number of real and synthetic datasets are employed for the empirical study, which show the benefits of the representative model and the efficiency of the RING algorithm.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[subgraph mining representative]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1820672</person_id>
				<author_profile_id><![CDATA[81414616282]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shijie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1820673</person_id>
				<author_profile_id><![CDATA[81453606263]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jiong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1820674</person_id>
				<author_profile_id><![CDATA[81414597213]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shirong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677106</article_id>
		<sort_key>1490</sort_key>
		<display_label>Pages</display_label>
		<pages>1088-1093</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>145</seq_no>
		<title><![CDATA[A Cost-Effective LSH Filter for Fast Pairwise Mining]]></title>
		<page_from>1088</page_from>
		<page_to>1093</page_to>
		<doi_number>10.1109/ICDM.2009.112</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677106</url>
		<abstract>
			<par><![CDATA[The pairwise mining problem is to discover pairwise objects having measures greater than the user-specified minimum threshold from a collection of objects. It is essential in a large variety of database and data-mining applications. Of late, there has been increasing interest in applying a Locality-Sensitive Hashing (LSH) scheme for pairwise mining. LSH-type methods have shown themselves to be simply implementable and capable of achieving significant performance gain in running time over most exact methods. However, the present LSH-type methods still suffer from some bottlenecks, such as &#8221;the curse of threshold&#8221;. In this paper, we proposed a novel LSHbased method, namely Cost-effective LSH filter (Ce-LSH for short), for pairwise mining. Compared with previous LSH-type methods, it uses a lower fixed number of LSH functions and is thus more cost-effective. Substantial experiments evidence that our method gives significant improvement in running time over existing LSH-type methods and some recently reported method based on upper-bound. Experimental results also indicate that it scales well even for a relatively low minimum threshold and for a fairly small miss ratio.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[pairwise mining, locality hashing function]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1825602</person_id>
				<author_profile_id><![CDATA[81453608146]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825603</person_id>
				<author_profile_id><![CDATA[81100313927]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xiong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825604</person_id>
				<author_profile_id><![CDATA[81453627992]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Longbing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825605</person_id>
				<author_profile_id><![CDATA[81100416155]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Dan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Luo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825606</person_id>
				<author_profile_id><![CDATA[81453623016]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Xuchun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Su]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1825607</person_id>
				<author_profile_id><![CDATA[81452599411]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Yangyong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677112</article_id>
		<sort_key>1500</sort_key>
		<display_label>Pages</display_label>
		<pages>1094-1099</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>146</seq_no>
		<title><![CDATA[A New Kernel-Based Classification Algorithm]]></title>
		<page_from>1094</page_from>
		<page_to>1099</page_to>
		<doi_number>10.1109/ICDM.2009.80</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677112</url>
		<abstract>
			<par><![CDATA[A new kernel-based learning algorithm called kernel affine subspace nearest point (KASNP) approach is proposed in this paper. Inspired by the geometrical explanation of Support Vector Machines (SVMs) and its nearest point problem in convex hulls, we extend the convex hull of each class to its corresponding affine subspace in high dimensional space induced by kernel. In two class affine subspaces, KASNP finds the nearest points and then constructs a separating hyperplane, which bisects the line segment joining them. The nearest point problem of KASNP is only an unconstrained optimal problem whose solution can be directly computed. Compared with SVM, KASNP avoids solving convex quadratic programming. Experiments on two-spiral dataset, two UCI credit datasets, and face recognition datasets show that our proposed KASNP is effective for data classification.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[classification, SVM, subspace, kernel, nearest points]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1831648</person_id>
				<author_profile_id><![CDATA[81461645805]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xiaofei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831649</person_id>
				<author_profile_id><![CDATA[81461654580]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wenhan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jiang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831650</person_id>
				<author_profile_id><![CDATA[81339532275]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yingjie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tian]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831651</person_id>
				<author_profile_id><![CDATA[81384603947]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Peng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831652</person_id>
				<author_profile_id><![CDATA[81388590913]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Guangli]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1831653</person_id>
				<author_profile_id><![CDATA[81409595544]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Yong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677113</article_id>
		<sort_key>1510</sort_key>
		<display_label>Pages</display_label>
		<pages>1100-1106</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>147</seq_no>
		<title><![CDATA[Author Index]]></title>
		<page_from>1100</page_from>
		<page_to>1106</page_to>
		<doi_number>10.1109/ICDM.2009.148</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677113</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
	</article_rec>
	<article_rec>
		<article_id>1677114</article_id>
		<sort_key>1520</sort_key>
		<display_label>Page</display_label>
		<pages>1108</pages>
		<article_publication_date>12-06-2009</article_publication_date>
		<seq_no>148</seq_no>
		<title><![CDATA[Publisher Information]]></title>
		<page_from>1108</page_from>
		<doi_number>10.1109/ICDM.2009.149</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1677114</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
	</article_rec>
</content>
</proceeding>
